{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab Assignment Seven: Wide and Deep Network Architectures\n",
    "Erik Gabrielsen, Danh Nguyen, Conrad Appel\n",
    "\n",
    "In this lab, you will select a prediction task to perform on your dataset, evaluate two different deep learning architectures and tune hyper-parameters for each architecture. If any part of the assignment is not clear, ask the instructor to clarify. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (one per team) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report.\n",
    "\n",
    "## Dataset Selection\n",
    "\n",
    "Select a dataset identically to lab one. That is, the dataset must be table data. In terms of generalization performance, it is helpful to have a large dataset for building a wide and deep network. It is also helpful to have many different categorical features to create the embeddings and cross-product embeddings. It is fine to perform binary classification or multi-class classification.\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "### Preparation (40 points total)\n",
    "[10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). \n",
    "\n",
    "[10 points] Identify groups of features in your data that should be combined into cross-product features. \n",
    "\n",
    "[10 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\n",
    "[10 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "\n",
    "### Modeling (50 points total)\n",
    "[20 points] Create a combined wide and deep network to classify your data using tensorflow.\n",
    "\n",
    "[20 points] Investigate generalization performance by altering the number of layers. Try at least two different deep network architectures. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab. \n",
    "\n",
    "[10 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve.   \n",
    "\n",
    "### Exceptional Work (10 points total)\n",
    "You have free reign to provide additional analyses.\n",
    "One idea: Investigate which cross-product features are most important and hypothesize why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bank Marketing Data Set\n",
    "\n",
    "For our data set we chose to use the bank marketing data set from a Portuguese banking institute. This data was collected and used to determine whether or not a client will subscribe to a term deposite based on demographical and other information of the call recipient. The training data set has 45211 number of instances, and there are 20 attributes associated with each instance. The following are the columns listed in order. The last column 'y' is the column that we are attempting to predict. \n",
    "\n",
    "| Variable | Description   |\n",
    "|------|------|\n",
    "| age | numerical value of call recipients age |\n",
    "| job | type of job |\n",
    "| marital | individuals marital status |\n",
    "| education | type of education |\n",
    "| default | has a credit in default? yes or no |\n",
    "| housing | has a housing loan? |\n",
    "| loan | has a personal loan? |\n",
    "| contact | contact communication type: cellular or telephone |\n",
    "| month | last contact month |\n",
    "| day_of_week | day of the week contacted |\n",
    "| duration | last contact duration in seconds |\n",
    "| campaign | number of contacts performed durint this campaign and for client |\n",
    "| pdays |number of days passed from last contact |\n",
    "| previous | number of contacts performed before this client |\n",
    "| poutcome | outcome of previous campaign |\n",
    "| emp.var.rate | employment variation rate |\n",
    "| cons.price.idx | consumer price index |\n",
    "| cons.conf.idx | consumer confidence index |\n",
    "| euribor3m | euribor 3 month rate |\n",
    "| nr.employed | number of employees |\n",
    "| y | The variable we are trying to predict, whether or not a client has subscribed a term deposit ('yes', 'no') |\n",
    " \n",
    "In the real world, this data would be used to determine which demographics to target in order to get the best response. Because making the call is a very easy task and has very little risk and no concequences when the caller does not sign up for a deposit, we will only consider true positives in our data set. These are the values that would help marketers determine who to target. The more data we recieve the better we would be able to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) # control the verbosity of tensor flow\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_roc(probas, y_true):\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "\n",
    "    classes = np.unique(y_true)\n",
    "    perclass_mean_tpr = 0.0\n",
    "    roc_auc = 0\n",
    "    for j in classes:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, probas[:, j], pos_label=j)\n",
    "        perclass_mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        perclass_mean_tpr[0] = 0.0\n",
    "        roc_auc += auc(fpr, tpr)\n",
    "\n",
    "    perclass_mean_tpr /= len(classes)\n",
    "    roc_auc /= len(classes)\n",
    "    mean_tpr += perclass_mean_tpr\n",
    "\n",
    "    plt.plot(mean_fpr,perclass_mean_tpr,'--',lw=1,label='Mean Class ROC for ensemble, AUC=%0.4f'\n",
    "                     %(roc_auc))\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('false positive rate')\n",
    "    plt.ylabel('true positive rate')\n",
    "    plt.title('ROC Curve')\n",
    "    \n",
    "def tp_score(y_true, yhat):\n",
    "    y_true = np.array(y_true) if type(y_true) == list else y_true\n",
    "    yhat = np.array(yhat) if type(yhat) == list else yhat\n",
    "    total1 = np.bincount(y_true)[1]\n",
    "    y_true1s = np.where(y_true == 1)[0]\n",
    "    yhat1s = np.where(yhat == 1)[0]\n",
    "    return len(np.intersect1d(y_true1s, yhat1s))/total1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      "age               41188 non-null int64\n",
      "job               41188 non-null object\n",
      "marital           41188 non-null object\n",
      "education         41188 non-null object\n",
      "default           41188 non-null object\n",
      "housing           41188 non-null object\n",
      "loan              41188 non-null object\n",
      "contact           41188 non-null object\n",
      "month             41188 non-null object\n",
      "day_of_week       41188 non-null object\n",
      "duration          41188 non-null int64\n",
      "campaign          41188 non-null int64\n",
      "pdays             41188 non-null int64\n",
      "previous          41188 non-null int64\n",
      "poutcome          41188 non-null object\n",
      "emp.var.rate      41188 non-null float64\n",
      "cons.price.idx    41188 non-null float64\n",
      "cons.conf.idx     41188 non-null float64\n",
      "euribor3m         41188 non-null float64\n",
      "nr.employed       41188 non-null float64\n",
      "y                 41188 non-null object\n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 6.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# http://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "df_train_orig = pd.read_csv('data/bank-additional-full.csv', sep=';') # read in the csv file\n",
    "\n",
    "df_test_orig = pd.read_csv('data/bank-additional.csv', sep=';')\n",
    "\n",
    "df_test_orig = df_test_orig.ix[1:]\n",
    "print(df_train_orig.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train = deepcopy(df_train_orig)\n",
    "df_test = deepcopy(df_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.855</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jun</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.465</td>\n",
       "      <td>-41.8</td>\n",
       "      <td>4.962</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jun</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.465</td>\n",
       "      <td>-41.8</td>\n",
       "      <td>4.959</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>93.200</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>4.191</td>\n",
       "      <td>5195.8</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>sep</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>2</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.199</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>0.884</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age       job  marital          education default  housing     loan  \\\n",
       "1   39  services   single        high.school      no       no       no   \n",
       "2   25  services  married        high.school      no      yes       no   \n",
       "3   38  services  married           basic.9y      no  unknown  unknown   \n",
       "4   47    admin.  married  university.degree      no      yes       no   \n",
       "5   32  services   single  university.degree      no       no       no   \n",
       "\n",
       "     contact month day_of_week ...  campaign  pdays  previous     poutcome  \\\n",
       "1  telephone   may         fri ...         4    999         0  nonexistent   \n",
       "2  telephone   jun         wed ...         1    999         0  nonexistent   \n",
       "3  telephone   jun         fri ...         3    999         0  nonexistent   \n",
       "4   cellular   nov         mon ...         1    999         0  nonexistent   \n",
       "5   cellular   sep         thu ...         3    999         2      failure   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "1          1.1          93.994          -36.4      4.855       5191.0  no  \n",
       "2          1.4          94.465          -41.8      4.962       5228.1  no  \n",
       "3          1.4          94.465          -41.8      4.959       5228.1  no  \n",
       "4         -0.1          93.200          -42.0      4.191       5195.8  no  \n",
       "5         -1.1          94.199          -37.5      0.884       4963.6  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning data - getting rid of null values\n",
    "\n",
    "df_train.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index()\n",
    "\n",
    "df_test.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index()\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Above we dropped all table data that was null-valued. \n",
    "\n",
    "The Bank Marketing Data has many categorical columns, so in this section we will prepare the data by transforming the categorical columns into integer numbers. Later we will one-hot encode all of these values, but for now a number representation will do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoders = dict() \n",
    "categorical_headers = ['job','marital','day_of_week',\n",
    "                       'poutcome','month','default','housing','loan','education']\n",
    "\n",
    "for col in categorical_headers+['y']:\n",
    "    df_train[col] = df_train[col].str.strip()\n",
    "    df_test[col] = df_test[col].str.strip()\n",
    "    \n",
    "    if col==\"y\":\n",
    "        tmp = LabelEncoder()\n",
    "        df_train[col] = tmp.fit_transform(df_train[col])\n",
    "        df_test[col] = tmp.transform(df_test[col])\n",
    "    else:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df_train[col+'_int'] = encoders[col].fit_transform(df_train[col])\n",
    "        df_test[col+'_int'] = encoders[col].transform(df_test[col])\n",
    "\n",
    "y_train = df_train['y'].values.astype(np.int)\n",
    "y_test = df_test['y'].values.astype(np.int)\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "\n",
    "#normalize continuous attributes\n",
    "numeric_headers = [\"age\", \"campaign\", \"pdays\", \"previous\"]\n",
    "\n",
    "for col in numeric_headers:\n",
    "    df_train[col] = df_train[col].astype(np.float)\n",
    "    df_test[col] = df_test[col].astype(np.float)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    df_train[col] = ss.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "    df_test[col] = ss.transform(df_test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>y</th>\n",
       "      <th>job_int</th>\n",
       "      <th>marital_int</th>\n",
       "      <th>day_of_week_int</th>\n",
       "      <th>poutcome_int</th>\n",
       "      <th>month_int</th>\n",
       "      <th>default_int</th>\n",
       "      <th>housing_int</th>\n",
       "      <th>loan_int</th>\n",
       "      <th>education_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.533034</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.628993</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.290186</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002309</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.533034</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age        job  marital    education  default housing loan    contact  \\\n",
       "0  1.533034  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1  1.628993   services  married  high.school  unknown      no   no  telephone   \n",
       "2 -0.290186   services  married  high.school       no     yes   no  telephone   \n",
       "3 -0.002309     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4  1.533034   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week      ...        y  job_int  marital_int  day_of_week_int  \\\n",
       "0   may         mon      ...        0        3            1                1   \n",
       "1   may         mon      ...        0        7            1                1   \n",
       "2   may         mon      ...        0        7            1                1   \n",
       "3   may         mon      ...        0        0            1                1   \n",
       "4   may         mon      ...        0        7            1                1   \n",
       "\n",
       "  poutcome_int  month_int  default_int  housing_int  loan_int  education_int  \n",
       "0            1          6            0            0         0              0  \n",
       "1            1          6            1            0         0              3  \n",
       "2            1          6            0            2         0              3  \n",
       "3            1          6            0            0         0              1  \n",
       "4            1          6            0            0         2              3  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    36548\n",
      "1     4640\n",
      "Name: y, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41188 entries, 0 to 41187\n",
      "Data columns (total 30 columns):\n",
      "age                41188 non-null float64\n",
      "job                41188 non-null object\n",
      "marital            41188 non-null object\n",
      "education          41188 non-null object\n",
      "default            41188 non-null object\n",
      "housing            41188 non-null object\n",
      "loan               41188 non-null object\n",
      "contact            41188 non-null object\n",
      "month              41188 non-null object\n",
      "day_of_week        41188 non-null object\n",
      "duration           41188 non-null int64\n",
      "campaign           41188 non-null float64\n",
      "pdays              41188 non-null float64\n",
      "previous           41188 non-null float64\n",
      "poutcome           41188 non-null object\n",
      "emp.var.rate       41188 non-null float64\n",
      "cons.price.idx     41188 non-null float64\n",
      "cons.conf.idx      41188 non-null float64\n",
      "euribor3m          41188 non-null float64\n",
      "nr.employed        41188 non-null float64\n",
      "y                  41188 non-null int64\n",
      "job_int            41188 non-null int64\n",
      "marital_int        41188 non-null int64\n",
      "day_of_week_int    41188 non-null int64\n",
      "poutcome_int       41188 non-null int64\n",
      "month_int          41188 non-null int64\n",
      "default_int        41188 non-null int64\n",
      "housing_int        41188 non-null int64\n",
      "loan_int           41188 non-null int64\n",
      "education_int      41188 non-null int64\n",
      "dtypes: float64(9), int64(11), object(10)\n",
      "memory usage: 9.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_train.y.value_counts())\n",
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Adding Crossed Columns\n",
    "\n",
    "Here we add some cross columns on strongly correlated data. We chose to cross the marital attribute with the housing loan attribute because of the people who are married they are more likely to have a housing loan. We also chose to cross the job attribute with education, thinking that a higher education will yield a better job. This will help us classify people who subscribe to the deposit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now lets create a wide model \n",
    "# https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "def process_input(df, label_header, categ_headers, numeric_headers, undersample=False, oversample=False):\n",
    "    # input: dataframe - what ever you need it to be\n",
    "    # output: (dict of feature columns as tensors), (labels as tensors)\n",
    "    \n",
    "    df_copy = deepcopy(df)\n",
    "    if undersample:\n",
    "        groups = df_copy.groupby('y')\n",
    "        if groups.size()[0] > groups.size()[1]:\n",
    "            df_copy = df_copy.drop(groups.get_group(0).index[groups.size()[1]:])\n",
    "        else:\n",
    "            df_copy = df_copy.drop(groups.get_group(1).index[groups.size()[0]:])\n",
    "    elif oversample:\n",
    "        groups = df_copy.groupby('y')\n",
    "        amt_to_add = groups.size()[0] - groups.size()[1]\n",
    "        df_copy = pd.concat([groups.get_group(1).sample(amt_to_add, replace=True), df_copy])\n",
    "    \n",
    "    # ========Process Inputs=========\n",
    "    # not much changes here - we leave the numerics as tc.constants\n",
    "    continuous_cols = {k: tf.constant(df_copy[k].values) for k in numeric_headers}\n",
    "      \n",
    "    # and we shift these tensors to be sparse one-hot encoded values\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "                              indices=[[i, 0] for i in range(df_copy[k].size)],\n",
    "                              values=df_copy[k].values,\n",
    "                              dense_shape=[df_copy[k].size, 1])\n",
    "                        for k in categ_headers}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(categorical_cols)\n",
    "    feature_cols.update(continuous_cols)\n",
    "    \n",
    "    # Convert the label column into a constant Tensor.\n",
    "    label = None\n",
    "    if label_header is not None:\n",
    "        label = tf.constant(df_copy[label_header].values)\n",
    "        \n",
    "    return feature_cols, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Columns Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_columns():\n",
    "    # let's create the column structure that the learn API can expect\n",
    "    \n",
    "    wide_columns = []\n",
    "    # add in each of the categorical columns\n",
    "    for col in categorical_headers:\n",
    "        wide_columns.append(layers.sparse_column_with_keys(col, keys=encoders[col].classes_))\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    cross_columns = [('marital','housing')]\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=encoders[tup[0]].classes_),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=encoders[tup[1]].classes_)],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "                        \n",
    "    return wide_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3635   32]\n",
      " [ 390   61]] 0.135254988914\n",
      "CPU times: user 1min 33s, sys: 4.96 s, total: 1min 38s\n",
      "Wall time: 44.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# setup \n",
    "wide_columns = setup_wide_columns()\n",
    "input_wrapper = lambda:process_input(df_train,'y',categorical_headers, numeric_headers)\n",
    "output_wrapper = lambda:process_input(df_test,None,categorical_headers, numeric_headers)\n",
    "\n",
    "clf = learn.LinearClassifier(feature_columns=wide_columns)\n",
    "\n",
    "# when we provide the process function, they expect us to control the mini-batch\n",
    "clf.fit(input_fn=input_wrapper, steps=300)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "yhat = [x for x in yhat]\n",
    "print(mt.confusion_matrix(y_test,yhat), tp_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Columns Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_deep_columns():\n",
    "    # now make up the deep columns\n",
    "    \n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_headers:\n",
    "        tmp = layers.sparse_column_with_keys(col, keys=encoders[col].classes_)\n",
    "        deep_columns.append(layers.embedding_column(tmp, dimension=8))\n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in numeric_headers:\n",
    "        deep_columns.append(layers.real_valued_column(col))\n",
    "                    \n",
    "    return deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3613   54]\n",
      " [ 355   96]] 0.212860310421\n",
      "CPU times: user 5min 13s, sys: 29.3 s, total: 5min 42s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# setup deep columns\n",
    "deep_columns = setup_deep_columns()\n",
    "clf = learn.DNNClassifier(feature_columns=deep_columns, hidden_units=[100, 50])\n",
    "\n",
    "clf.fit(input_fn=input_wrapper, steps=300)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "yhat = [x for x in yhat]\n",
    "print(mt.confusion_matrix(y_test,yhat), tp_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Wide & Deep Network\n",
    "\n",
    "We implemented a wide and deep network using tensorflow to classify our data, first with 2 layers: [100, 50]. We compared the ROC of our network under different sampling conditions to maximize our true positive rate and keep the false positive rate as low as possible. We began with the full and test data sets provided by UCI to have a simple idea of which sampling methods to use, and experimented with oversampling and undersampling the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_deep_columns():\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    wide_columns = []\n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_headers:\n",
    "        wide_columns.append(layers.sparse_column_with_keys(col, keys=encoders[col].classes_))\n",
    "        \n",
    "        dim = round(np.log2(len(encoders[col].classes_)))\n",
    "        deep_columns.append(layers.embedding_column(wide_columns[-1], dimension=dim))\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    cross_columns = [('marital','housing')]\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=encoders[tup[0]].classes_),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=encoders[tup[1]].classes_)],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in numeric_headers:\n",
    "        deep_columns.append(layers.real_valued_column(col))\n",
    "                    \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When we neither oversampled nor undersampled, our model had a low false positive rate but a very low true positive rate (.21). While our ROC graph indicates that we can have a good trade-off between our true positive rate and false positive rate, our model as is will not be good enough for predictive purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 6s, sys: 2min 12s, total: 24min 18s\n",
      "Wall time: 7min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "clf = learn.DNNLinearCombinedClassifier(\n",
    "                        linear_feature_columns=wide_columns,\n",
    "                        dnn_feature_columns=deep_columns,\n",
    "                        dnn_hidden_units=[100, 50])\n",
    "\n",
    "input_wrapper = lambda:process_input(df_train,'y',categorical_headers, numeric_headers)\n",
    "output_wrapper = lambda:process_input(df_test,None,categorical_headers, numeric_headers)\n",
    "clf_fit = clf.fit(input_fn=input_wrapper, steps=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3622   45]\n",
      " [ 355   96]] 0.212860310421\n"
     ]
    }
   ],
   "source": [
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "yhat = [x for x in yhat]\n",
    "print(mt.confusion_matrix(y_test,yhat), tp_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XlcVFX/wPHPHUABRWRRETErldyXJHcpBSm3NFNbfFIz\ns1IzS7NMfVxSs8XHJbVMDbOfZqWlWVpImeaSmvuSCeYOssoi+8w9vz94nEdicVCGAeb7fr14vZi5\n5977PSz3O+ece8/RlFIKIYQQ4h8Mtg5ACCFE2SQJQgghRIEkQQghhCiQJAghhBAFkgQhhBCiQJIg\nhBBCFEgShBBCiAJJghDl3rBhw9A0DU3TcHBwwM/PjyFDhnDlypV8Zc+ePcuwYcOoU6cOlSpVwtfX\nl6FDh3L27Nl8ZdPT05k1axYtWrTA1dUVT09P2rVrx4cffkh6enqRMZ08eZJnnnmGOnXqULlyZerV\nq8djjz3G9u3bS6zeQlibJAhRIXTp0oXo6GguXrzI2rVrOXz4MAMHDsxT5vDhwwQEBHD58mXWrl1L\nZGQk69atIyoqioCAAI4cOWIum5KSQqdOnfjwww8ZPXo0e/bs4eDBg0yYMIGvvvqKsLCwQmP56aef\nCAgIICoqihUrVnDq1Ck2b95M+/bteeGFF+6ontnZ2Xe0vxDFooQo54YOHaqCgoLyvLdo0SIFqOTk\nZKWUUrquqxYtWqjmzZurnJycPGVzcnJUs2bNVMuWLZWu60oppcaMGaOcnZ3V33//ne98uq6ra9eu\nFRhLWlqaqlmzpnrkkUcK3J6YmGj+HlCff/55nu1BQUFq6NCh5tf16tVTkydPVi+99JLy9PRUbdu2\nVU8//bTq3r17vmM/8sgjavDgwebXYWFhqmPHjsrZ2Vn5+vqqYcOGqfj4+ALjEqIg0oIQFU5UVBTr\n16/HwcEBBwcHAI4dO8axY8eYOHEijo6Oeco7OjoyceJEjh49yvHjx9F1nTVr1jB48GDuueeefMfX\nNI3q1asXeO6wsDBiY2OZPHlygds9PDyKXZ9FixZRs2ZN9u7dS2hoKEOHDuXnn38mKirKXCY6Oppt\n27YxZMgQAH755Rf69u3Lk08+ybFjx9i4cSPnz5+nf//+KJldR1jI8dZFhCj7fv31V6pWrYqu62Rk\nZAAwfvx4qlSpAsBff/0FQNOmTQvc/8b7f/31Fz4+Ply7do0mTZoUO44zZ84A3Na+hXnggQeYPn26\n+XWjRo3w8fFhzZo1vP766wCsWbMGHx8fgoODAZg5cyZjx47l5ZdfNu/32WefUa9ePY4ePUqrVq1K\nLD5RcUkLQlQI7dq148iRI+zfv5+pU6fSoUMHZs2adVvHupNP2Nb4dN62bds8rw0GA//617/4/PPP\nze99/vnnDB48GIMh91/6wIEDLFiwgKpVq5q/biStiIiIEo9RVEzSghAVgouLCw0aNACgWbNmnD17\nlpdffpnly5cD4O/vD8CJEydo3bp1vv1PnjwJwH333UeNGjXw8PDg1KlTxY7jvvvuA+DUqVN07ty5\nyLKapuVLKDk5OfnK3WgF3WzIkCG899575oH1Y8eO8cUXX5i367rOG2+8wTPPPJNvXx8fn1tXRAik\nBSEqqOnTpxMaGsoff/wBQMuWLWnWrBnvv/8+RqMxT1mj0cj7779PixYtaN68OQaDgaeffpo1a9Zw\n7ty5fMdWSpGcnFzgeUNCQqhZsyazZ88ucPu1a9fM39esWTPPOEJWVpbFSalp06a0adOGzz//nNWr\nV9OmTZs83VoBAQGcPHmSBg0a5PuqWrWqRecQQhKEqJAaNmxInz59zIPFmqaxatUqLly4QI8ePdi5\ncyeXLl3it99+o2fPnly8eJFVq1ahaRoAs2fPpmHDhrRv355PPvmEo0ePcu7cOb799lsefPDBQp9n\ncHV1ZdWqVWzfvp3g4GC2bt3K33//zfHjx/nggw9o3769uWxwcDAff/wxe/fu5cSJEwwbNqxYt7EO\nGTKEtWvX8sUXXzB06NA822bOnMmmTZt47bXXOHLkCGfPnuXHH3/kueeeM4/RCHFLNr2HSogSUNBt\nrkoptXv3bgWo7du3m987c+aMGjJkiKpdu7ZydHRUPj4+asiQISoyMjLf/tevX1czZsxQzZo1U87O\nzqp69erqgQceUO+8845KS0srMqZjx46pp59+WtWuXVs5OTmpunXrqt69e6utW7eay0RHR6vevXsr\nNzc35efnp5YuXVrgba5vv/12geeIi4tTTk5OysnJScXFxeXbvnPnThUUFKSqVq2qXF1dVaNGjdQr\nr7yS7zZfIQqjKSX3vAkhhMhPupiEEEIUSBKEEEKIAkmCEEIIUSBJEEIIIQokCUIIIUSByv2T1Dc/\naFQc3t7exMfHl3A0ZZvU2T5Ine3DndTZ19fXonLSghBCCFEgSRBCCCEKJAlCCCFEgSRBCCGEKJAk\nCCGEEAUqlbuYli5dyqFDh3B3d2fevHn5tiulCA0N5fDhw1SuXJlRo0Zx7733lkZoQgghClEqLYiH\nHnqIt956q9Dthw8f5urVqyxatIiRI0eyYsWK0ghLCCFEEUolQTRp0qTIRUr++OMPAgMD0TQNf39/\n0tLS8iysIoQQ9kjlZKOSElBXLqByctcKUYlxqPOls2xsmXhQLjExEW9vb/NrLy8vEhMT8fDwyFc2\nPDyc8PBwAObOnZtnv+JwdHS87X3LK6mzfZA6l22mawkYL52DrCyUbgKlUDlZVG7ZDkM1dzJ+C+P6\n/y1DpSSjjNkY3NzRqlaj+qR3cfT2JjPiONnHDuLYvovV61wmEkRxBAcHExwcbH59u08SypOX9kHq\nbB/KWp1V2nWIjULFRsOV86izpzE8Pgzt3vtQxw6gb9sETpXAwQE0Dc3BkeuePmi1fFF1G8Ar09Hc\n3NGcXcyrHCYBxMdDw+bQsDlGo9HqT1KXiQTh6emZp6IJCQl4enraMCIhhLg1ZcyBlGRITYLqXmju\nHqjTx9CXzIaavmg1a0NtPww9B0GdegBoLR7AocUDhR5Tq1oNqlYrrSoUqUwkiICAAH788Uc6depE\nREQErq6uBXYvCSFEaVHZWZB8DZITITsbvGui1fRFpSajvlqJijgFSYngVg3c3NF6DER7oDP4N8Ww\n8As0Q/l/iqBUEsSCBQs4deoUqampvPjiiwwaNAij0QhASEgIrVu35tChQ4wdO5ZKlSoxatSo0ghL\nCGHnVPI1iLqIir6EVqM2WvM2qPTr6JOez00K7h65X5Wd0doGotX0BWdXaNgUQ8+BUKtOvkSgGRxs\nVJuSVyoJYty4cUVu1zSNESNGlEYoQgg7pHQdMjPQXKugdB3947lwLgJysqHOXWg+daFm7dzCLlUw\nzFoGVd3M/f8305yc0AIfLuUa2EaZ6GISQog7pXST+dO7/vt2iI2GmCjU1SsQE4XWqi3aiPFoBgOG\nLiEwcDh418qXBDRNy+02EpIghBDlj4o4hYqNyh0DiI8h4dLf6JWdcXj9ndwCMdGAgmZtMAQ/CrV8\n0Vz/9yyW1jzANoGXM5IghBBljjLmwKVzubeJZmVARgZkZWB49Onc7Uf3QUoSuHtCvfpU6/skSVWr\nm/c39H3aVqFXKJIghBA2pzLS4e+/oEkrNE1D/d9HqAtn0XzqgIsrOLuAZw2UUmiahmHAs3n2d/L2\nRitDz0FUFJIghBA2oYw5qMO/o3b8COcj4O6GGO69D1xc0Ya+jKGAAWJRuiRBCCGsTqWmwMWzuWMB\n3rVQl8+hz58GteuiPdgD7ZVpaE6VzOULuntIlD5JEEKIEqfSrqN2bYNzZ3Inlku/DnfVx9DnKfCu\nBe6eGCbMRqtd19ahiiJIghBC3BGVfA115iREnspNAp2CAAWJcdCqLYZ+/4KatfM8UKa5uYObu+2C\nFhaRBCGEKDZlMqG+XI7682ju3UQNm6I1aIx2dwMAtCpuaE+NtG2Q4o5JghBCWEwlX0Nz90BzcEDd\n2whD5xDwq1ehppcQ/yMJQghRIKUUnD6G2vsLKiYKYqPAwQnDrI/QnF0wtH/I1iEKK5MEIYQokNq7\nHbXla7Sg3hi6PJw7V1G16nKHkR2RBCGEyG0tXDmP2r8TrV4DtDad0B7ogtb+Qek+smOSIISwYyru\nKmr/TtS+HZCVifZAF/D978I2Tk42jk7YmiQIIeyI0nWIj8ld6QzQVy1E862HYchoqN9Yuo9EHpIg\nhKjAlMkE5yNQZ07kroB29k/wvQvDxLlomva/2U+FKIAkCCEqAKUUpKdBQgyZEcdRdzdCc3JCrQ9F\n/XkUrVELDJ2CYdjLaNVkOV9hGUkQQpRj+o4fUWHfQlICODiCV00y69wFPneBkzvawGcxyCCzuE2S\nIIQoZ1R6GqRfR/OuhRbQGe2+ZlDdC83ZBYDq3t7E/3fqa7kDSdwJSRBClBPq8jnUrnDUvh1oPR5H\nC3kMrUpVqFL11jsLcRskQQhRxqkj+9C3fA1JiWidgjC89QFaDR9bhyXsgCQIIcoglX4dKrvkznmU\nkoSh5wBo8YB0GYlSJQlCiDJAKQWJ8ai/jqMO7oaIkxhefwfq3oMh8GFbhyfslCQIIUqZUgquXgFn\nFzQPL9Tl8+gLpoPSoUHj3CkuRoxHc3G1dajCzkmCEKKUKF2Hw7+jb10PqUlojw9DaxsItXwxvDEX\nvGvJk8yiTJEEIUQp0PftQH3/Jbi4Yug9CFq0Na+wpjlVAhl0FmWQJAghrEQZc9Ac/zvhXfI1DE+/\nAI1aSCtBlBuGWxcRQhSHSk1B/34d+psjchfaAQwh/dAat5TkIMoVaUEIUUJUVhbqh3WoHT+i3d8R\nw2tvo9XytXVYQtw2SRBClAClm9DfmYDmexeG6YvRPLxsHZIQd0wShBB3QOk6msGAZnDA8PJUNK+a\ntg5JiBJTagniyJEjhIaGous6QUFB9OvXL8/29PR0Fi1aREJCAiaTiT59+tC1a9fSCk+IYlPpaejL\n3sMQ1ButxQOSHESFUyoJQtd1Vq5cyZQpU/Dy8mLSpEkEBATg5+dnLvPjjz/i5+fHm2++SUpKCq+8\n8gpdunTB0VEaOaLsUfEx6ItnoTVsCk3vt3U4QlhFqVx9IyMj8fHxoVatWgB07NiRAwcO5EkQmqaR\nmZmJUorMzEyqVq2KwSA3WYmyRV04i/pxA+rUYbTeT6IFPyp3JokKq1QSRGJiIl5e/xu08/LyIiIi\nIk+ZRx55hPfee48XXniBjIwMXn311QITRHh4OOHh4QDMnTsXb2/v24rJ0dHxtvctr6TOt0fl5IBu\nQqvsTNaFM5haPYDzuH9jKKPTbMvv2T6URp3LTP/N0aNHqVevHv/+97+JiYnh7bffplGjRri65p2P\nJjg4mODgYPPrGwujFJf3TYuq2Aupc/Go1GTUb2Go7VvQ+g/B0KEr1POHev6kZ2RCRmYJR1sy5Pds\nH+6kzr6+lt1+XSoJwtPTk4SEBPPrhIQEPD0985TZvn07/fr1Q9M0fHx8qFmzJlFRUTRo0KA0QhTC\nTEWeQv26FXX8D7TW7XPvTrrrXluHJUSpK5VO/vr16xMdHU1sbCxGo5E9e/YQEBCQp4y3tzfHjx8H\nICkpiaioKGrWlLtCROlQaan/+37HT1CvAYY5n2AY9ookB2G3SqUF4eDgwPDhw5k9eza6rtO1a1fq\n1q1LWFgYACEhITz++OMsXbqU8ePHAzB48GCqVatWGuEJO6Yy01Gbv0Tt34FhznI0JycMz71q67CE\nKBM0pZSydRB3Iioq6rb2kz5L+1BYnZVSqAO/ob4OzZ0j6fGhaO4eNoiw5Mnv2T5UmDEIIcoSZcxB\nXzQTUpMxvPA6WoMmtg5JiDJJEoSwG0opNE1Dc3TCEPIYNG6J5iBrPAtRGHkSTdgFlRCH/v4k1JWL\nAGjN7pfkIMQtWNyCOHbsGLt37yY5OZk333yTs2fPkpGRQbNmzawZnxB3TJ04hB66AK17X/Cta+tw\nhCg3LGpBbN26leXLl1O7dm3+/PNPACpVqsS6deusGpwQd0IpRdqmL9BXLcIwciKGRx6XaTGEKAaL\nEsSWLVuYOnUq/fr1M09/UadOndu+g0iI0qD+2E3mLz9gmPQ+2n3S0hWiuCzqYsrIyMg354fRaJSZ\nVkWZpEwmNAcHtPs74PFgdxLTM2wdkhDlkkUtiMaNG7Nx48Y8723dupWmTZtaJSghboe6noK+4TP0\nma+g9NwkYXCtYuuwhCi3LEoQw4cPZ//+/YwePZrMzExeeeUV9u7dy9ChQ60dnxAWUUd+R5/6EqSn\nYXhlGppB7lAS4k5Z1Efk4eHBO++8w9mzZ4mLi8PLy4sGDRrIeg2iTFAHd6OvXYZh3Ay0ejK5oxAl\nxaIr/HvvvYemaTRo0IAOHTrg7++PwWDggw8+sHZ8QhRJKYU6eRjDK9MlOQhRwixqQZw8ebJY7wtR\nGlRWJlplZ7QhY2wdihAVUpEJ4ssvvwRy71i68f0NMTEx1KhRw3qRCVEIZTSivloJqcloL0y0dThC\nVFhFJogbi/zoup5nwR/InUlw0KBB1otMiAKo6ynoy94DRycMz0+wdThCVGhFJohRo0YB4O/vn2eZ\nTyFsQUVfRl/8Nlqr9miPD5E7lYSwMovGIG4kh4yMDFJTU7l5CYlatWpZJzIhbqKUQv9sEVrPgRg6\nyYcVIUqDRQni8uXLLFq0iAsXLuTb9s+xCSFKmtJNaAYHDBNmozk62TocIeyGRbe5rlixgqZNm/Lp\np5/i6upKaGgo3bt3Z/To0daOT9gxZcxB/3wp6vvcDyGSHIQoXRYliAsXLjB48GCqVKmCUgpXV1f+\n9a9/SetBWI1Kvob+n6mo5ES07v1sHY4QdsmiLiYnJydMJhOOjo64ubkRHx9PlSpVuH79urXjE3ZG\nRV9CffcF6uRhtKA+aH2eRJMn9oWwCYsSRKNGjdi7dy8PPfQQ7du3Z86cOTg5OclkfaLkaQbwb4rh\nX6PQqlS1dTRC2DWLEsRrr71m/v6pp56ibt26ZGZmEhgYaLXAhP3Q9+2Ao/sxjHwdzacOmk8dW4ck\nhMCCMQhd15k+fTo5OTm5OxgMBAYGEhISgrOzs9UDFBWXSr+OHroQtXkdWo8Btg5HCPEPt0wQBoOB\n2NjYPM8+CHGn1LED6NPHgpMThinz0OreY+uQhBD/YNHo34ABA1i+fDlxcXHoup7nS4jiUpkZ6D98\nheHZV3LHGpxdbR2SEKIAFo1BLFu2DICdO3fm2ya3ugpLqcvnwbcumrMLhjdzp5AXQpRdFiWIxYsX\nWzsOUYEpXUeFfYsK24hh/CyoU0+SgxDlgEUJQqb1FrdLpSajfzofMtIxTP4Pmpf8LQlRXliUIIS4\nHSryFPonH6C1exCt72A0R/lzE6I8kf9YUaKU0QjGHDRnFzAaMTwzCq15gK3DEkLchlJLEEeOHCE0\nNBRd1wkKCqJfv/zz65w8eZJVq1ZhMplwc3NjxowZpRWeuEMqJxu18yfUjxvQ+jyFFvgwWqMWtg5L\nCHEHipUg4uPjSUxMxN/fv1gn0XWdlStXMmXKFLy8vJg0aRIBAQH4+fmZy6SlpbFixQomT56Mt7c3\nycnJxTqHsA2l66hd21A/fAl17sYwZgpavQa2DksIUQIsShDx8fEsXLiQ8+fPA/D555/z+++/c+TI\nEV588cVb7h8ZGYmPj495caGOHTty4MCBPAli165dtGvXDm9vbwDc3d2LWxdhA2rTGtSfRzGMnIhW\nv5GtwxFClCCLEsQnn3xC69atmTFjBs899xwALVq0YPXq1RadJDExES8vL/NrLy8vIiIi8pSJjo7G\naDQyffp0MjIy6NmzJw8++GC+Y4WHhxMeHg7A3LlzzQmluBwdHW973/LKGnU2DRyKwbUKWqXKJXrc\nkiK/Z/sgdbbSOSwpFBkZyZtvvonhpmmXXV1dSU9PL7FATCYT586dY+rUqWRnZzNlyhQaNmyIr69v\nnnLBwcF51seOj4+/rfN5e3vf9r7lVUnVWV29gvp5M9rTL+Q+z5CSCqTeeYBWIL9n+yB1Lp5/XlcL\nY9FUG+7u7ly9ejXPe5cvX7Y4e3l6epKQkGB+nZCQgKenZ54yXl5etGzZEmdnZ6pVq0bjxo0LXOJU\n2JaKvow+bwrcda887CZEBWdRgujTpw/vvvsu27dvR9d1du3axfz58+nbt69FJ6lfvz7R0dHExsZi\nNBrZs2cPAQF5b30MCAjg9OnTmEwmsrKyiIyMpE4dmfa5LFFXLqL/ZwraY89g6BJi63CEEFZmURdT\nt27dcHNzIzw8HC8vL3bu3MkTTzxB27ZtLTqJg4MDw4cPZ/bs2ei6TteuXalbty5hYWEAhISE4Ofn\nR6tWrZgwYQIGg4Fu3bpx11133X7NRIlSVy6iz/832sBnMbTLPzYkhKh4NGXBPN66rucZfyhLoqKi\nbms/6bMsHnX6GCr5WrlLDvJ7tg9S5+KxdAzCohbE888/T4cOHejcuTONGsmtjPZE5WSjOVVCa9QC\nGXEQwr5Y1CyYMmUKzs7OLFy4kNGjR7N27VouXrxo7diEjalrCegzX0FdPmfrUIQQNmBRC+Kee+7h\nnnvu4V//+henTp1i165dzJgxAw8PDz744ANrxyhsQJ05kTvRXkhfND9Z7U0Ie1TsuZh8fX3x8/Pj\n7Nmz+W59FeWfUgr182bU1vUYhr+K1rS1rUMSQtiIRQkiLS2Nffv2sWvXLiIiImjRogV9+/bNd6uq\nqABOHETt/SV3xbcaPraORghhQxYliBdeeIH77ruPzp07M378eKpUqWLtuEQpUVevoHaFgVctDF17\nQpPWGJq2RjM42Do0IYSNWZQgPvzwQzw8PKwdiyhFKuIU+qY1EHURrWMQWtNWAGgOkhiEELkKTRCn\nTp2iSZMmAFy5coUrV64UWK5Zs2bWiUxYjTp7Gn3pHLQnnkML6Izm6GTrkIQQZVChCWLlypXMmzcP\ngI8++qjAMpqmsXjxYutEJqzHxRXD8+PRmsgAtBCicIUmiBvJAWDJkiWlEoywLj0lCaUUmu9d4CvT\nmAghimbRg3Lvvfdege/LMxDlh0qMJ2HCcPj7L1uHIoQoJyxKECdPnizW+6JsUakp6Aum4drjcVn1\nTQhhsSLvYvryyy8BMBqN5u9viImJoUaNGtaLTJQIlZmOvmgGWosHqPLYYDLsbEIzIcTtKzJB3Fjk\nR9f1PAv+QO5MgoMGDbJeZOKOKd2E/uHbaH53oz0+1NbhCCHKmSITxKhRowDw9/fPs8ynKB80gwOG\nx56BexvJ6m9CiGIrNEHExsZSs2ZNAJo3b05MTEyB5WrVqmWdyMRt07dvQfOuhda8DVqDJrYORwhR\nThWaICZMmMDq1asBGDt2bKEH+OfYhLAt9ddx1A9for1Z8J1nQghhqUITxI3kAJIEyguVmoK+cj6G\nYWPRvKVlJ4S4M7e1jmhMTAyxsbElHYu4A0op9M8WoT3QGa1ZG1uHI4SoACxKEAsWLOCvv3IfsNq+\nfTuvvfYa48eP55dffrFqcKIYThyEpES0x56xdSRCiArCogRx4sQJ6tevD8D333/P1KlTmTNnDhs3\nbrRqcKIYmrXB8NrbMvGeEKLEWDTdt9FoxNHRkcTERK5fv06jRrlP4yYnJ1s1OHFr+q5taPUbo9X2\nA1dZp0MIUXIsakHcfffdfPvtt6xfv577778fgMTERFxcXKwanCiaHvYt6vsvwbHYK8cKIcQtWZQg\nXnzxRS5evEh2djZPPPEEAGfOnKFz585WDU4UTCmF/s1nqN/CMEx8R5YGFUJYhUUfPX18fHjllVfy\nvNe+fXvat29vlaBE4ZTJhPp8CerKBQwT30Vzq2brkIQQFZTFfRPbt29n586dJCYm4unpSWBgIF27\ndrVmbKIgOVng7IJh/Cw0Z+niE0JYj0UJ4ptvvmHHjh306dMHb29v4uPj+e6777h27Rr9+/e3dox2\nTykFR/ZB4xZozq5oTz5v65CEEHbAogTx888/M3369DzTe7ds2ZJp06ZJgrAilRiHijiF2r8T4q5i\nqP0W+LjaOiwhhJ2wKEFkZWVRrVrevm43Nzeys7OtEpS9U6kp6HNfh/Q0aNgErdn9aJ1D0JzkGQch\nROmxKEG0atWKRYsWMXjwYLy9vYmLi+OLL76gZcuW1o7PLmlu1TCMmwHetWSabiGEzViUIIYPH86n\nn37KhAkTMJlMODg40LFjR5599llrx2dX1MWzkJ6G1qiF3LoqhLA5ixKEq6srY8aMYdSoUaSmpuLm\n5obBULx5/o4cOUJoaCi6rhMUFES/fv0KLBcZGcmUKVMYN26cXd1Gq04eRv90PobBL9o6FCGEAIpx\nm2t0dDR79+413+baoUMHateubdG+uq6zcuVKpkyZgpeXF5MmTSIgIAA/P7985dasWWNXXVcq/Trq\nq09Rp49hGDEerbH91F0IUbZZ1AzYtWsXEydO5MKFCzg7O3Px4kXeeOMNdu3aZdFJIiMj8fHxoVat\nWjg6OtKxY0cOHDiQr9zWrVtp165dvgHxikrFRqNPHwuOjhimL5LkIIQoUyxqQaxbt45JkybRpMn/\nlq/8888/Wbx4sUXTbSQmJuLl5WV+7eXlRURERL4y+/fvZ9q0aXz00UeFHis8PJzw8HAA5s6di7e3\ntyVVyMfR0fG29y0pytMT45T3cbr3vlI5X1moc2mTOtsHqbOVzmFJoYyMDPz9/fO817BhQzIzM0ss\nkFWrVjF48OBbjm0EBwcTHBxsfh0fH39b57vxwJ8tKGMOpCSjeXpDNS8opThsWWdbkTrbB6lz8fj6\n+lpUzqIE0bt3b7744gueeOIJKlWqRHZ2Nl999RW9e/e26CSenp4kJCSYXyckJODp6ZmnzNmzZ1m4\ncCEAKSkpHD58GIPBQNu2bS06R3mi1i2H7Gy04eNsHYoQQhTKogQRFhZGUlISW7ZsoWrVqly/fh2A\n6tWrExYWZi5XWNdQ/fr1iY6OJjY2Fk9PT/bs2cPYsWPzlFmyZEme79u0aVMhk4O+bwfqzEkMk963\ndShCCFEkixLEyy+/fEcncXBwYPjw4cyePRtd1+natSt169Y1J5eQkJA7On55obIyURs+w/DCRDQX\nmTJDCFEZLwcKAAAgAElEQVS2WZQgbh6cvl3333+/ebGhGwpLDKNHj77j85VF6sdv0Bo2QavfyNah\nCCHELRXvaTdx25RSkJSA1n+orUMRQgiLyFqVpUTTNLShd9ZVJ4QQpUlaEKVAnT2Nir5s6zCEEKJY\nipUgdF3n2rVr1oqlwlFKof+6BX3JbLhmX/doCyHKP4u6mNLS0lixYgW///47jo6OfP755/zxxx9E\nRkby5JNPWjvGckllZ6H+7yPUxbMY3ngXrZZlD6YIIURZYVELYvny5bi6urJ06VIcHXNzir+/P3v2\n7LFqcOWVUgp9xTxUViaGSe9LchBClEsWtSCOHz/OsmXLzMkBoFq1aiQnJ1stsHItKxPNwxttwLOy\nCpwQotyyqAXh6upKampqnvfi4+Px8PCwSlDlnebsguGpkZIchBDlmkUJIigoiHnz5nHixAmUUpw5\nc4YlS5bQvXt3a8dXrqisTPTVi1FZWbYORQgh7phFXUx9+/alUqVKrFy5EpPJxEcffURwcDA9e/a0\ndnzlhsrJRl/xHzRnF7TKlW0djhBC3DGLEoSmafTs2VMSQiFU+nX0JXPQqlVHGzLG1uEIIUSJsChB\nnDhxotBtzZo1K7FgyiN1LQF94XS0Ri3QBj2HVsy1uoUQoqyyKEH8cxrvlJQUjEYjXl5eLF682CqB\nlRspSWjdeqF1eRhN02wdjRBClBiLEsTNazVA7hPVGzZswMXFxSpBlSdavfpo9erbOgwhhChxt9Uf\nYjAY6N+/P5s2bSrpeMoV9ccuVLJMPSKEqJhuu8P82LFjt1w/uiJT6dfRP18CSrd1KEIIYRUWdTG9\n9NJLeV5nZ2eTnZ3NiBEjrBJUeaB2/4zW9H606l62DkUIIazitpYcrVy5MrVr18bV1T6XzVS6jtr+\nA4bhr9o6FCGEsJpbJghd1/nqq6+YPHkyTjJ1RK6Th8ClCsjSoUKICuyWgwgGg4HY2NjcJTMFAOry\nBbSg3nJbqxCiQrNolHnAgAEsX76cuLg4dF3P82WPDD0eR+vQzdZhCCGEVVk0BrFs2TIAdu7cmW/b\nl19+WbIRlWHq3BlwckLzu0daD0KICs+iBGH3T0sDKu06+rL3MDz1AvjdY+twhBDC6izqYtq7dy81\natTI97Vv3z5rx1dmqHWfoLVqh9byAVuHIoQQpcKiBLFhw4ZivV/RqOspqKP70foNtnUoQghRaors\nYroxi6uu6/lmdI2JibGbuZjUob3QpBWas30+9yGEsE9FJogbs7hmZ2fnmdFV0zSqV6/O8OHDrRtd\nWWEwYOgcYusohBCiVBWZIG7M4rp48WLGjLHfhXAMnWVpVSGE/bFoDMKek4NKjEeZTLYOQwghSp39\nTsdqIf2T9+DUYVuHIYQQpc6i5yBKwpEjRwgNDUXXdYKCgujXr1+e7b/99hubNm1CKYWLiwsjRozg\n7rvvLq3wCqQSYiHmCjRuadM4hBDCFkqlBaHrOitXruStt95i/vz57N69m8uXL+cpU7NmTaZPn868\nefN4/PHH+eSTT0ojtCKpP3ahte6A5iiTFAoh7E+pJIjIyEh8fHyoVasWjo6OdOzYkQMHDuQpc999\n91G1alUAGjZsSEJCQmmEViilFOr3X9Ee6GLTOIQQwlZKpYspMTERL6//Lazj5eVFREREoeV/+eUX\nWrduXeC28PBwwsPDAZg7dy7e3t63FZOjo2OR+2Yd3Mt1R0c8O3VFqyAr592qzhWR1Nk+SJ2tdA6r\nHv02nDhxgu3btzNz5swCtwcHBxMcHGx+HR8ff1vn8fb2LnJf5XcvvDiJhMTE2zp+WXSrOldEUmf7\nIHUuHl9fX4vKlcpHY09PzzxdRgkJCXh6euYrd+HCBZYtW8brr7+Om5tbaYRWKM3BAc2rhk1jEEII\nWyqVBFG/fn2io6OJjY3FaDSyZ88eAgIC8pSJj4/ngw8+YMyYMRZnN2tQugnTf6aiEuJsFoMQQpQF\npdLF5ODgwPDhw5k9eza6rtO1a1fq1q1LWFgYACEhIaxfv57r16+zYsUK8z5z584tjfDyUAd2QWYG\neNpXf6YQQvyTpsr5WqJRUVG3tV9B/XdKKfRpYzA8OQKtScGD5OWZ9NPaB6mzfagwYxDlRkwUZKRD\n41a2jkQIIWxOEsRN1J9H0Zq0kuVEhRCCMnibqy1pde+Gu+61dRhCCFEmSIK4idagia1DEEKIMkO6\nmP5LXUtARV++dUEhhLATkiD+S+0OR/32k63DEEKIMkMSxH+pP4+iyd1LQghhJgkCUFmZcOEs+De1\ndShCCFFmSIIAOHMS6tVHq+xs60iEEKLMkAQBqD+PoMmqcUIIkYfc5gpoj/QHTXKlEELcTBIEoFXz\nsHUIQghR5tj9x2aVGIe+O9zWYQghRJlj9wmC85Gow7/bOgohhChz7D5BqPiraDV8bB2GEEKUOXaf\nIIi7CpIghBAiH7tPECpWWhBCCFEQu08QxEVLC0IIIQpg97e5GqYugMqVbR2GEEKUOXafIDQXV1uH\nIIQQZZJddzGpy+fQfwuzdRhCCFEm2XULQp39C85HQJcQW4diNUopMjMz0XUdk8lEVlaWrUMqVTEx\nMVJnOyB1zk8phcFgwNnZGU3Tbuscdp0giIsG71q2jsKqMjMzcXJywtHREUdHRxwcHGwdUqmSOtsH\nqXPBjEYjmZmZuLi43NY57LuLKe4q1Kxt6zCsStd1HB3t+3OAEPbK0dERXddve3+7ThDEVfxnIG63\naSmEqBju5Bpg3wniWjzUqNgtCCGEuF12nSAM81ajValq6zAqvDp16vDyyy+bXxuNRpo3b86QIUOs\nfu6lS5cSGBhI9+7d6dmzJ19//TUAAwYM4OjRoyV+vkuXLlG/fn26d+/OQw89xNixY8nJyTFv379/\nP7169SIwMJDAwED+7//+L8/+X3/9Nd26dSMoKIiQkBA+/vjjfOdISEigd+/ehISEsG/fvhKvQ2nZ\ns2dPoX8D7dq1IzEx8baPfeNvbM6cOUUe958x/PLLL/To0YOHHnqIkJAQZsyYYfE5v/rqKzp16kSn\nTp346quvCiwzbdo0unfvTvfu3encuTONGzfOsz01NZU2bdowefJk83uhoaF06tSJOnXq5Il9yZIl\n5mN169aNunXrcu3aNYvjtYRdd05rBvsa1LIVV1dXTp8+TUZGBi4uLuzcuRMfH+t37a1evZodO3bw\nww8/4ObmRmpqKlu3brX6eevVq8e2bdswmUw8+eSTbN68mf79+xMbG8vo0aP59NNPad68OYmJiTz9\n9NP4+PgQHBzML7/8wooVK1i7di0+Pj5kZWWxfv36fMfftWsXjRo14oMPPrA4JpPJZFeDuDt37uTe\ne+/l+++/Z9KkSRZ1s5w+fZopU6awevVqGjRogMlkypfAC3Pt2jXmz5/Pli1b0DSNHj16EBISQvXq\n1fOUuznhfPrpp5w4cSLP9vfff5/27dvnee+BBx4gODiYAQMG5Hl/9OjRvPDCCwCEhYWxfPlyPDxK\ndm0bu21BZB8/KM9AlKJu3brx888/A7Bx40b69etn3paens5rr71Gr169CAkJ4aeffgJyP40/9thj\nPPzwwzz88MMcOHAAyP3UN2DAAJ5//nkCAwMZM2YMSql85/zwww959913cXNzA8DNzY1BgwblK/fm\nm2/So0cPunbtmueiO2fOHB566CGCg4OZOXMmAJs3b6Zbt24EBwfTv3//Iuvs4OBA69atuXr1KgCr\nVq1i0KBBNG/eHABPT08mT57MkiVLAFi8eDFTp041J8/KlSszePDgPMc8ceIEs2bN4qeffqJ79+5k\nZGSwceNGgoKC6NatG7NnzzaXbdiwITNmzCA4OJiDBw/mOc758+cZPHgwjzzyCI899hiRkZEAjBs3\njqlTp/Loo4/SoUMHvv/+eyD3lsr+/fubP63eaLns2LGDPn368PDDDzNy5EjS0tKA3E/q77zzDt27\nd6dHjx4cP36cp59+mo4dO7J69WpzHNevX+eZZ56hS5cuvPHGGwUOqG7YsIFevXrRvXt3Jk6ciMlk\nKvLnDrl/Y8899xy+vr788ccftywPua3NsWPH0qBBAyD39zd06FCL9t2xYwddunTBw8OD6tWr06VL\nF3799ddbxnjz/8GxY8eIi4sjMDAwT7lmzZpRt27dIo+1adOmPMcqKXbbgsg+dRSSk2wdRqnTv1uL\n2rwu3/uG90PRqnsVe7vW50kMjz59y/P27duX+fPnExwczJ9//smTTz5pvsgsXLiQTp068Z///Ifk\n5GR69epFly5d8Pb25osvvsDZ2Zm///6b0aNHm1sAJ06c4JdffsHHx4e+ffty4MAB2rZtaz5famoq\naWlp3H333RiNxiJje+ONN/Dw8MBkMvHEE09w6tQpfHx82Lp1Kzt37kTTNJKTkwFYsGABa9asoXbt\n2ub3CpOZmcmhQ4fMyeXMmTMMHDgwT5mWLVty5swZAP766y9atGhR5DGbNWvGhAkTOHbsGLNnz+bq\n1avMnj2bH3/8EXd3d5566im2bNlCSEgI6enptG7dmmnTpuU7zsSJE5k7dy733nsvhw4dYtKkSebu\nt5iYGDZu3EhkZCTPPvssvXv35ttvv+XBBx/klVdewWQykZGRQWJiIgsXLuTLL7/E1dWVJUuW8Mkn\nn/Dqq68C4Ovry7Zt25g2bRqvvvoqGzduJCsri27dupm7dY4cOcL27dvx8/Nj8ODBbNmyhd69e5vj\njIiI4LvvvmPjxo04OTkxadIkvvnmm3w/x3/+3Hft2sW7775LSkoKmzZt4oEHHijy53rj53/jE/k/\nffPNN3z00Uf53r/77rtZvnw5V69exdfX1/x+7dq1zR8MCnL58mUuXbpEp06dgNy7DWfOnMmiRYv4\n7bffbhnrzTIyMvj111+ZNWtWsfazhN0mCFNMFNS919ZhlDrDo09DERf0O91emCZNmnD58mU2bdpE\nt27d8mzbuXMn27ZtM/e3Z2VlceXKFWrVqsXkyZM5deoUBoOBv//+27xPq1atzP+QTZs25dKlS3kS\nRHFs3ryZNWvWYDKZiImJISIiAn9/fypXrsz48eMJDg4mODgYgICAAF599VX69OlDjx49CjzehQsX\n6N69O5cuXSIoKIgmTZrcVlyWOHr0KB06dMDLywuA/v378/vvvxMSEoKDgwO9evXKt09aWhoHDx7M\nczHMzs42f//II49gMBjw9/cnLi4OyP15jx8/HqPRyMMPP0yzZs3Ytm0bZ86coW/fvgDk5OTQpk0b\n83FCQnIfQG3cuDHp6elUrVqVqlWrUqlSJXNybdWqFfXq1QOgX79+7N+/P0+C2LVrF8ePH6dnz55A\n7sXf29u7yJ9JeHg4HTt2xMXFhZ49e7JgwQJmzJiBg4NDgV1NlnQ/9e/f/5YtxuLYtGkTvXr1Mnf7\nffbZZ3Tr1i1PkrFUWFgYAQEBJd69BKWYII4cOUJoaCi6rhMUFJSvOaSUIjQ0lMOHD1O5cmVGjRrF\nvfda7wJuunoFrU0nqx1f5BcSEsLMmTNZv359nsE0pRSffPKJuWl/w7x586hRowbbtm1D1/U8fw+V\nKlUyf+/g4JCvleDm5oarqyvnz5/Hz8+v0JguXrzIsmXL+OGHH6hevTrjxo0jMzMTR0dHfvjhB3bt\n2sUPP/xAaGgoX3/9Ne+++y6HDh3i559/pkePHmzduhVPT888x7wxBpGYmEjfvn0JCwsjJCSEhg0b\ncuzYMR5++GFz2WPHjuHv7w+Av78/x44do3PnzsX4qRaucuXKBY476LpOtWrV2LZtW4H73fyzvdF1\n1759ezZs2MDPP//Mq6++ysiRI3F3dycwMJClS5cWen7IvQDffEyDwWDuJvrnxfmfr5VSDBw4kEmT\nJt2qumabNm1i//79tGvXDsgdH9i9ezeBgYF4eHiQlJRk/p3d/L2/vz/Hjx+nadOm+Y55qxaEj48P\ne/bsMb8fHR1Nx44di4zx5u7AgwcPsm/fPj777DPS0tLIycmhSpUqvPXWW7es73fffWeV7iUopTEI\nXddZuXIlb731FvPnz2f37t1cvnw5T5nDhw9z9epVFi1axMiRI1mxYoVVYzJdvSLTfJeyJ554gtde\ney3fnRsPPvggoaGh5ovRjYG7lJQUatasicFgYMOGDRb1Pd9szJgxTJo0idTUVCD3k/ONbpQbUlNT\ncXFxoVq1asTFxbF9+3Zz2dTUVIKCgpg+fTqnTp0Ccvvu77//fl5//XW8vLyIiooq9Pyenp689dZb\nfPjhhwAMGzaMr776yly/xMRE5syZw0svvWSOd9asWcTGxgK5n+rXrl1bZB1btWrF77//TmJiIiaT\niY0bN9KhQ4ci93Fzc6Nu3bps3rwZyL0Inzx5ssh9Ll++TI0aNRg8eDBPP/00x48fp02bNhw4cIBz\n584BuWNJZ8+eLfI4/3TkyBEuXryIrut89913+VqBnTt35vvvvyc+Ph7IvdjfuHaMHTuWw4cP5ymf\nmprKvn372L9/P/v27WPfvn3MmTOHjRs3AtChQwc2bNgA5A7cf/PNN+YL+UsvvcSHH35oroOu6+bx\nkv79+7Nt27Z8X8uXLwdy/4Z37txJUlISSUlJ7Ny5kwcffLDAOkdGRpKcnExAQID5vcWLF3PgwAH2\n7dvH1KlTGTBggEXJISUlhd9//z3Ph46SVCotiMjISHx8fKhVK3dai44dO3LgwIE8n+z++OMPAgMD\n0TQNf39/0tLSuHbtmlWaTcqYAxpQ3fOWZUXJ8fX15bnnnsv3/rhx45g2bRrBwcHouk7dunVZvXo1\nQ4cOZeTIkaxfv56uXbvi6lq8mXeHDh1KZmYmPXv2NE838s8+5qZNm9KsWTMCAwPx9fU191Vfv36d\n4cOHk5WVhVLK3I8/a9Yszp07h1KKzp07F/hp82aPPPII8+bNY9++fbRr144PP/yQiRMncv36dZRS\njBgxwtwVExQURHx8PE8++SRKKTRN44knnijy+LVq1eKtt95i4MCBKKUICgqiR48etxx3Wbx4MZMm\nTWLhwoUYjUb69u1bZF327NnDxx9/jKOjI1WqVGHhwoV4eXkxf/58Ro8ebe6imjhxIvXr1y/y3Ddr\n2bIlkydP5vz583Ts2DFft52/vz8TJ07kqaeeQimFo6Mjs2fPxs/Pjz///NN8Tblh69atdOrUydx6\ngdyW66xZs8jKymLcuHFMmjTJ3GX40EMP8fjjjwO53aDTp09n9OjRZGRkoGmaudyteHh4MG7cOHOX\n3quvvmq+dr3//vu0bNnS/HvetGkTffv2tfgBtpUrV7J06VLi4uIIDg6mW7du5psptm7dSmBgYLH/\nNyylqYJu/yhhv//+O0eOHOHFF18EcvucIyIi8lws5s6dS79+/WjUqBEAM2fOZPDgwfn+2MLDwwkP\nDzfvc3PfaXE4Ojre8p+oIoiJicnzzyJERZCamsqrr75q9Z6GiiArKytfIr25y68o5W6Q+uYBQ8Dc\n9Cwub2/v2963PMnKyjL3Q9tLUryZ1LlicnFx4eOPPzbX0x7q/E+W1jkrKyvftc7SwfBSGYPw9PQk\nISHB/DohISHfwJ6np2eeShRURgghROkplQRRv359oqOjiY2NxWg0smfPnjwDNJB7++DOnTtRSnHm\nzBlcXV2tMv5gb0qhB1EIUYbdyTWgVLqYHBwcGD58OLNnz0bXdbp27UrdunUJC8t9kjkkJITWrVtz\n6NAhxo4dS6VKlRg1alRphFbhGQwGjEajTPkthB0yGo0YDLffDiiVQWprKuo2w6LYyxjEzSvKOTs7\n292qW5UrV5Y62wGpc35FrShn6RiEfKys4DRNM68mZS9J8WZSZ/sgdbYOu52sTwghRNEkQQghhCiQ\nJAghhBAFKveD1EIIIazDblsQb775pq1DKHVSZ/sgdbYPpVFnu00QQgghiiYJQgghRIEcpk+fPt3W\nQdiKNRckKqukzvZB6mwfrF1nGaQWQghRIOliEkIIUSBJEEIIIQpU4ediOnLkCKGhoei6TlBQUL7F\nvZVShIaGcvjwYSpXrsyoUaPKfV/mrer822+/sWnTJpRSuLi4MGLECO6++27bBFtCblXnGyIjI5ky\nZQrjxo2jffv2pRxlybKkzidPnmTVqlWYTCbc3NyYMWOGDSItObeqc3p6OosWLSIhIQGTyUSfPn3o\n2rWrjaK9c0uXLuXQoUO4u7szb968fNutfv1SFZjJZFJjxoxRV69eVTk5OWrChAnq0qVLecocPHhQ\nzZ49W+m6rv766y81adIkG0VbMiyp8+nTp1VqaqpSSqlDhw7ZRZ1vlJs+fbqaM2eO2rt3rw0iLTmW\n1Pn69etq3LhxKi4uTimlVFJSki1CLTGW1HnDhg3q888/V0oplZycrIYNG6ZycnJsEW6JOHnypDp7\n9qx67bXXCtxu7etXhe5iioyMxMfHh1q1auHo6EjHjh05cOBAnjJ//PEHgYGBaJqGv78/aWlpXLt2\nzUYR3zlL6nzfffdRtWpVABo2bJhntb/yyJI6Q+4C7+3ataNatWo2iLJkWVLnXbt20a5dO7y9vQFw\nd3e3RaglxpI6a5pGZmameZr7qlWr3tF6CLbWpEkT8/9qQax9/Sq/PzkLJCYm4uXlZX7t5eVFYmJi\nvjI3/oEKK1OeWFLnm/3yyy+0bt26NEKzGkt/z/v37yckJKS0w7MKS+ocHR3N9evXmT59Om+88QY7\nduwo7TBLlCV1fuSRR7hy5QovvPAC48eP59lnny3XCeJWrH39qvBjEKJwJ06cYPv27cycOdPWoVjd\nqlWrGDx4cIW+WPyTyWTi3LlzTJ06lezsbKZMmULDhg0tXiymPDp69Cj16tXj3//+NzExMbz99ts0\natQIV1dXW4dWLlXoBOHp6Zmn+yQhIQFPT898ZW5edKOgMuWJJXUGuHDhAsuWLWPSpEm4ubmVZogl\nzpI6nz17loULFwKQkpLC4cOHMRgMtG3btlRjLSmW1NnLyws3NzecnZ1xdnamcePGXLhwodwmCEvq\nvH37dvr164emafj4+FCzZk2ioqJo0KBBaYdbKqx9/arQH6fq169PdHQ0sbGxGI1G9uzZQ0BAQJ4y\nAQEB7Ny5E6UUZ86cwdXVFQ8PDxtFfOcsqXN8fDwffPABY8aMKbcXi5tZUuclS5aYv9q3b8+IESPK\nbXIAy/+2T58+jclkIisri8jISOrUqWOjiO+cJXX29vbm+PHjACQlJREVFUXNmjVtEW6psPb1q8I/\nSX3o0CE+++wzdF2na9eu9O/fn7CwMABCQkJQSrFy5UqOHj1KpUqVGDVqFPXr17dx1HfmVnX++OOP\n2bdvn7nv0sHBgblz59oy5Dt2qzrfbMmSJbRp06bc3+ZqSZ2/++47tm/fjsFgoFu3bvTq1cuWId+x\nW9U5MTGRpUuXmgdq+/btS2BgoC1DviMLFizg1KlTpKam4u7uzqBBgzAajUDpXL8qfIIQQghxeyp0\nF5MQQojbJwlCCCFEgSRBCCGEKJAkCCGEEAWSBCGEEKJAkiCEzUVFRfH6668zZMgQtmzZUmTZ2NhY\nBg0ahMlkKqXobt9rr73GyZMnC90+Z84cfv3119ILSIhikttchc199NFHuLi4MGzYsFuWjY2NZcyY\nMXzxxRc4ODhYP7gS8tVXX3H16lXGjh1r61DyKcuxCduSFoSwufj4eOrWrWvrMCqk8tDSEmVXhZ6L\nSZR9M2bM4NSpU5w+fZpVq1bx7rvvcvXqVdatW0dMTAyurq507dqVQYMGFbj/r7/+yvr160lJScHN\nzY0nn3ySLl26ALkz1W7evJmkpCQaNGjAyJEjqVGjRr5j3GiVjBw5kq+//hqlFL179+bRRx8FICcn\nhzVr1rB3714AOnTowODBg3FyciIlJYWlS5dy+vRpNE2jbt26TJ8+HYPBwOjRo3nhhRfQdZ1vv/0W\ngAMHDuDj48P777/P9OnT6dKlC4GBgTz//PPMnDmTu+66C8idL+qll15i6dKluLu7c/DgQdatW0dc\nXBx+fn48//zz1KtXr8CfyaBBgxg+fDhbtmzBZDKxZMkSQkND2b9/P+np6fj4+DBs2DAaN27MkSNH\nCowtPT2dzz77jMOHD6Npmvl3YE+THQoq9oJBonyYNm2aCg8PN78+ceKEunDhgjKZTOr8+fNqxIgR\nat++fUoppWJiYtTAgQOV0WhUGRkZasiQIerKlStKKaUSExPVxYsXlVJK7d+/X40ZM0ZdunRJGY1G\ntX79ejV58uQCz3/jmPPnz1cZGRnqwoULavjw4ero0aNKKaXWrVun3nrrLZWUlKSSk5PV5MmT1Rdf\nfKGUUmrNmjVq2bJlKicnR+Xk5KhTp04pXdeVUkqNGjXKfIwvv/xSLVy4sNB6L1myRK1du9a8bevW\nrWrWrFlKKaX+/vtv9dxzz6kzZ84ok8mktm/frkaNGqWys7MLrM/AgQPVzJkzVWpqqsrKylJKKbVj\nxw6VkpKijEaj+u6779SIESPM2wqK7b333lPLli1TGRkZKikpSb355psqLCyskN+gqKjk44Aoc5o2\nbcpdd92FwWCgXr16dOrUiVOnThVYVtM0Ll68SHZ2Nh4eHuauqm3btvHYY4/h5+eHg4MDjz32GOfP\nnycuLq7Q8w4cOBBnZ2fuuusuunbtyu7du4HchXcef/xx3N3dqVatGgMGDOC3334DcuexSkpKIj4+\nHkdHRxo3boymacWuc+fOndmzZ4/59e7du+ncuTMA4eHhBAcH07BhQwwGAw899BCOjo5EREQUerzH\nHnuMqlWrUqlSJQACAwNxc3PDwcGBPn36YDQaiYqKKnDfpKQkDh8+zLBhw3B2dsbd3Z1evXrliU/Y\nB+liEmVOREQEa9eu5eLFixiNRoxGY4ET6zk7OzNu3Dg2b97Mxx9/zH333ceQIUOoU6cOcXFxhIaG\nsnr1anN5pRSJiYkFdjMBeRaj8fb25uLFiwD59qlRo4Z5UZZHH32Ur7/+mlmzZgEQHBxc6HrYRWnW\nrJSqpB0AAAMnSURBVBlZWVlERETg7u7O+fPnzbPNxsfHs2PHDn788UdzeaPRWOTCMDfXBf43aV9i\nYiKappGRkUFqamqB+8bHx2MymRg5cqT5PaVUvmOKik8ShChzFi1axMMPP8ykSZOoVKkSq1atIiUl\npcCyrVq1olWrVmRnZ7Nu3TqWLVvGzJkz8fb2pn///ubxCEskJCSYp8OOj483T5vs6elJXFycuXUS\nHx9vnnPfxcWFIUOGMGTIEC5evMjMmTOpX78+zZs3z3PsW7UqDAYDHTp0YPfu3bi7u3P//ffj4uIC\n5F7s+/fvT//+/S2uy83n+/PPP/nuu+/497//jZ+fHwaDgWeffRb13xsY/xmbl5cXjo6OrFy5slzd\nKSZKnnQxiTInIyPD3D0SGRnJrl27CiyXlJTEgQMHyMzMxNHREWdnZ/PFrnv37mzcuJFLly4BkJ6e\nbh5kLsyGDRvIysri0qVL/Prrr3Ts2BGATp068c0335CSkkJKSgrr1683J56DBw9y9epVlFK4urpi\nMBgKTAbu7u7ExcWh63qh57/RzbRr1y5z9xJAUFAQ27ZtIyIiwrzW8qFDh8jIyCiyPjdkZGTg4OBA\ntWrV0HWd9evXk56eXmhsHh4etGzZktWrV5Oeno6u61y9erXQbj5RcUkLQpQ5I0aMYPXq1Xz66ac0\nadKEDh06kJaWlq+cUorvv/+exYsXo2kad999N88//zwAbdu2JTMzkwULFhAfH4+rqyvNmzenQ4cO\nhZ63SZMmjB07Fl3X6dOnDy1btgSgf//+pKenM2HCBADa/397d4grIQxFYfgoKvCwAEICCQnIYQMY\nBItAYEhI2AWbqGMlLKYOgcE+95LJXDHmiTfzf7ppm5rT3uSmj8fvbT6EIO+9rutSHMfquk5VVb3M\n3batjuPQOI5KkkTbtr2MyfNczjmd5/n0T3iWZZqmSd57hRAURZGKolBZlm+dZ9M0qutay7LIOae+\n75/+Mbb2Ns+z9n3Xuq6671tpmmoYhrfWw+egUQ5f77823wF/jRITAMBEQAAATJSYAAAmXhAAABMB\nAQAwERAAABMBAQAwERAAANMPPPsi6UyZ9joAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda158c3f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probas = clf.predict_proba(input_fn=output_wrapper)\n",
    "probas = np.array([x for x in probas])\n",
    "plot_roc(probas, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When using oversampling, we noticed a higher true positive rate (.54) even though the false positive rate is little higher, and we had a better AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-19-4626a0052feb>\", line 11, in <module>\n",
      "    clf_fit = clf.fit(input_fn=input_wrapper, steps=2500)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 280, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 426, in fit\n",
      "    loss = self._train_model(input_fn=input_fn, hooks=hooks)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 984, in _train_model\n",
      "    _, loss = mon_sess.run([model_fn_ops.train_op, model_fn_ops.loss])\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 462, in run\n",
      "    run_metadata=run_metadata)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 786, in run\n",
      "    run_metadata=run_metadata)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 744, in run\n",
      "    return self._sess.run(*args, **kwargs)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 891, in run\n",
      "    run_metadata=run_metadata)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 744, in run\n",
      "    return self._sess.run(*args, **kwargs)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 767, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 965, in _run\n",
      "    feed_dict_string, options, run_metadata)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\n",
      "    target_list, options, run_metadata)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/inspect.py\", line 1454, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/inspect.py\", line 1411, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/inspect.py\", line 666, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/inspect.py\", line 709, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/inspect.py\", line 678, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/conrad/miniconda3/lib/python3.6/inspect.py\", line 657, in getsourcefile\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "clf = learn.DNNLinearCombinedClassifier(\n",
    "                        linear_feature_columns=wide_columns,\n",
    "                        dnn_feature_columns=deep_columns,\n",
    "                        dnn_hidden_units=[100, 50])\n",
    "\n",
    "input_wrapper = lambda:process_input(df_train,'y',categorical_headers, numeric_headers, oversample=True)\n",
    "output_wrapper = lambda:process_input(df_test,None,categorical_headers, numeric_headers)\n",
    "clf_fit = clf.fit(input_fn=input_wrapper, steps=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "yhat = [x for x in yhat]\n",
    "print(mt.confusion_matrix(y_test,yhat), tp_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "probas = clf.predict_proba(input_fn=output_wrapper)\n",
    "probas = np.array([ x for x in probas])\n",
    "plot_roc(probas, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Undersampling gave us the best true positive score (.86), but we also have the worst false positive score. Our AUC is not as great as the other models, but with this high true positive score, we decided to examing the oversampling and undersampling models further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "clf = learn.DNNLinearCombinedClassifier(\n",
    "                        linear_feature_columns=wide_columns,\n",
    "                        dnn_feature_columns=deep_columns,\n",
    "                        dnn_hidden_units=[100, 50])\n",
    "\n",
    "input_wrapper = lambda:process_input(df_train,'y',categorical_headers, numeric_headers, undersample=True)\n",
    "output_wrapper = lambda:process_input(df_test,None,categorical_headers, numeric_headers)\n",
    "clf_fit = clf.fit(input_fn=input_wrapper, steps=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "yhat = [x for x in yhat]\n",
    "print(mt.confusion_matrix(y_test,yhat), tp_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "probas = clf.predict_proba(input_fn=output_wrapper)\n",
    "probas = np.array([ x for x in probas])\n",
    "plot_roc(probas, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to better determine the training model to use, we performed stratified shuffle split 3 times (in the interest of run time) on the full dataset for oversampling and undersampling models. This better defines our testing dataset than we were using before. By stratifying, we obtained similar ROC and mean AUC of .75 for both methods, but oversampling takes a much longer run time. \n",
    "\n",
    "For undersampling, we had a mean true positive rate of .603 for the three splits. For oversampling, we had a mean true positive rate of .539. Therefore, we decided to continue our analysis with the network fitted with undersampling and stratified training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "cv = StratifiedShuffleSplit(n_splits=K, test_size=.2)\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "clf = learn.DNNLinearCombinedClassifier(\n",
    "                        linear_feature_columns=wide_columns,\n",
    "                        dnn_feature_columns=deep_columns,\n",
    "                        dnn_hidden_units=[100, 50])\n",
    "\n",
    "for train_index, test_index in cv.split(df_train, df_train.y):\n",
    "    input_wrapper = lambda:process_input(df_train.loc[train_index],'y',categorical_headers, numeric_headers, undersample=True)\n",
    "    output_wrapper = lambda:process_input(df_train.loc[test_index],None,categorical_headers, numeric_headers)\n",
    "    \n",
    "    clf.fit(input_fn=input_wrapper, steps=2500)\n",
    "\n",
    "    yhat = clf.predict(input_fn=output_wrapper)\n",
    "    yhat = [x for x in yhat]\n",
    "    print(mt.confusion_matrix(y_train[test_index],yhat), tp_score(y_train[test_index],yhat))\n",
    "    \n",
    "    # ROC\n",
    "    probas = clf.predict_proba(input_fn=output_wrapper)\n",
    "    probas = np.array([ x for x in probas])\n",
    "    \n",
    "    perclass_mean_tpr = 0.0\n",
    "    roc_auc = 0\n",
    "    classes = np.unique(y_train[train_index])\n",
    "    # get the mean fpr and tpr, per class\n",
    "    for j in classes:\n",
    "        fpr, tpr, thresholds = roc_curve(y_train[test_index], probas[:, j], pos_label=j)\n",
    "        perclass_mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        perclass_mean_tpr[0] = 0.0\n",
    "        roc_auc += auc(fpr, tpr)\n",
    "        \n",
    "    perclass_mean_tpr /= len(classes)\n",
    "    roc_auc /= len(classes)\n",
    "    mean_tpr += perclass_mean_tpr\n",
    "    plt.plot(mean_fpr,perclass_mean_tpr,'--',lw=1,label='Mean Class ROC (area = %0.2f)'\n",
    "                   % (roc_auc))\n",
    "    \n",
    "mean_tpr /= K\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr,perclass_mean_tpr,'k-',lw=2,label='Total Mean ROC (area = %0.2f)'\n",
    "                   % (mean_auc))\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "cv = StratifiedShuffleSplit(n_splits=K, test_size=.2)\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "clf = learn.DNNLinearCombinedClassifier(\n",
    "                        linear_feature_columns=wide_columns,\n",
    "                        dnn_feature_columns=deep_columns,\n",
    "                        dnn_hidden_units=[100, 50])\n",
    "\n",
    "for train_index, test_index in cv.split(df_train, df_train.y):\n",
    "    input_wrapper = lambda:process_input(df_train.loc[train_index],'y',categorical_headers, numeric_headers, oversample=True)\n",
    "    output_wrapper = lambda:process_input(df_train.loc[test_index],None,categorical_headers, numeric_headers)\n",
    "    \n",
    "    clf.fit(input_fn=input_wrapper, steps=2500)\n",
    "\n",
    "    yhat = clf.predict(input_fn=output_wrapper)\n",
    "    # the output is now an iterable value, so we need to step over it\n",
    "    yhat = [x for x in yhat]\n",
    "    print(mt.confusion_matrix(y_train[test_index],yhat), tp_score(y_train[test_index],yhat))\n",
    "    \n",
    "    # ROC\n",
    "    probas = clf.predict_proba(input_fn=output_wrapper)\n",
    "    probas = np.array([ x for x in probas])\n",
    "    \n",
    "    perclass_mean_tpr = 0.0\n",
    "    roc_auc = 0\n",
    "    classes = np.unique(y_train[train_index])\n",
    "    # get the mean fpr and tpr, per class\n",
    "    for j in classes:\n",
    "        fpr, tpr, thresholds = roc_curve(y_train[test_index],\n",
    "                                         probas[:, j],\n",
    "                                         pos_label=j)\n",
    "        perclass_mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        perclass_mean_tpr[0] = 0.0\n",
    "        roc_auc += auc(fpr, tpr)\n",
    "        \n",
    "    perclass_mean_tpr /= len(classes)\n",
    "    roc_auc /= len(classes)\n",
    "    mean_tpr += perclass_mean_tpr\n",
    "    plt.plot(mean_fpr,perclass_mean_tpr,'--',lw=1,label='Mean Class ROC (area = %0.2f)'\n",
    "                   % (roc_auc))\n",
    "    \n",
    "mean_tpr /= K\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr,perclass_mean_tpr,'k-',lw=2,label='Total Mean ROC (area = %0.2f)'\n",
    "                   % (mean_auc))\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Altering Architecture\n",
    "\n",
    "We continued our analysis with 3 hidden layers of [100, 50, 20] and trained using stratified shuffle split with undersampling. The ROC and AUC of 0.75 are similar to our 2 hidden layers network, and a similar mean true positive rate of .611. In the future, we may increase the number of layers and change the layer sizes to more exhaustively determine a good network size, but the 2-layer and 3-layer networks appear to perform about the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# attempting to statify\n",
    "K = 3\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=K, test_size=.2)\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "clf = learn.DNNLinearCombinedClassifier(\n",
    "                        linear_feature_columns=wide_columns,\n",
    "                        dnn_feature_columns=deep_columns,\n",
    "                        dnn_hidden_units=[100, 50, 20])\n",
    "\n",
    "for train_index, test_index in cv.split(df_train, df_train.y):\n",
    "    input_wrapper = lambda:process_input(df_train.loc[train_index],'y',categorical_headers, numeric_headers, undersample=True)\n",
    "    output_wrapper = lambda:process_input(df_train.loc[test_index],None,categorical_headers, numeric_headers)\n",
    "    \n",
    "    clf.fit(input_fn=input_wrapper, steps=2500)\n",
    "\n",
    "    yhat = clf.predict(input_fn=output_wrapper)\n",
    "    # the output is now an iterable value, so we need to step over it\n",
    "    yhat = [x for x in yhat]\n",
    "    print(mt.confusion_matrix(y_train[test_index],yhat), tp_score(y_train[test_index],yhat))\n",
    "    \n",
    "    # ROC\n",
    "    probas = clf.predict_proba(input_fn=output_wrapper)\n",
    "    probas = np.array([ x for x in probas])\n",
    "    \n",
    "    perclass_mean_tpr = 0.0\n",
    "    roc_auc = 0\n",
    "    classes = np.unique(y_train[train_index])\n",
    "    # get the mean fpr and tpr, per class\n",
    "    for j in classes:\n",
    "        fpr, tpr, thresholds = roc_curve(y_train[test_index],\n",
    "                                         probas[:, j],\n",
    "                                         pos_label=j)\n",
    "        perclass_mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        perclass_mean_tpr[0] = 0.0\n",
    "        roc_auc += auc(fpr, tpr)\n",
    "        \n",
    "    perclass_mean_tpr /= len(classes)\n",
    "    roc_auc /= len(classes)\n",
    "    mean_tpr += perclass_mean_tpr\n",
    "    plt.plot(mean_fpr,perclass_mean_tpr,'--',lw=1,label='Mean Class ROC (area = %0.2f)'\n",
    "                   % (roc_auc))\n",
    "    \n",
    "mean_tpr /= K\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr,perclass_mean_tpr,'k-',lw=2,label='Total Mean ROC (area = %0.2f)'\n",
    "                   % (mean_auc))\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Adding Columns\n",
    "\n",
    "We then expanded the number of cross columns in our network, namely seeing if associations between marital status and education as well as job title and education have an impact on our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_deep_columns(cross_columns):\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    wide_columns = []\n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_headers:\n",
    "        wide_columns.append(\n",
    "            layers.sparse_column_with_keys(col, keys=encoders[col].classes_)\n",
    "        )\n",
    "        \n",
    "        dim = round(np.log2(len(encoders[col].classes_)))\n",
    "        deep_columns.append(\n",
    "            layers.embedding_column(wide_columns[-1], dimension=dim)\n",
    "        )\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=encoders[tup[0]].classes_),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=encoders[tup[1]].classes_)],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in numeric_headers:\n",
    "        deep_columns.append(\n",
    "            layers.real_valued_column(col)\n",
    "        )\n",
    "                    \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# attempting to stratify\n",
    "K = 3\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=K, test_size=.2)\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "\n",
    "\n",
    "for i in range(0, 3):\n",
    "    cross_columns = [('marital','housing'), ('marital','education'), ('job', 'education')]\n",
    "    del cross_columns[i]\n",
    "    print(cross_columns)\n",
    "    \n",
    "    wide_columns, deep_columns = setup_wide_deep_columns(cross_columns)\n",
    "    clf = learn.DNNLinearCombinedClassifier(\n",
    "                            linear_feature_columns=wide_columns,\n",
    "                            dnn_feature_columns=deep_columns,\n",
    "                            dnn_hidden_units=[100, 50, 20])\n",
    "\n",
    "    for train_index, test_index in cv.split(df_train, df_train.y):\n",
    "        input_wrapper = lambda:process_input(df_train.loc[train_index],'y',categorical_headers, numeric_headers, undersample=True)\n",
    "        output_wrapper = lambda:process_input(df_train.loc[test_index],None,categorical_headers, numeric_headers)\n",
    "\n",
    "        clf.fit(input_fn=input_wrapper, steps=2500)\n",
    "\n",
    "        yhat = clf.predict(input_fn=output_wrapper)\n",
    "        # the output is now an iterable value, so we need to step over it\n",
    "        yhat = [x for x in yhat]\n",
    "        print(mt.confusion_matrix(y_train[test_index],yhat), tp_score(y_train[test_index],yhat))\n",
    "\n",
    "        # ROC\n",
    "        probas = clf.predict_proba(input_fn=output_wrapper)\n",
    "        probas = np.array([ x for x in probas])\n",
    "\n",
    "        perclass_mean_tpr = 0.0\n",
    "        roc_auc = 0\n",
    "        classes = np.unique(y_train[train_index])\n",
    "        # get the mean fpr and tpr, per class\n",
    "        for j in classes:\n",
    "            fpr, tpr, thresholds = roc_curve(y_train[test_index],\n",
    "                                             probas[:, j],\n",
    "                                             pos_label=j)\n",
    "            perclass_mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "            perclass_mean_tpr[0] = 0.0\n",
    "            roc_auc += auc(fpr, tpr)\n",
    "\n",
    "        perclass_mean_tpr /= len(classes)\n",
    "        roc_auc /= len(classes)\n",
    "        mean_tpr += perclass_mean_tpr\n",
    "        plt.plot(mean_fpr,perclass_mean_tpr,'--',lw=1,label='Mean Class ROC (area = %0.2f)'\n",
    "                       % (roc_auc))\n",
    "\n",
    "    mean_tpr /= K\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr,perclass_mean_tpr,'k-',lw=2,label='Total Mean ROC (area = %0.2f)'\n",
    "                       % (mean_auc))\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Comparing to MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

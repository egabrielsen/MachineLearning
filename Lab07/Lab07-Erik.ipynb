{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Seven: Wide and Deep Network Architectures\n",
    "Erik Gabrielsen, Danh Nguyen, Conrad Appel\n",
    "\n",
    "In this lab, you will select a prediction task to perform on your dataset, evaluate two different deep learning architectures and tune hyper-parameters for each architecture. If any part of the assignment is not clear, ask the instructor to clarify. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (one per team) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report.\n",
    "\n",
    "## Dataset Selection\n",
    "\n",
    "Select a dataset identically to lab one. That is, the dataset must be table data. In terms of generalization performance, it is helpful to have a large dataset for building a wide and deep network. It is also helpful to have many different categorical features to create the embeddings and cross-product embeddings. It is fine to perform binary classification or multi-class classification.\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "### Preparation (40 points total)\n",
    "[10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). \n",
    "\n",
    "[10 points] Identify groups of features in your data that should be combined into cross-product features. \n",
    "\n",
    "[10 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\n",
    "[10 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "\n",
    "### Modeling (50 points total)\n",
    "[20 points] Create a combined wide and deep network to classify your data using tensorflow.\n",
    "\n",
    "[20 points] Investigate generalization performance by altering the number of layers. Try at least two different deep network architectures. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab. \n",
    "\n",
    "[10 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve.   \n",
    "\n",
    "### Exceptional Work (10 points total)\n",
    "You have free reign to provide additional analyses.\n",
    "One idea: Investigate which cross-product features are most important and hypothesize why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      "age               41188 non-null int64\n",
      "job               41188 non-null object\n",
      "marital           41188 non-null object\n",
      "education         41188 non-null object\n",
      "default           41188 non-null object\n",
      "housing           41188 non-null object\n",
      "loan              41188 non-null object\n",
      "contact           41188 non-null object\n",
      "month             41188 non-null object\n",
      "day_of_week       41188 non-null object\n",
      "duration          41188 non-null int64\n",
      "campaign          41188 non-null int64\n",
      "pdays             41188 non-null int64\n",
      "previous          41188 non-null int64\n",
      "poutcome          41188 non-null object\n",
      "emp.var.rate      41188 non-null float64\n",
      "cons.price.idx    41188 non-null float64\n",
      "cons.conf.idx     41188 non-null float64\n",
      "euribor3m         41188 non-null float64\n",
      "nr.employed       41188 non-null float64\n",
      "y                 41188 non-null object\n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 6.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# http://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "df_train_orig = pd.read_csv('/Users/erikgabrielsen/Downloads/bank-additional/bank-additional-full.csv', sep=';') # read in the csv file\n",
    "\n",
    "df_test_orig = pd.read_csv('/Users/erikgabrielsen/Downloads/bank-additional/bank-additional.csv', sep=';')\n",
    "\n",
    "df_test_orig = df_test_orig.ix[1:]\n",
    "print(df_train_orig.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "df_train = deepcopy(df_train_orig)\n",
    "df_test = deepcopy(df_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.855</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jun</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.465</td>\n",
       "      <td>-41.8</td>\n",
       "      <td>4.962</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jun</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.465</td>\n",
       "      <td>-41.8</td>\n",
       "      <td>4.959</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>93.200</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>4.191</td>\n",
       "      <td>5195.8</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>sep</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>2</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.199</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>0.884</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age       job  marital          education default  housing     loan  \\\n",
       "1   39  services   single        high.school      no       no       no   \n",
       "2   25  services  married        high.school      no      yes       no   \n",
       "3   38  services  married           basic.9y      no  unknown  unknown   \n",
       "4   47    admin.  married  university.degree      no      yes       no   \n",
       "5   32  services   single  university.degree      no       no       no   \n",
       "\n",
       "     contact month day_of_week ...  campaign  pdays  previous     poutcome  \\\n",
       "1  telephone   may         fri ...         4    999         0  nonexistent   \n",
       "2  telephone   jun         wed ...         1    999         0  nonexistent   \n",
       "3  telephone   jun         fri ...         3    999         0  nonexistent   \n",
       "4   cellular   nov         mon ...         1    999         0  nonexistent   \n",
       "5   cellular   sep         thu ...         3    999         2      failure   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "1          1.1          93.994          -36.4      4.855       5191.0  no  \n",
       "2          1.4          94.465          -41.8      4.962       5228.1  no  \n",
       "3          1.4          94.465          -41.8      4.959       5228.1  no  \n",
       "4         -0.1          93.200          -42.0      4.191       5195.8  no  \n",
       "5         -1.1          94.199          -37.5      0.884       4963.6  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning data - getting rid of null values\n",
    "\n",
    "df_train.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index()\n",
    "\n",
    "df_test.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index()\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "encoders = dict() \n",
    "categorical_headers = ['job','marital','day_of_week',\n",
    "                       'poutcome','month','default','housing','loan','education']\n",
    "\n",
    "for col in categorical_headers+['y']:\n",
    "    df_train[col] = df_train[col].str.strip()\n",
    "    df_test[col] = df_test[col].str.strip()\n",
    "    \n",
    "    if col==\"y\":\n",
    "        tmp = LabelEncoder()\n",
    "        df_train[col] = tmp.fit_transform(df_train[col])\n",
    "        df_test[col] = tmp.transform(df_test[col])\n",
    "    else:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df_train[col+'_int'] = encoders[col].fit_transform(df_train[col])\n",
    "        df_test[col+'_int'] = encoders[col].transform(df_test[col])\n",
    "\n",
    "\n",
    "#normalize continuous attributes\n",
    "numeric_headers = [\"age\", \"duration\", \"campaign\", \"pdays\",\"previous\"]\n",
    "\n",
    "for col in numeric_headers:\n",
    "    df_train[col] = df_train[col].astype(np.float)\n",
    "    df_test[col] = df_test[col].astype(np.float)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    df_train[col] = ss.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "    df_test[col] = ss.transform(df_test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>y</th>\n",
       "      <th>job_int</th>\n",
       "      <th>marital_int</th>\n",
       "      <th>day_of_week_int</th>\n",
       "      <th>poutcome_int</th>\n",
       "      <th>month_int</th>\n",
       "      <th>default_int</th>\n",
       "      <th>housing_int</th>\n",
       "      <th>loan_int</th>\n",
       "      <th>education_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.533034</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.628993</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.290186</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002309</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.533034</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age        job  marital    education  default housing loan    contact  \\\n",
       "0  1.533034  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1  1.628993   services  married  high.school  unknown      no   no  telephone   \n",
       "2 -0.290186   services  married  high.school       no     yes   no  telephone   \n",
       "3 -0.002309     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4  1.533034   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week      ...        y  job_int  marital_int  day_of_week_int  \\\n",
       "0   may         mon      ...        0        3            1                1   \n",
       "1   may         mon      ...        0        7            1                1   \n",
       "2   may         mon      ...        0        7            1                1   \n",
       "3   may         mon      ...        0        0            1                1   \n",
       "4   may         mon      ...        0        7            1                1   \n",
       "\n",
       "  poutcome_int  month_int  default_int  housing_int  loan_int  education_int  \n",
       "0            1          6            0            0         0              0  \n",
       "1            1          6            1            0         0              3  \n",
       "2            1          6            0            2         0              3  \n",
       "3            1          6            0            0         0              1  \n",
       "4            1          6            0            0         2              3  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    36548\n",
      "1     4640\n",
      "Name: y, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41188 entries, 0 to 41187\n",
      "Data columns (total 30 columns):\n",
      "age                41188 non-null float64\n",
      "job                41188 non-null object\n",
      "marital            41188 non-null object\n",
      "education          41188 non-null object\n",
      "default            41188 non-null object\n",
      "housing            41188 non-null object\n",
      "loan               41188 non-null object\n",
      "contact            41188 non-null object\n",
      "month              41188 non-null object\n",
      "day_of_week        41188 non-null object\n",
      "duration           41188 non-null float64\n",
      "campaign           41188 non-null float64\n",
      "pdays              41188 non-null float64\n",
      "previous           41188 non-null float64\n",
      "poutcome           41188 non-null object\n",
      "emp.var.rate       41188 non-null float64\n",
      "cons.price.idx     41188 non-null float64\n",
      "cons.conf.idx      41188 non-null float64\n",
      "euribor3m          41188 non-null float64\n",
      "nr.employed        41188 non-null float64\n",
      "y                  41188 non-null int64\n",
      "job_int            41188 non-null int64\n",
      "marital_int        41188 non-null int64\n",
      "day_of_week_int    41188 non-null int64\n",
      "poutcome_int       41188 non-null int64\n",
      "month_int          41188 non-null int64\n",
      "default_int        41188 non-null int64\n",
      "housing_int        41188 non-null int64\n",
      "loan_int           41188 non-null int64\n",
      "education_int      41188 non-null int64\n",
      "dtypes: float64(10), int64(10), object(10)\n",
      "memory usage: 9.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_train.y.value_counts())\n",
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job_int', 'marital_int', 'day_of_week_int', 'poutcome_int', 'month_int', 'default_int', 'housing_int', 'loan_int', 'education_int', 'age', 'duration', 'campaign', 'pdays', 'previous']\n"
     ]
    }
   ],
   "source": [
    "# let's start as simply as possible, without any feature preprocessing\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "\n",
    "## DONT DO THIS LOL\n",
    "# we will forego one-hot encoding right now and instead just scale all inputs\n",
    "feature_columns = categorical_headers_ints+numeric_headers\n",
    "X_train =  ss.fit_transform(df_train[feature_columns].values).astype(np.float32)\n",
    "X_test =  ss.transform(df_test[feature_columns].values).astype(np.float32)\n",
    "\n",
    "y_train = df_train['y'].values.astype(np.int)\n",
    "y_test = df_test['y'].values.astype(np.int)\n",
    "\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "tf.logging.set_verbosity(tf.logging.WARN) # control the verbosity of tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/_p/dtdq00kx1w53tc77v2y8q7fm0000gn/T/tmpau_8h52r\n",
      "WARNING:tensorflow:From /Users/erikgabrielsen/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "CPU times: user 7.07 s, sys: 389 ms, total: 7.46 s\n",
      "Wall time: 7.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we need to tell tensorflow how many inputs to expect and what the data types will be\n",
    "# for this early example, everything is just numeric, real valued\n",
    "features_tf = [layers.real_valued_column('', dimension=X_train.shape[1])]\n",
    "clf = SKCompat(# wrap with SKCompat for easy usage like sklearn\n",
    "            learn.DNNClassifier(hidden_units=[50], feature_columns=features_tf)\n",
    "        )\n",
    "\n",
    "clf.fit(X_train,y_train,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3594   73]\n",
      " [ 314  137]] 0.906022340942\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "# notice that the output needs some interpretation\n",
    "# as its not completely the same as sklearn\n",
    "yhat = yhat['classes']\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/_p/dtdq00kx1w53tc77v2y8q7fm0000gn/T/tmpsrkiu0v_\n",
      "WARNING:tensorflow:From /Users/erikgabrielsen/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "[[3565  102]\n",
      " [ 282  169]] 0.906750849927\n",
      "CPU times: user 9.81 s, sys: 744 ms, total: 10.6 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we can also custimize the classifier somewhat:\n",
    "clf = SKCompat(# wrap with SKCompat for easy usage like sklearn\n",
    "            learn.DNNClassifier(hidden_units=[50], \n",
    "                                feature_columns=features_tf,\n",
    "                                activation_fn=tf.nn.sigmoid \n",
    "                                # tf.tanh, tf.sigmoid, tf.nn.relu, tf.nn.softmax etc.\n",
    "                                )\n",
    "        )\n",
    "\n",
    "clf.fit(X_train,y_train,steps=1000)\n",
    "\n",
    "yhat = clf.predict(X_test)['classes']\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's start by just using the tflearn library out of the box on the data we have\n",
    "def my_model(features, targets, mode):\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    # =====SETUP ARCHITECTURE=====\n",
    "    # we can use functions from learn to add layers and complexity to the model\n",
    "    # pass features through one hidden layer with relu activation\n",
    "    features = layers.relu(features, num_outputs=50) \n",
    "    # now pass the features through a fully connected layer\n",
    "    features = layers.fully_connected(features, num_outputs=1) \n",
    "    # and pass them through a sigmoid activation\n",
    "    output_layer = tf.sigmoid(features) \n",
    "    # reshape the output to be one dimensional\n",
    "    predictions = tf.reshape(output_layer, [-1])\n",
    "    \n",
    "    # depending on the mode, we may not want to evaluate these\n",
    "    loss_mse = None\n",
    "    train_op = None\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        # =====LOSS=======\n",
    "        # we want to use MSE as our loss function, but could also choose \n",
    "        # cross entropy, or other objective functions here\n",
    "        loss_mse = tf.losses.mean_squared_error(targets, predictions) \n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        # =====OPTIMIZER PARAMS========\n",
    "        # now let's setup how we want thing to optimize \n",
    "        train_op = layers.optimize_loss(\n",
    "            loss=loss_mse, \n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adagrad', # adaptive gradient, so that the learning rate is not SO important\n",
    "            learning_rate=0.1)\n",
    "    \n",
    "    # what format to have the output in when calling clf.predict?\n",
    "    predictions_out = predictions>0.5\n",
    "    return model_fn_lib.ModelFnOps(\n",
    "      mode=mode, predictions=predictions_out, loss=loss_mse, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/_p/dtdq00kx1w53tc77v2y8q7fm0000gn/T/tmp70eitz4e\n",
      "[[3615   52]\n",
      " [ 343  108]] 0.904079650316\n",
      "CPU times: user 13 s, sys: 3.42 s, total: 16.4 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# now let train the estimator\n",
    "# we can wrap the tf estimator in the SKCompat class\n",
    "# so that we can use similar syntax to to SKLearn \n",
    "clf = SKCompat(learn.Estimator(model_fn=my_model))\n",
    "clf.fit(X_train, y_train, steps=5000, batch_size=32)\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test,yhat), \n",
    "      accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Handling different feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's start with the TF example (manipulated to work with new syntax)\n",
    "# https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "def process_input(df, label_header, categ_headers, numeric_headers):\n",
    "    # input: what ever you need it to be\n",
    "    # output: (dict of feature columns as tensors), (labels as tensors)\n",
    "    \n",
    "    # ========Process Inputs=========\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.expand_dims( # make it a column vector\n",
    "                            tf.cast( # cast to a float32\n",
    "                                tf.constant(df[k].values), \n",
    "                                tf.float32), \n",
    "                            1)\n",
    "                       for k in numeric_headers}\n",
    "    \n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored as constant Tensors (numeric)\n",
    "    # then use tensor flow to one hot encode them using the given number of classes \n",
    "    categorical_cols = {k: tf.one_hot(indices=tf.constant(df[k].values),\n",
    "                                      depth=len(encoders[k[:-4]].classes_)) \n",
    "                        for k in categ_headers}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(continuous_cols)\n",
    "    feature_cols.update(categorical_cols)\n",
    "    \n",
    "    # Convert the label column into a constant Tensor.\n",
    "    label = None\n",
    "    if label_header is not None:\n",
    "        label = tf.constant(df[label_header].values)\n",
    "        \n",
    "    return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def my_model(dict_features, targets, mode):\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    #=======DECODE FEATURES================\n",
    "    # now let's combine the tensors from the input dictionary\n",
    "    # into a list of the feature columns\n",
    "    features = []\n",
    "    for col in numeric_headers:\n",
    "        features.append(dict_features[col])\n",
    "    \n",
    "    # also add in the one hot encoded features\n",
    "    for col in categorical_headers_ints:\n",
    "        features.append(dict_features[col])\n",
    "    \n",
    "    # now we can just combine all the features together\n",
    "    features = tf.concat(values=features,axis=1)\n",
    "    \n",
    "    # =====SETUP ARCHITECTURE=====\n",
    "    # we can use functions from learn to add layers and complexity to the model\n",
    "    # pass features through one hidden layer with relu activation\n",
    "    features = layers.relu(features, num_outputs=50) \n",
    "    # now pass the features through a fully connected layer\n",
    "    features = layers.fully_connected(features, num_outputs=1) \n",
    "    # and pass them through a sigmoid activation\n",
    "    output_layer = tf.sigmoid(features) \n",
    "    # reshape the output to be one dimensional\n",
    "    predictions = tf.reshape(output_layer, [-1])\n",
    "    \n",
    "    # depending on the mode, we may not want to evaluate these\n",
    "    loss_mse = None\n",
    "    train_op = None\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        # =====LOSS=======\n",
    "        # we want to use MSE as our loss function\n",
    "        loss_mse = tf.losses.mean_squared_error(targets, predictions) \n",
    "    \n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        # =====OPTIMIZER PARAMS========\n",
    "        # now let's setup how we want thing to optimize \n",
    "        train_op = layers.optimize_loss(\n",
    "            loss=loss_mse, \n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adagrad', # adaptive gradient, so that the learning rate is not SO important \n",
    "            learning_rate=0.1)\n",
    "    \n",
    "    # what format to have the output in when calling clf.predict?\n",
    "    predictions_out = predictions>0.5\n",
    "    \n",
    "    return model_fn_lib.ModelFnOps(\n",
    "      mode=mode, predictions={'y':predictions_out}, loss=loss_mse, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/_p/dtdq00kx1w53tc77v2y8q7fm0000gn/T/tmp3lkbjhl8\n",
      "CPU times: user 1min 18s, sys: 10.2 s, total: 1min 29s\n",
      "Wall time: 39.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = learn.Estimator(model_fn=my_model)\n",
    "\n",
    "# when we provide the process function, they expect us to control the mini-batch\n",
    "clf.fit(input_fn=lambda:process_input(df_train,'y',categorical_headers_ints, numeric_headers), \n",
    "        steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3666    1]\n",
      " [ 450    1]] 0.89048081593\n"
     ]
    }
   ],
   "source": [
    "yhat = clf.predict(input_fn=lambda:process_input(df_test,None,categorical_headers_ints, numeric_headers))\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x['y'] for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Crossed Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets create a wide model \n",
    "# https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "def process_input(df, label_header, categ_headers, numeric_headers):\n",
    "    # input: what ever you need it to be\n",
    "    # output: (dict of feature columns as tensors), (labels as tensors)\n",
    "    \n",
    "    # ========Process Inputs=========\n",
    "    # not much changes here, except we leave the numerics as tc.constants\n",
    "    continuous_cols = {k: tf.constant(df[k].values) for k in numeric_headers}\n",
    "      \n",
    "    # and we shift these tensors to be sparse one-hot encoded values\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "                              indices=[[i, 0] for i in range(df[k].size)],\n",
    "                              values=df[k].values,\n",
    "                              dense_shape=[df[k].size, 1])\n",
    "                        for k in categ_headers}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(categorical_cols)\n",
    "    feature_cols.update(continuous_cols)\n",
    "    \n",
    "    # Convert the label column into a constant Tensor.\n",
    "    label = None\n",
    "    if label_header is not None:\n",
    "        label = tf.constant(df[label_header].values)\n",
    "        \n",
    "    return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_columns():\n",
    "    # let's create the column structure that the learn API can expect\n",
    "    \n",
    "    wide_columns = []\n",
    "    # add in each of the categorical columns\n",
    "    for col in categorical_headers:\n",
    "        wide_columns.append(layers.sparse_column_with_keys(col, keys=encoders[col].classes_))\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    cross_columns = [('marital','housing')]\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=encoders[tup[0]].classes_),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=encoders[tup[1]].classes_)],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "                        \n",
    "    return wide_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3635   32]\n",
      " [ 390   61]] 0.897523069451\n",
      "CPU times: user 2min 12s, sys: 15 s, total: 2min 27s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ignore all the deprecations that the learn API needs to deal with... ugh\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# setup \n",
    "wide_columns = setup_wide_columns()\n",
    "input_wrapper = lambda:process_input(df_train,'y',categorical_headers, numeric_headers)\n",
    "output_wrapper = lambda:process_input(df_test,None,categorical_headers, numeric_headers)\n",
    "\n",
    "clf = learn.LinearClassifier(feature_columns=wide_columns)\n",
    "\n",
    "# when we provide the process function, they expect us to control the mini-batch\n",
    "clf.fit(input_fn=input_wrapper, steps=300)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_deep_columns():\n",
    "    # now make up the deep columns\n",
    "    \n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_headers:\n",
    "        \n",
    "        tmp = layers.sparse_column_with_keys(col, keys=encoders[col].classes_)\n",
    "        \n",
    "        deep_columns.append(\n",
    "            layers.embedding_column(tmp, dimension=8)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in numeric_headers:\n",
    "        deep_columns.append(\n",
    "            layers.real_valued_column(col)\n",
    "        )\n",
    "                    \n",
    "    return deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3560  107]\n",
      " [ 269  182]] 0.908693540554\n",
      "CPU times: user 5min 57s, sys: 1min 5s, total: 7min 3s\n",
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# setup deep columns\n",
    "deep_columns = setup_deep_columns()\n",
    "clf = learn.DNNClassifier(feature_columns=deep_columns, hidden_units=[100, 50])\n",
    "\n",
    "clf.fit(input_fn=input_wrapper, steps=300)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_deep_columns():\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    wide_columns = []\n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_headers:\n",
    "        wide_columns.append(\n",
    "            layers.sparse_column_with_keys(col, keys=encoders[col].classes_)\n",
    "        )\n",
    "        \n",
    "        dim = round(np.log2(len(encoders[col].classes_)))\n",
    "        deep_columns.append(\n",
    "            layers.embedding_column(wide_columns[-1], dimension=dim)\n",
    "        )\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    cross_columns = [('marital','housing')]\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=encoders[tup[0]].classes_),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=encoders[tup[1]].classes_)],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in numeric_headers:\n",
    "        deep_columns.append(\n",
    "            layers.real_valued_column(col)\n",
    "        )\n",
    "                    \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "clf = learn.DNNLinearCombinedClassifier(\n",
    "                        linear_feature_columns=wide_columns,\n",
    "                        dnn_feature_columns=deep_columns,\n",
    "                        dnn_hidden_units=[100, 50])\n",
    "\n",
    "\n",
    "clf.fit(input_fn=input_wrapper, steps=2500)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

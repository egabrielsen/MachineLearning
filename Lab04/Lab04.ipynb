{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04\n",
    "\n",
    "## Conrad Appel, Erik Gabrielson, Danh Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation and Overview (30 points total)\n",
    "\n",
    "[5 points] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results.\n",
    "\n",
    "[10 points] (mostly the same processes as from lab one) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "[15 points] Divide you data into training and testing data using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. Argue for or against splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?  \n",
    "\n",
    "### Modeling (50 points total)\n",
    "\n",
    "[20 points] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template used in the course. You should add the following functionality to the logistic regression classifier:\n",
    "Ability to choose optimization technique when class is instantiated: either steepest descent, stochastic gradient descent, or Newton's method. Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1/L2 norm of the weights). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  \n",
    "\n",
    "[15 points] Train your classifier to achieve good generalization performance. That is, adjust the optimization technique and the value of the regularization term \"C\" to achieve the best performance on your test set. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?\n",
    "\n",
    "[15 points] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time, training iterations, and memory usage while training. Discuss the results. \n",
    "\n",
    "### Deployment (10 points total)\n",
    "\n",
    "Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?\n",
    "\n",
    "### Exceptional Work (10 points total)\n",
    "\n",
    "You have free reign to provide additional analyses.\n",
    "One idea: Make your implementation of logistic regression compatible with the GridSearchCV function that is part of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as p\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BinaryClassifierBase:\n",
    "    def __init__(self, eta, iterations=20, cost=0.001, norm=2):\n",
    "        self.eta = eta\n",
    "        self.cost = cost\n",
    "        self.iters = iterations\n",
    "        self.norm = norm\n",
    "    \n",
    "    def normalize(self, w, gradient):\n",
    "        # regularization (does both if 3)\n",
    "        if self.norm & 1: # L1 norm\n",
    "            gradient[1:] += -1 * w[1:] * self.cost\n",
    "        elif self.norm & 2: # L2 norm\n",
    "            gradient[1:] += -2 * w[1:] * self.cost\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.w_ = np.zeros((x.shape[1],1))\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(x,y)\n",
    "            self.normalize(self.w_, gradient)\n",
    "            self.w_ += gradient*self.eta\n",
    "    \n",
    "    def predict_proba(self,x):\n",
    "        return 1/(1+np.exp(-(x @ self.w_)))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return (self.predict_proba(x)>0.5)\n",
    "    \n",
    "    \n",
    "class BinaryStochDescClassifier(BinaryClassifierBase):\n",
    "    def _get_gradient(self, x, y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(x[idx]) # get y difference (now scalar)\n",
    "        gradient = x[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "class BinarySteepDescClassifier(BinaryClassifierBase):\n",
    "    def _get_gradient(self, x, y):\n",
    "        ydiff = y-self.predict_proba(x).ravel()\n",
    "        gradient = np.mean(x * ydiff[:,np.newaxis], axis=0)\n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "    \n",
    "class BinaryNewtonClassifier(BinaryClassifierBase):\n",
    "    def fit(self, x, y):\n",
    "        def obj_fn(w, x, y, c):\n",
    "            g = expit(x @ w)\n",
    "            return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + c*sum(w**2)\n",
    "        \n",
    "        def obj_grad(w, x, y, c):\n",
    "            g = expit(x @ w)\n",
    "            ydiff = y-g\n",
    "            gradient = np.mean(x * ydiff[:,np.newaxis], axis=0)\n",
    "            gradient = gradient.reshape(w.shape)\n",
    "            self.normalize(w, gradient)\n",
    "            return -gradient\n",
    "        \n",
    "        self.w_ = fmin_bfgs(obj_fn, \n",
    "                            np.zeros((x.shape[1], 1)), \n",
    "                            fprime=obj_grad, \n",
    "                            args=(x, y, self.cost), \n",
    "                            gtol=1e-03, \n",
    "                            maxiter=self.iters,\n",
    "                            disp=False).reshape((x.shape[1], 1))\n",
    "\n",
    "        \n",
    "class LogRegClassifier:\n",
    "    def __init__(self, eta, iterations=20, optimize='steepdesc', cost=0.001, norm=2):\n",
    "        typesofoptimize = {\n",
    "            'steepdesc': BinarySteepDescClassifier, \n",
    "            'stochdesc': BinaryStochDescClassifier, \n",
    "            'newton': BinaryNewtonClassifier\n",
    "        }\n",
    "        if optimize not in typesofoptimize.keys():\n",
    "            raise ValueError('optimize must be one of: ' + ' '.join(typesofoptimize.keys()))\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.optimize = optimize\n",
    "        self.classifier = typesofoptimize[optimize]\n",
    "        self.cost = cost\n",
    "        self.norm = norm\n",
    "        self.classifiers = [] # fill with binary classifiers during fit\n",
    "    \n",
    "    def _add_bias(self, x):\n",
    "        return np.hstack((np.ones((x.shape[0],1)),x))\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        Xb = self._add_bias(x)\n",
    "        classes = np.unique(y)\n",
    "        \n",
    "        for cl in classes:\n",
    "            cur_y = y==cl\n",
    "            cur_classifier = self.classifier(self.eta, self.iters, cost=self.cost, norm=self.norm)\n",
    "            cur_classifier.fit(x, cur_y)\n",
    "            self.classifiers.append(cur_classifier)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if not self.classifiers:\n",
    "            raise RuntimeError('Classifier not fit!')\n",
    "        \n",
    "        probabilities = []\n",
    "        for classifier in self.classifiers:\n",
    "            probabilities.append(classifier.predict_proba(x))\n",
    "        probabilities = np.hstack(probabilities)\n",
    "        return np.argmax(probabilities,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steep: 0.813333333333\n",
      "newton: 0.813333333333\n",
      "stoch: 0.666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "ds = load_iris()\n",
    "kwargs = {\n",
    "    'norm': 2,\n",
    "    'cost': .1,\n",
    "    'iterations': 500\n",
    "}\n",
    "regrs = {\n",
    "    'newton': LogRegClassifier(.1, optimize='newton', **kwargs),\n",
    "    'stoch': LogRegClassifier(.1, optimize='stochdesc', **kwargs), # Different results every time, random\n",
    "    'steep': LogRegClassifier(.1, optimize='steepdesc', **kwargs)\n",
    "}\n",
    "for regr in regrs.values():\n",
    "    regr.fit(ds.data, ds.target)\n",
    "\n",
    "for key, val in regrs.items():\n",
    "    res = val.predict(ds.data)\n",
    "    print(key + ': ' +str(accuracy_score(ds.target, res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

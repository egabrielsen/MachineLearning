{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Logistic Regression: 2015 American Community Survey\n",
    "\n",
    "## Conrad Appel, Erik Gabrielsen, Danh Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation and Overview (30 points total)\n",
    "\n",
    "[5 points] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results.\n",
    "\n",
    "[10 points] (mostly the same processes as from lab one) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "[15 points] Divide you data into training and testing data using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. Argue for or against splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?  \n",
    "\n",
    "### Modeling (50 points total)\n",
    "\n",
    "[20 points] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template used in the course. You should add the following functionality to the logistic regression classifier:\n",
    "Ability to choose optimization technique when class is instantiated: either steepest descent, stochastic gradient descent, or Newton's method. Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1/L2 norm of the weights). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  \n",
    "\n",
    "[15 points] Train your classifier to achieve good generalization performance. That is, adjust the optimization technique and the value of the regularization term \"C\" to achieve the best performance on your test set. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?\n",
    "\n",
    "[15 points] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time, training iterations, and memory usage while training. Discuss the results. \n",
    "\n",
    "### Deployment (10 points total)\n",
    "\n",
    "Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?\n",
    "\n",
    "### Exceptional Work (10 points total)\n",
    "\n",
    "You have free reign to provide additional analyses.\n",
    "One idea: Make your implementation of logistic regression compatible with the GridSearchCV function that is part of scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "In this lab, we are investigating the relationship among working individuals' income and personal attributes as provided by the [2015 American Community Survey](https://www.kaggle.com/census/2015-american-community-survey). We used a subset of the entire dataset, which gave us around 1.5 million individual records to work with. \n",
    "\n",
    "Specifically, we want to see how well we can classify people's income based on their age, race, education level, degree, where they were born, marital status, military status, and time when they arrive and leave work. \n",
    "\n",
    "Marketing and advertising researchers may be interested in our work. They can try to predict their target audience's income based on certain attributes to determine how they should price their products. They can use this to bolster their market research surveys and see how their samples compare to census data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "## redoing large files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%load_ext memory_profiler\n",
    "\n",
    "# %time df = pd.read_csv('~/Downloads/ss15pusb.csv') # read in the csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1528516, 12)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)\n",
    "df.shape\n",
    "\n",
    "new_df = df.filter(items=['AGEP','PINCP', 'MAR', 'MIL', 'SCHL','QTRBIR','JWAP','JWDP','YOEP','WAOB','RAC1P', 'FOD1P'])\n",
    "new_df.shape\n",
    "# new_df.to_csv('~/Downloads/ss15pusb3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "We began with approximately 1.5 million records from the dataset and kept approximately 12 attributes of the almost 300 attributes tracked by the survey, and wrote it out to a CSV for reuse later on. Of the attributes we examine, Age, Personal Income, and Year of entry to the US are ordinal data represented as integers. The remaining attributes were recorded as discrete values in the dataset, and we continue using their integer representation. For instance, Time of Arrival at work is recorded as 1 for times between 12:00 a.m. and 12:04 a.m. For classification purposes, we cut income into fifths, with 0 being the 20% of people with the lowest income. The full list of attributes we are working with is displayed in the table below.\n",
    "\n",
    "\n",
    "**Attribute**|**Description**|**Example**\n",
    ":-----:|:-----:|:-----:\n",
    "AGEP|Age| \n",
    "PINCP|Total person's income (signed)| 0 - bottom 20% of population, 4 - top 20% of population\n",
    "YOEP|Year of entry to the US| \n",
    "QTRBIR|Quarter of birth|1 - January through March, 2 -  April through June, etc.\n",
    "JWAP|Time of arrival at work |1 - 12:00 a.m. to 12:04 a.m., 2 - 12:05 a.m. to 12:09 a.m., etc.\n",
    "JWDP|Time of departure for work|1 - 12:00 a.m. to 12:29 a.m., 2 - 12:30 a.m. to 12:59 a.m., etc.\n",
    "WAOB|World area of birth|1 - US state, 2 - PR and US Island Areas, etc.\n",
    "RAC1P|Recoded detailed race code|1 - White alone, 2 - Black or African American alone, etc.\n",
    "MAR|Marital status|1 - Married, 2 - Widowed\n",
    "MIL|Military Service |1 - Now on active duty, 4 - Never served in the military\n",
    "SCHL|Educational attainment|23 - Professional degree beyond a bachelor's degree, 24 - Doctorate degree, etc.\n",
    "FOD1P|Recoded field of degree|1904 - advertising and public relations, 2001 - communication technologies, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 s, sys: 264 ms, total: 2.1 s\n",
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%time new_df = pd.read_csv('~/Downloads/ss15pusb3.csv') # read in the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "\n",
    "We delete all of the records younger than 18 to exclude the set to working individuals. Time of arival and departure are `NAN` for those who are not working, so we map this to a discrete value of 0. We also map the `NAN` values for those who do not have a college degree and were born in the united states to 0 for Field of Degree and Year of Entry, respectively. We then use quantile cut divide income into fifths for classification purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JWAP', 'JWDP', 'YOEP', 'FOD1P']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete younger than 18\n",
    "new_df = new_df[new_df.AGEP >= 18]\n",
    "\n",
    "# find null columns\n",
    "new_df.columns[new_df.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_df[new_df['JWAP'].isnull()]\n",
    "# clean out JWAP and JWDP. They are ordinal variables, create a new one to classify them\n",
    "\n",
    "df_grouped = new_df.groupby(by=['JWAP', 'JWDP'])\n",
    "# new_df = new_df.groupby(by=['JWAP', 'JWDP']).transform('median')\n",
    "\n",
    "new_df['JWAP'].fillna(0, inplace=True)\n",
    "new_df['JWDP'].fillna(0, inplace=True)\n",
    "new_df['FOD1P'].fillna(0, inplace=True)\n",
    "\n",
    "## fill null values in YOEP (Year of entrance into US)\n",
    "new_df['YOEP'].fillna(0, inplace=True)\n",
    "\n",
    "new_df.columns[new_df.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>PINCP</th>\n",
       "      <th>MAR</th>\n",
       "      <th>MIL</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>QTRBIR</th>\n",
       "      <th>JWAP</th>\n",
       "      <th>JWDP</th>\n",
       "      <th>YOEP</th>\n",
       "      <th>WAOB</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>FOD1P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>9100.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>25300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>82.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGEP    PINCP  MAR  MIL  SCHL  QTRBIR  JWAP  JWDP  YOEP  WAOB  RAC1P  \\\n",
       "0    77   9100.0    3  4.0  16.0       1   0.0   0.0   0.0     1      1   \n",
       "1    30   5000.0    1  4.0  11.0       2   0.0   0.0   0.0     1      1   \n",
       "2    65  20000.0    1  4.0  22.0       4   0.0   0.0   0.0     1      1   \n",
       "3    69  11000.0    1  4.0  16.0       2   0.0   0.0   0.0     1      1   \n",
       "4    24  25300.0    1  4.0  21.0       2  82.0  34.0   0.0     1      1   \n",
       "\n",
       "    FOD1P  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2  5404.0  \n",
       "3     0.0  \n",
       "4  3600.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[pd.isnull(new_df).any(axis=1)]\n",
    "new_df.dropna(inplace=True)\n",
    "new_df.drop(new_df.columns[0], axis=1, inplace=True)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# new_df['PINCP'] = pd.cut(new_df.PINCP,[0,20000,60000,100000,150000,1e8],5,labels=[0,1,2,3,4]) # this creates a new variable\n",
    "\n",
    "new_df['PINCP'] = pd.qcut(new_df.PINCP, 5, labels=[0,1,2,3,4])\n",
    "# 4. drop rows that still had missing values after grouped imputation\n",
    "new_df.dropna(inplace=True)\n",
    "new_df['PINCP'] = new_df['PINCP'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251045\n",
      "241657\n",
      "234571\n",
      "241334\n",
      "238193\n"
     ]
    }
   ],
   "source": [
    "for value in new_df.PINCP.unique(): \n",
    "    print(len(new_df[new_df['PINCP'] == value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1206800 entries, 0 to 1528515\n",
      "Data columns (total 12 columns):\n",
      "AGEP      1206800 non-null int64\n",
      "PINCP     1206800 non-null int64\n",
      "MAR       1206800 non-null int64\n",
      "MIL       1206800 non-null float64\n",
      "SCHL      1206800 non-null float64\n",
      "QTRBIR    1206800 non-null int64\n",
      "JWAP      1206800 non-null float64\n",
      "JWDP      1206800 non-null float64\n",
      "YOEP      1206800 non-null float64\n",
      "WAOB      1206800 non-null int64\n",
      "RAC1P     1206800 non-null int64\n",
      "FOD1P     1206800 non-null float64\n",
      "dtypes: float64(6), int64(6)\n",
      "memory usage: 119.7 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>PINCP</th>\n",
       "      <th>MAR</th>\n",
       "      <th>MIL</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>QTRBIR</th>\n",
       "      <th>JWAP</th>\n",
       "      <th>JWDP</th>\n",
       "      <th>YOEP</th>\n",
       "      <th>WAOB</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>FOD1P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>82.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGEP  PINCP  MAR  MIL  SCHL  QTRBIR  JWAP  JWDP  YOEP  WAOB  RAC1P   FOD1P\n",
       "0    77      1    3  4.0  16.0       1   0.0   0.0   0.0     1      1     0.0\n",
       "1    30      0    1  4.0  11.0       2   0.0   0.0   0.0     1      1     0.0\n",
       "2    65      2    1  4.0  22.0       4   0.0   0.0   0.0     1      1  5404.0\n",
       "3    69      1    1  4.0  16.0       2   0.0   0.0   0.0     1      1     0.0\n",
       "4    24      2    1  4.0  21.0       2  82.0  34.0   0.0     1      1  3600.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.info()\n",
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Sets\n",
    "\n",
    "We used scikit learn to randomly select 80% of our dataset for training classifiers and 20% of our dataset for testing. Since we used quantile cut to evenly split our dataset and balance the income classes, the 80-20 split is appropriate for our purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(1206800, n_iter=1, test_size=0.2, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "df_imputed = new_df.copy()\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'PINCP' in new_df:\n",
    "    y = df_imputed['PINCP'].values # get the labels we want\n",
    "    del df_imputed['PINCP'] # get rid of the class label\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 1\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1636.75 MiB, increment: 165.00 MiB\n",
      "peak memory: 1195.41 MiB, increment: 14.75 MiB\n",
      "====Iteration 0  ====\n",
      "accuracy 0.428231687106\n",
      "confusion matrix\n",
      " [[32762  8391  2258  1583  3482]\n",
      " [13166 21791  4981  4031  6235]\n",
      " [ 6602 14494  7076  8192 10406]\n",
      " [ 4037  7909  6165 10163 19449]\n",
      " [ 3219  3452  3086  6864 31566]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object: \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    %memit lr_clf.fit(X_train,y_train)  # train object\n",
    "    %memit y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for income bracket  0 = 0.675839590725\n",
      "Accuracy for income bracket  1 = 0.434049079755\n",
      "Accuracy for income bracket  2 = 0.151293564251\n",
      "Accuracy for income bracket  3 = 0.212958112441\n",
      "Accuracy for income bracket  4 = 0.655072944985\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix accuracy per class\n",
    "for i in range(0,len(conf)):\n",
    "    print(\"Accuracy for income bracket \", i, \"=\",conf[i][i]/sum(conf[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as p\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BinaryClassifierBase:\n",
    "    def __init__(self, eta, iterations=20, cost=0.001, norm=2):\n",
    "        self.eta = eta\n",
    "        self.cost = cost\n",
    "        self.iters = iterations\n",
    "        self.norm = norm\n",
    "    \n",
    "    def normalize(self, w, gradient):\n",
    "        # regularization (does both if 3)\n",
    "        if self.norm & 1: # L1 norm\n",
    "            gradient[1:] += -1 * w[1:] * self.cost\n",
    "        elif self.norm & 2: # L2 norm\n",
    "            gradient[1:] += -2 * w[1:] * self.cost\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.w_ = np.zeros((x.shape[1],1))\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(x,y)\n",
    "            self.normalize(self.w_, gradient)\n",
    "            self.w_ += gradient*self.eta\n",
    "    \n",
    "    def predict_proba(self,x):\n",
    "        return 1/(1+np.exp(-(x @ self.w_)))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return (self.predict_proba(x)>0.5)\n",
    "    \n",
    "    \n",
    "class BinaryStochDescClassifier(BinaryClassifierBase):\n",
    "    def _get_gradient(self, x, y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(x[idx]) # get y difference (now scalar)\n",
    "        gradient = x[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "class BinarySteepDescClassifier(BinaryClassifierBase):\n",
    "    def _get_gradient(self, x, y):\n",
    "        ydiff = y-self.predict_proba(x).ravel()\n",
    "        gradient = np.mean(x * ydiff[:,np.newaxis], axis=0)\n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "    \n",
    "class BinaryNewtonClassifier(BinaryClassifierBase):\n",
    "    def fit(self, x, y):\n",
    "        def obj_fn(w, x, y, c):\n",
    "            g = expit(x @ w)\n",
    "            return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + c*sum(w**2)\n",
    "        \n",
    "        def obj_grad(w, x, y, c):\n",
    "            g = expit(x @ w)\n",
    "            ydiff = y-g\n",
    "            gradient = np.mean(x * ydiff[:,np.newaxis], axis=0)\n",
    "            gradient = gradient.reshape(w.shape)\n",
    "            self.normalize(w, gradient)\n",
    "            return -gradient\n",
    "        \n",
    "        self.w_ = fmin_bfgs(obj_fn, \n",
    "                            np.zeros((x.shape[1], 1)), \n",
    "                            fprime=obj_grad, \n",
    "                            args=(x, y, self.cost), \n",
    "                            gtol=1e-03, \n",
    "                            maxiter=self.iters,\n",
    "                            disp=False).reshape((x.shape[1], 1))\n",
    "\n",
    "        \n",
    "class LogRegClassifier:\n",
    "    def __init__(self, eta, iterations=20, optimize='steepdesc', cost=0.001, norm=2):\n",
    "        typesofoptimize = {\n",
    "            'steepdesc': BinarySteepDescClassifier, \n",
    "            'stochdesc': BinaryStochDescClassifier, \n",
    "            'newton': BinaryNewtonClassifier\n",
    "        }\n",
    "        if optimize not in typesofoptimize.keys():\n",
    "            raise ValueError('optimize must be one of: ' + ' '.join(typesofoptimize.keys()))\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.optimize = optimize\n",
    "        self.classifier = typesofoptimize[optimize]\n",
    "        self.cost = cost\n",
    "        self.norm = norm\n",
    "        self.classifiers = [] # fill with binary classifiers during fit\n",
    "    \n",
    "    def _add_bias(self, x):\n",
    "        return np.hstack((np.ones((x.shape[0],1)),x))\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        Xb = self._add_bias(x)\n",
    "        classes = np.unique(y)\n",
    "        \n",
    "        for cl in classes:\n",
    "            cur_y = y==cl\n",
    "            cur_classifier = self.classifier(self.eta, self.iters, cost=self.cost, norm=self.norm)\n",
    "            cur_classifier.fit(x, cur_y)\n",
    "            self.classifiers.append(cur_classifier)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if not self.classifiers:\n",
    "            raise RuntimeError('Classifier not fit!')\n",
    "        \n",
    "        probabilities = []\n",
    "        for classifier in self.classifiers:\n",
    "            probabilities.append(classifier.predict_proba(x))\n",
    "        probabilities = np.hstack(probabilities)\n",
    "        return np.argmax(probabilities,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 95.31 MiB, increment: 0.21 MiB\n",
      "peak memory: 95.47 MiB, increment: 0.16 MiB\n",
      "peak memory: 95.48 MiB, increment: 0.00 MiB\n",
      "peak memory: 95.48 MiB, increment: 0.00 MiB\n",
      "stoch: 0.02\n",
      "peak memory: 95.48 MiB, increment: 0.00 MiB\n",
      "newton: 0.813333333333\n",
      "peak memory: 95.48 MiB, increment: 0.00 MiB\n",
      "steep: 0.813333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "ds = load_iris()\n",
    "kwargs = {\n",
    "    'norm': 2,\n",
    "    'cost': .1,\n",
    "    'iterations': 500\n",
    "}\n",
    "regrs = {\n",
    "    'newton': LogRegClassifier(.1, optimize='newton', **kwargs),\n",
    "    'stoch': LogRegClassifier(.1, optimize='stochdesc', **kwargs), # Different results every time, random\n",
    "    'steep': LogRegClassifier(.1, optimize='steepdesc', **kwargs)\n",
    "}\n",
    "for regr in regrs.values():\n",
    "    %memit regr.fit(ds.data, ds.target)\n",
    "\n",
    "for key, val in regrs.items():\n",
    "    %memit res = val.predict(ds.data)\n",
    "    print(key + ': ' +str(accuracy_score(ds.target, res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 325.75 MiB, increment: 0.00 MiB\n",
      "peak memory: 511.92 MiB, increment: 186.16 MiB\n",
      "<__main__.LogRegClassifier object at 0x1047d3630>\n",
      "Accuracy of:  0.23147166059\n"
     ]
    }
   ],
   "source": [
    "for train_indices, test_indices in cv_object:\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    kwargs = {\n",
    "        'norm': 2,\n",
    "        'cost': .1,\n",
    "        'iterations': 500\n",
    "    }\n",
    "\n",
    "    %memit lr = LogRegClassifier(.1, optimize='steepdesc', **kwargs)\n",
    "    %memit lr.fit(X_train,y_train)\n",
    "    print(lr)\n",
    "\n",
    "    yhat = lr.predict(X_test)\n",
    "\n",
    "    print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.376107419712\n",
      "{'optimize': 'steepdesc', 'norm': 3, 'eta': 0.0001, 'cost': 0.25}\n",
      "peak memory: 107.11 MiB, increment: 0.16 MiB\n",
      "Best score: 0.376107419712\n",
      "{'optimize': 'newton', 'norm': 0, 'eta': 0.0001, 'cost': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "class LogRegClassifier:\n",
    "    def __init__(self, eta=.0001, iters=20, optimize='steepdesc', cost=0.001, norm=2):\n",
    "        self._set_optimization(optimize)\n",
    "        self.eta = eta\n",
    "        self.iters = iters\n",
    "        self.cost = cost\n",
    "        self.norm = norm\n",
    "        \n",
    "        self.classifiers = [] # fill with binary classifiers during fit\n",
    "        self._estimator_type = 'classifier'\n",
    "    \n",
    "    def _add_bias(self, x):\n",
    "        return np.hstack((np.ones((x.shape[0],1)),x))\n",
    "    \n",
    "    def _set_optimization(self, optimize):\n",
    "        typesofoptimize = {\n",
    "            'steepdesc': BinarySteepDescClassifier, \n",
    "            'stochdesc': BinaryStochDescClassifier, \n",
    "            'newton': BinaryNewtonClassifier\n",
    "        }\n",
    "        if optimize not in typesofoptimize.keys():\n",
    "            raise ValueError('optimize must be one of: ' + ' '.join(typesofoptimize.keys()))\n",
    "            \n",
    "        self.optimize = optimize\n",
    "        self.classifier = typesofoptimize[optimize]\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        x, y = check_X_y(x, y)\n",
    "        \n",
    "        Xb = self._add_bias(x)\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.X_ = x\n",
    "        self.y_ = y\n",
    "        \n",
    "        for cl in self.classes_:\n",
    "            cur_y = y==cl\n",
    "            cur_classifier = self.classifier(self.eta, self.iters, cost=self.cost, norm=self.norm)\n",
    "            cur_classifier.fit(x, cur_y)\n",
    "            self.classifiers.append(cur_classifier)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "        x = check_array(x)\n",
    "        \n",
    "        probabilities = []\n",
    "        for classifier in self.classifiers:\n",
    "            probabilities.append(classifier.predict_proba(x))\n",
    "        probabilities = np.hstack(probabilities)\n",
    "        return np.argmax(probabilities,axis=1)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        res = self.predict(x)\n",
    "        return accuracy_score(y, res)\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'eta': self.eta,\n",
    "            'iters': self.iters,\n",
    "            'optimize': self.optimize,\n",
    "            'cost': self.cost,\n",
    "            'norm': self.norm\n",
    "        }\n",
    "    \n",
    "# check_estimator(LogRegClassifier)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as p\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "params = {\n",
    "    \"optimize\": ['newton', 'stochdesc', 'steepdesc'],\n",
    "    \"eta\": [.0001, .001, .01, .1, .25, .5],\n",
    "    \"norm\": [0, 1, 2, 3],\n",
    "    \"cost\": [.0001, .001, .01, .1, .25, .5]\n",
    "}\n",
    "cv = GridSearchCV(LogRegClassifier(), params, n_jobs=-1)\n",
    "random_search = RandomizedSearchCV(LogRegClassifier(), params, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "print('Best score: '+str(random_search.best_score_))\n",
    "y_hat = random_search.predict(X_test)\n",
    "\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print(\"confusion matrix\\n\",conf)\n",
    "print(random_search.best_params_)\n",
    "\n",
    "%memit cv.fit(X_train, y_train)\n",
    "print('Best score: '+str(cv.best_score_))\n",
    "y_hat = cv.predict(X_test)\n",
    "\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print(\"confusion matrix\\n\",conf)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:machinelearning]",
   "language": "python",
   "name": "conda-env-machinelearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

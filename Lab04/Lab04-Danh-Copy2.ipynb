{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Logistic Regression: 2015 American Community Survey\n",
    "\n",
    "## Conrad Appel, Erik Gabrielsen, Danh Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation and Overview (30 points total)\n",
    "\n",
    "[5 points] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results.\n",
    "\n",
    "[10 points] (mostly the same processes as from lab one) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "[15 points] Divide you data into training and testing data using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. Argue for or against splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?  \n",
    "\n",
    "### Modeling (50 points total)\n",
    "\n",
    "[20 points] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template used in the course. You should add the following functionality to the logistic regression classifier:\n",
    "Ability to choose optimization technique when class is instantiated: either steepest descent, stochastic gradient descent, or Newton's method. Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1/L2 norm of the weights). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  \n",
    "\n",
    "[15 points] Train your classifier to achieve good generalization performance. That is, adjust the optimization technique and the value of the regularization term \"C\" to achieve the best performance on your test set. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?\n",
    "\n",
    "[15 points] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time, training iterations, and memory usage while training. Discuss the results. \n",
    "\n",
    "### Deployment (10 points total)\n",
    "\n",
    "Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?\n",
    "\n",
    "### Exceptional Work (10 points total)\n",
    "\n",
    "You have free reign to provide additional analyses.\n",
    "One idea: Make your implementation of logistic regression compatible with the GridSearchCV function that is part of scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "In this lab, we are investigating the relationship among working individuals' income and personal attributes as provided by the [2015 American Community Survey](https://www.kaggle.com/census/2015-american-community-survey). We used a subset of the entire dataset, which gave us around 1.5 million individual records to work with. In the interest of computation time, we used about 500,000 records.\n",
    "\n",
    "Specifically, we want to see how well we can classify people's income based on their age, race, education level, degree, where they were born, marital status, military status, and time when they arrive and leave work. \n",
    "\n",
    "Marketing and advertising researchers may be interested in our work. They can try to predict their target audience's income based on certain attributes to determine how they should price their products. They can use this to bolster their market research surveys and see how their samples compare to census data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 3.86 s, total: 21.9 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "number_of_rows = 500000 # None to get all\n",
    "\n",
    "%time df = pd.read_csv('~/Downloads/ss15pusb.csv', nrows=number_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at all these columns we have to work with! \n",
      "\n",
      "['RT', 'SERIALNO', 'SPORDER', 'PUMA', 'ST', 'ADJINC', 'PWGTP', 'AGEP', 'CIT', 'CITWP', 'COW', 'DDRS', 'DEAR', 'DEYE', 'DOUT', 'DPHY', 'DRAT', 'DRATX', 'DREM', 'ENG', 'FER', 'GCL', 'GCM', 'GCR', 'HINS1', 'HINS2', 'HINS3', 'HINS4', 'HINS5', 'HINS6', 'HINS7', 'INTP', 'JWMNP', 'JWRIP', 'JWTR', 'LANX', 'MAR', 'MARHD', 'MARHM', 'MARHT', 'MARHW', 'MARHYP', 'MIG', 'MIL', 'MLPA', 'MLPB', 'MLPCD', 'MLPE', 'MLPFG', 'MLPH', 'MLPI', 'MLPJ', 'MLPK', 'NWAB', 'NWAV', 'NWLA', 'NWLK', 'NWRE', 'OIP', 'PAP', 'RELP', 'RETP', 'SCH', 'SCHG', 'SCHL', 'SEMP', 'SEX', 'SSIP', 'SSP', 'WAGP', 'WKHP', 'WKL', 'WKW', 'WRK', 'YOEP', 'ANC', 'ANC1P', 'ANC2P', 'DECADE', 'DIS', 'DRIVESP', 'ESP', 'ESR', 'FHICOVP', 'FOD1P', 'FOD2P', 'HICOV', 'HISP', 'INDP', 'JWAP', 'JWDP', 'LANP', 'MIGPUMA', 'MIGSP', 'MSP', 'NAICSP', 'NATIVITY', 'NOP', 'OC', 'OCCP', 'PAOC', 'PERNP', 'PINCP', 'POBP', 'POVPIP', 'POWPUMA', 'POWSP', 'PRIVCOV', 'PUBCOV', 'QTRBIR', 'RAC1P', 'RAC2P', 'RAC3P', 'RACAIAN', 'RACASN', 'RACBLK', 'RACNH', 'RACNUM', 'RACPI', 'RACSOR', 'RACWHT', 'RC', 'SCIENGP', 'SCIENGRLP', 'SFN', 'SFR', 'SOCP', 'VPS', 'WAOB', 'FAGEP', 'FANCP', 'FCITP', 'FCITWP', 'FCOWP', 'FDDRSP', 'FDEARP', 'FDEYEP', 'FDISP', 'FDOUTP', 'FDPHYP', 'FDRATP', 'FDRATXP', 'FDREMP', 'FENGP', 'FESRP', 'FFERP', 'FFODP', 'FGCLP', 'FGCMP', 'FGCRP', 'FHINS1P', 'FHINS2P', 'FHINS3C', 'FHINS3P', 'FHINS4C', 'FHINS4P', 'FHINS5C', 'FHINS5P', 'FHINS6P', 'FHINS7P', 'FHISP', 'FINDP', 'FINTP', 'FJWDP', 'FJWMNP', 'FJWRIP', 'FJWTRP', 'FLANP', 'FLANXP', 'FMARHDP', 'FMARHMP', 'FMARHTP', 'FMARHWP', 'FMARHYP', 'FMARP', 'FMIGP', 'FMIGSP', 'FMILPP', 'FMILSP', 'FOCCP', 'FOIP', 'FPAP', 'FPERNP', 'FPINCP', 'FPOBP', 'FPOWSP', 'FPRIVCOVP', 'FPUBCOVP', 'FRACP', 'FRELP', 'FRETP', 'FSCHGP', 'FSCHLP', 'FSCHP', 'FSEMP', 'FSEXP', 'FSSIP', 'FSSP', 'FWAGP', 'FWKHP', 'FWKLP', 'FWKWP', 'FWRKP', 'FYOEP', 'pwgtp1', 'pwgtp2', 'pwgtp3', 'pwgtp4', 'pwgtp5', 'pwgtp6', 'pwgtp7', 'pwgtp8', 'pwgtp9', 'pwgtp10', 'pwgtp11', 'pwgtp12', 'pwgtp13', 'pwgtp14', 'pwgtp15', 'pwgtp16', 'pwgtp17', 'pwgtp18', 'pwgtp19', 'pwgtp20', 'pwgtp21', 'pwgtp22', 'pwgtp23', 'pwgtp24', 'pwgtp25', 'pwgtp26', 'pwgtp27', 'pwgtp28', 'pwgtp29', 'pwgtp30', 'pwgtp31', 'pwgtp32', 'pwgtp33', 'pwgtp34', 'pwgtp35', 'pwgtp36', 'pwgtp37', 'pwgtp38', 'pwgtp39', 'pwgtp40', 'pwgtp41', 'pwgtp42', 'pwgtp43', 'pwgtp44', 'pwgtp45', 'pwgtp46', 'pwgtp47', 'pwgtp48', 'pwgtp49', 'pwgtp50', 'pwgtp51', 'pwgtp52', 'pwgtp53', 'pwgtp54', 'pwgtp55', 'pwgtp56', 'pwgtp57', 'pwgtp58', 'pwgtp59', 'pwgtp60', 'pwgtp61', 'pwgtp62', 'pwgtp63', 'pwgtp64', 'pwgtp65', 'pwgtp66', 'pwgtp67', 'pwgtp68', 'pwgtp69', 'pwgtp70', 'pwgtp71', 'pwgtp72', 'pwgtp73', 'pwgtp74', 'pwgtp75', 'pwgtp76', 'pwgtp77', 'pwgtp78', 'pwgtp79', 'pwgtp80']\n"
     ]
    }
   ],
   "source": [
    "print('Look at all these columns we have to work with! \\n\\n'+str(list(df.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AGEP',\n",
       " 'PINCP',\n",
       " 'MAR',\n",
       " 'MIL',\n",
       " 'SCHL',\n",
       " 'QTRBIR',\n",
       " 'JWAP',\n",
       " 'JWDP',\n",
       " 'YOEP',\n",
       " 'WAOB',\n",
       " 'RAC1P',\n",
       " 'FOD1P']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But let's bring the number of columns down to something more reasonable...\n",
    "cols_to_save = ['AGEP','PINCP', 'MAR', 'MIL', 'SCHL','QTRBIR','JWAP','JWDP','YOEP','WAOB','RAC1P', 'FOD1P']\n",
    "new_df = df.filter(items=cols_to_save)\n",
    "list(new_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "We began with approximately 1.5 million records from the dataset and kept approximately 12 attributes of the almost 300 attributes tracked by the survey, and wrote it out to a CSV for reuse later on. Of the attributes we examine, Age, Personal Income, and Year of entry to the US are ordinal data represented as integers. The remaining attributes were recorded as discrete values in the dataset, and we continue using their integer representation. For instance, Time of Arrival at work is recorded as 1 for times between 12:00 a.m. and 12:04 a.m. For classification purposes, we cut income into fifths, with 0 being the 20% of people with the lowest income. The full list of attributes we are working with is displayed in the table below.\n",
    "\n",
    "\n",
    "**Attribute**|**Description**|**Example**\n",
    ":-----:|:-----:|:-----:\n",
    "AGEP|Age| \n",
    "PINCP|Total person's income (signed)| 0 - bottom 20% of population, 4 - top 20% of population\n",
    "YOEP|Year of entry to the US| \n",
    "QTRBIR|Quarter of birth|1 - January through March, 2 -  April through June, etc.\n",
    "JWAP|Time of arrival at work |1 - 12:00 a.m. to 12:04 a.m., 2 - 12:05 a.m. to 12:09 a.m., etc.\n",
    "JWDP|Time of departure for work|1 - 12:00 a.m. to 12:29 a.m., 2 - 12:30 a.m. to 12:59 a.m., etc.\n",
    "WAOB|World area of birth|1 - US state, 2 - PR and US Island Areas, etc.\n",
    "RAC1P|Recoded detailed race code|1 - White alone, 2 - Black or African American alone, etc.\n",
    "MAR|Marital status|1 - Married, 2 - Widowed\n",
    "MIL|Military Service |1 - Now on active duty, 4 - Never served in the military\n",
    "SCHL|Educational attainment|23 - Professional degree beyond a bachelor's degree, 24 - Doctorate degree, etc.\n",
    "FOD1P|Recoded field of degree|1904 - advertising and public relations, 2001 - communication technologies, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "\n",
    "We delete all of the records younger than 18 to exclude the set to working individuals. Time of arival and departure are `NAN` for those who are not working or working at home, so we map this to a discrete value of 0. We also map the `NAN` values for those who were born in the United States to the year they were born for Year of Entry. For Field of Degree, we map `NAN` to 0 for those that do not have at least a bachelor's degree. We then use quantile cut divide income into fifths for classification purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete younger than 18\n",
    "new_df = new_df[new_df.AGEP >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that contain nulls: ['JWAP', 'JWDP', 'YOEP', 'FOD1P']\n"
     ]
    }
   ],
   "source": [
    "# find null columns\n",
    "print('Columns that contain nulls: '+str(new_df.columns[new_df.isnull().any()].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean out JWAP and JWDP. They are ordinal variables, create a new one to classify them\n",
    "new_df['JWAP'].fillna(0, inplace=True)\n",
    "new_df['JWDP'].fillna(0, inplace=True)\n",
    "new_df['FOD1P'].fillna(0, inplace=True)\n",
    "\n",
    "## fill null values in YOEP (Year of entrance into US)\n",
    "new_df['YOEP'].fillna((2015 - new_df['AGEP']), inplace=True)\n",
    "new_df['YOEP'] = new_df['YOEP'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are we good? Yes!\n"
     ]
    }
   ],
   "source": [
    "# make sure we took care of them all\n",
    "print('Are we good? '+('Yes!' if len(new_df.columns[new_df.isnull().any()].tolist()) == 0 else 'No...'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 4, 3]\n",
       "Categories (5, int64): [0 < 1 < 2 < 3 < 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['PINCP'] = pd.qcut(new_df.PINCP, 5, labels=[0,1,2,3,4])\n",
    "new_df['PINCP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df['PINCP'] = new_df['PINCP'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in each class:\n",
      "1: 80754\n",
      "0: 79793\n",
      "2: 78349\n",
      "4: 79469\n",
      "3: 79608\n"
     ]
    }
   ],
   "source": [
    "print('Number of people in each class:')\n",
    "for value in new_df.PINCP.unique(): \n",
    "    print(str(value)+': ' +str(len(new_df[new_df['PINCP'] == value])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 397973 entries, 0 to 499999\n",
      "Data columns (total 12 columns):\n",
      "AGEP      397973 non-null int64\n",
      "PINCP     397973 non-null int64\n",
      "MAR       397973 non-null int64\n",
      "MIL       397973 non-null float64\n",
      "SCHL      397973 non-null float64\n",
      "QTRBIR    397973 non-null int64\n",
      "JWAP      397973 non-null float64\n",
      "JWDP      397973 non-null float64\n",
      "YOEP      397973 non-null int64\n",
      "WAOB      397973 non-null int64\n",
      "RAC1P     397973 non-null int64\n",
      "FOD1P     397973 non-null float64\n",
      "dtypes: float64(5), int64(7)\n",
      "memory usage: 39.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Show final datatypes\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Sets\n",
    "\n",
    "We used scikit learn to randomly select 80% of our dataset for training classifiers and 20% of our dataset for testing. Since we used quantile cut to evenly split our dataset and balance the income classes, the 80-20 split is appropriate for our purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=1, random_state=None, test_size=0.1, train_size=0.8)\n"
     ]
    }
   ],
   "source": [
    "# We need to get a copy of the data and the \"targets\" as numpy matrices for training (rather than Pandas DataFrames)\n",
    "df_imputed = new_df.copy()\n",
    "if 'PINCP' in new_df:\n",
    "    y = df_imputed['PINCP'].values # get a copy of the target classes\n",
    "    del df_imputed['PINCP'] # get rid of the class column\n",
    "    X = df_imputed.values # use everything else to predict!    \n",
    "    \n",
    "# We use a Scikit Learn Cross-Validation object to split our data into training/testing subsets\n",
    "cv_object = ShuffleSplit(train_size=.8, n_splits=1)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Iteration 0 ====\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "\n",
    "# first we create a reusable sklearn logisitic regression object\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "# different training and testing sets. Each time we will reuse the logisitic regression \n",
    "# object, but it gets re-trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "for train_indices, test_indices in cv_object.split(X):\n",
    "    print(\"==== Iteration\",iter_num,\"====\")\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    %memit lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    for i in range(0,len(conf)):\n",
    "        print(\"Accuracy for income bracket\", i, \":\",conf[i][i]/sum(conf[i]))\n",
    "    print(\"Overall accuracy:\", acc )\n",
    "    print(\"Confusion matrix\\n\",conf)\n",
    "    iter_num += 1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Different types of binary classifiers inherit from this and redefine methods as necessary\n",
    "class BinaryClassifierBase:\n",
    "    def __init__(self, eta, iters=20, cost=0.001, norm=2):\n",
    "        self.eta = eta\n",
    "        self.cost = cost\n",
    "        self.iters = iters\n",
    "        self.norm = norm\n",
    "    \n",
    "    def normalize(self, w, gradient):\n",
    "        # regularization (adds both if 3)\n",
    "        if self.norm & 1: # L1 norm\n",
    "            gradient[1:] += -1 * w[1:] * self.cost\n",
    "        elif self.norm & 2: # L2 norm\n",
    "            gradient[1:] += -2 * w[1:] * self.cost\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.w_ = np.zeros((x.shape[1],1))\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(x,y) # gradient calculation must be defined in subclass!\n",
    "            self.normalize(self.w_, gradient)\n",
    "            self.w_ += gradient*self.eta\n",
    "    \n",
    "    def predict_proba(self,x):\n",
    "        return 1/(1+np.exp(-(x @ self.w_))) # sigmoid\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return (self.predict_proba(x)>0.5) # choose the class that has a greater probability\n",
    "    \n",
    "    \n",
    "# Uses the Stochastic Gradient Descent Algorithm (random movements - saves time!)\n",
    "class BinaryStochDescClassifier(BinaryClassifierBase):\n",
    "    def _get_gradient(self, x, y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(x[idx]) # get difference from target\n",
    "        gradient = x[idx] * ydiff[:,np.newaxis]\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return gradient    \n",
    "    \n",
    "# Uses the Steepest Gradient Descent algorithm\n",
    "class BinarySteepDescClassifier(BinaryClassifierBase):\n",
    "    def _get_gradient(self, x, y):\n",
    "        ydiff = y-self.predict_proba(x).ravel()\n",
    "        gradient = np.mean(x * ydiff[:,np.newaxis], axis=0)\n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "# Uses BFGS, similar to Newton's Method\n",
    "class BinaryNewtonClassifier(BinaryClassifierBase):\n",
    "    def fit(self, x, y):\n",
    "        def obj_fn(w, x, y, c):\n",
    "            g = expit(x @ w)\n",
    "            return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + c*sum(w**2)\n",
    "        \n",
    "        def obj_grad(w, x, y, c):\n",
    "            g = expit(x @ w)\n",
    "            ydiff = y-g\n",
    "            gradient = np.mean(x * ydiff[:,np.newaxis], axis=0)\n",
    "            gradient = gradient.reshape(w.shape)\n",
    "            self.normalize(w, gradient)\n",
    "            return -gradient\n",
    "        \n",
    "        self.w_ = fmin_bfgs(obj_fn, \n",
    "                            np.zeros((x.shape[1], 1)), \n",
    "                            fprime=obj_grad, \n",
    "                            args=(x, y, self.cost), \n",
    "                            gtol=1e-03, \n",
    "                            maxiter=self.iters,\n",
    "                            disp=False).reshape((x.shape[1], 1))\n",
    "\n",
    "# Utilizes a binary classifer for each class, similar to sklearn's implementation\n",
    "class LogRegClassifier:\n",
    "    def __init__(self, eta=.0001, iters=20, optimize='steepdesc', cost=0.001, norm=2):\n",
    "        self._set_optimization(optimize)\n",
    "        self.eta = eta\n",
    "        self.iters = iters\n",
    "        self.cost = cost\n",
    "        self.norm = norm\n",
    "        \n",
    "        self.classifiers = [] # fill with binary classifiers during fitting \n",
    "        self._estimator_type = 'classifier' # tells the sklearn API what this class is\n",
    "    \n",
    "    def _add_bias(self, x):\n",
    "        return np.hstack((np.ones((x.shape[0],1)),x))\n",
    "    \n",
    "    def _set_optimization(self, optimize):\n",
    "        typesofoptimize = {\n",
    "            'steepdesc': BinarySteepDescClassifier, \n",
    "            'stochdesc': BinaryStochDescClassifier, \n",
    "            'newton': BinaryNewtonClassifier\n",
    "        }\n",
    "        if optimize not in typesofoptimize.keys():\n",
    "            raise ValueError('optimize must be one of: ' + ' '.join(typesofoptimize.keys()))\n",
    "            \n",
    "        self.optimize = optimize\n",
    "        self.classifier = typesofoptimize[optimize]\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        x, y = check_X_y(x, y)\n",
    "        \n",
    "        Xb = self._add_bias(x)\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.X_ = x\n",
    "        self.y_ = y\n",
    "        \n",
    "        # create a binary classifier for each unique class (it either is or isn't part of the class)\n",
    "        for cl in self.classes_:\n",
    "            cur_y = y==cl\n",
    "            cur_classifier = self.classifier(self.eta, self.iters, cost=self.cost, norm=self.norm)\n",
    "            cur_classifier.fit(x, cur_y)\n",
    "            self.classifiers.append(cur_classifier)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "        x = check_array(x)\n",
    "        \n",
    "        probabilities = []\n",
    "        for classifier in self.classifiers:\n",
    "            probabilities.append(classifier.predict_proba(x))\n",
    "        probabilities = np.hstack(probabilities)\n",
    "        # for each row, find the maximum probability given by each class's classifier, which is probably the answer\n",
    "        return np.argmax(probabilities,axis=1) \n",
    "    \n",
    "    # predicts and returns how close we were to 100% accuracy (used in sklearn's API)\n",
    "    def score(self, x, y):\n",
    "        res = self.predict(x)\n",
    "        return accuracy_score(y, res)\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'eta': self.eta,\n",
    "            'iters': self.iters,\n",
    "            'optimize': self.optimize,\n",
    "            'cost': self.cost,\n",
    "            'norm': self.norm\n",
    "        }\n",
    "    \n",
    "# will throw errors if the class is ill-defined and won't work with sklearn's API\n",
    "check_estimator(LogRegClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steep accuracy: 0.766666666667\n",
      "newton accuracy: 0.966666666667\n",
      "stoch accuracy: 0.913333333333\n"
     ]
    }
   ],
   "source": [
    "# Let's test the LogRegClassifier on a smaller dataset!\n",
    "ds = load_iris()\n",
    "kwargs = {'eta': .01, 'norm': 2, 'cost': .001, 'iters': 500}\n",
    "regrs = {\n",
    "    'newton': LogRegClassifier(optimize='newton', **kwargs),\n",
    "    'stoch': LogRegClassifier(optimize='stochdesc', **kwargs), # Remember: different results every time\n",
    "    'steep': LogRegClassifier(optimize='steepdesc', **kwargs)\n",
    "}\n",
    "\n",
    "for regr in regrs.values():\n",
    "    regr.fit(ds.data, ds.target)\n",
    "\n",
    "for key, val in regrs.items():\n",
    "    res = val.predict(ds.data)\n",
    "    print(key + ' accuracy: ' +str(accuracy_score(ds.target, res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Iteration 0 ====\n",
      "peak memory: 366.06 MiB, increment: 0.06 MiB\n",
      "Accuracy for income bracket 0 : 0.650127226463\n",
      "Accuracy for income bracket 1 : 0.413071895425\n",
      "Accuracy for income bracket 2 : 0.180904522613\n",
      "Accuracy for income bracket 3 : 0.187421383648\n",
      "Accuracy for income bracket 4 : 0.635135135135\n",
      "Overall accuracy: 0.41380182002\n",
      "Confusion matrix\n",
      " [[511 147  37  34  57]\n",
      " [239 316  87  47  76]\n",
      " [131 215 144 138 168]\n",
      " [ 66 123 143 149 314]\n",
      " [ 50  79  67 101 517]]\n"
     ]
    }
   ],
   "source": [
    "# Now let's test our classifier on the bigger data set using the split from earlier!\n",
    "regr = LogRegClassifier(optimize='newton', eta=.01, norm=2, cost=.001, iters=500)\n",
    "\n",
    "iter_num=0\n",
    "for train_indices, test_indices in cv_object.split(X):\n",
    "    print(\"==== Iteration\",iter_num,\"====\")\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    %memit regr.fit(X_train,y_train)\n",
    "    y_hat = lr_clf.predict(X_test)\n",
    "\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    for i in range(0,len(conf)):\n",
    "        print(\"Accuracy for income bracket\", i, \":\",conf[i][i]/sum(conf[i]))\n",
    "    print(\"Overall accuracy:\", acc )\n",
    "    print(\"Confusion matrix\\n\",conf)\n",
    "    iter_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.242582235283\n",
      "Confusion matrix\n",
      " [[689   0  47   0  50]\n",
      " [684   0  38   0  43]\n",
      " [650   0  55   0  91]\n",
      " [550   0 115   0 130]\n",
      " [382   0 177   0 255]]\n",
      "{'optimize': 'newton', 'eta': 0.0001, 'cost': 0.5, 'norm': 2}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"optimize\": ['newton', 'stochdesc', 'steepdesc'],\n",
    "    \"eta\": [.0001, .001, .1, .5],\n",
    "    \"norm\": [0, 1, 2, 3],\n",
    "    \"cost\": [.0001, .001, .1, .5]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(LogRegClassifier(), params, n_jobs=-1) # runs in parallel to get the job done as quick as possible\n",
    "cv.fit(X_train, list(y_train))\n",
    "print('Best score: '+str(cv.best_score_))\n",
    "y_hat = cv.predict(X_test)\n",
    "conf = mt.confusion_matrix(y_test, y_hat)\n",
    "print(\"Confusion matrix\\n\",conf)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.227509716561\n",
      "Confusion matrix\n",
      " [[689   0  47   0  50]\n",
      " [684   0  38   0  43]\n",
      " [650   0  55   0  91]\n",
      " [550   0 115   0 130]\n",
      " [383   0 176   0 255]]\n",
      "{'cost': 0.001, 'optimize': 'steepdesc', 'norm': 0, 'eta': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"optimize\": ['newton', 'stochdesc', 'steepdesc'],\n",
    "    \"eta\": [.0001, .001, .1, .5],\n",
    "    \"norm\": [0, 1, 2, 3],\n",
    "    \"cost\": [.0001, .001, .1, .5]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(LogRegClassifier(), params, n_jobs=-1)\n",
    "random_search.fit(X_train, list(y_train))\n",
    "print('Best score: '+str(random_search.best_score_))\n",
    "y_hat = random_search.predict(X_test)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print(\"Confusion matrix\\n\",conf)\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:machinelearning]",
   "language": "python",
   "name": "conda-env-machinelearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

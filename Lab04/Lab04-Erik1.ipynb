{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04\n",
    "\n",
    "## Conrad Appel, Erik Gabrielsen, Danh Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation and Overview (30 points total)\n",
    "\n",
    "[5 points] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results.\n",
    "\n",
    "[10 points] (mostly the same processes as from lab one) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "[15 points] Divide you data into training and testing data using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. Argue for or against splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?  \n",
    "\n",
    "### Modeling (50 points total)\n",
    "\n",
    "[20 points] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template used in the course. You should add the following functionality to the logistic regression classifier:\n",
    "Ability to choose optimization technique when class is instantiated: either steepest descent, stochastic gradient descent, or Newton's method. Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1/L2 norm of the weights). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  \n",
    "\n",
    "[15 points] Train your classifier to achieve good generalization performance. That is, adjust the optimization technique and the value of the regularization term \"C\" to achieve the best performance on your test set. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?\n",
    "\n",
    "[15 points] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time, training iterations, and memory usage while training. Discuss the results. \n",
    "\n",
    "### Deployment (10 points total)\n",
    "\n",
    "Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?\n",
    "\n",
    "### Exceptional Work (10 points total)\n",
    "\n",
    "You have free reign to provide additional analyses.\n",
    "One idea: Make your implementation of logistic regression compatible with the GridSearchCV function that is part of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 18.1 s, total: 1min 24s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "## redoing large files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%time df = pd.read_csv('~/Downloads/ss15pusb.csv') # read in the csv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RT', 'SERIALNO', 'SPORDER', 'PUMA', 'ST', 'ADJINC', 'PWGTP',\n",
       "       'AGEP', 'CIT', 'CITWP', 'COW', 'DDRS', 'DEAR', 'DEYE', 'DOUT',\n",
       "       'DPHY', 'DRAT', 'DRATX', 'DREM', 'ENG', 'FER', 'GCL', 'GCM', 'GCR',\n",
       "       'HINS1', 'HINS2', 'HINS3', 'HINS4', 'HINS5', 'HINS6', 'HINS7',\n",
       "       'INTP', 'JWMNP', 'JWRIP', 'JWTR', 'LANX', 'MAR', 'MARHD', 'MARHM',\n",
       "       'MARHT', 'MARHW', 'MARHYP', 'MIG', 'MIL', 'MLPA', 'MLPB', 'MLPCD',\n",
       "       'MLPE', 'MLPFG', 'MLPH', 'MLPI', 'MLPJ', 'MLPK', 'NWAB', 'NWAV',\n",
       "       'NWLA', 'NWLK', 'NWRE', 'OIP', 'PAP', 'RELP', 'RETP', 'SCH', 'SCHG',\n",
       "       'SCHL', 'SEMP', 'SEX', 'SSIP', 'SSP', 'WAGP', 'WKHP', 'WKL', 'WKW',\n",
       "       'WRK', 'YOEP', 'ANC', 'ANC1P', 'ANC2P', 'DECADE', 'DIS', 'DRIVESP',\n",
       "       'ESP', 'ESR', 'FHICOVP', 'FOD1P', 'FOD2P', 'HICOV', 'HISP', 'INDP',\n",
       "       'JWAP', 'JWDP', 'LANP', 'MIGPUMA', 'MIGSP', 'MSP', 'NAICSP',\n",
       "       'NATIVITY', 'NOP', 'OC', 'OCCP', 'PAOC', 'PERNP', 'PINCP', 'POBP',\n",
       "       'POVPIP', 'POWPUMA', 'POWSP', 'PRIVCOV', 'PUBCOV', 'QTRBIR',\n",
       "       'RAC1P', 'RAC2P', 'RAC3P', 'RACAIAN', 'RACASN', 'RACBLK', 'RACNH',\n",
       "       'RACNUM', 'RACPI', 'RACSOR', 'RACWHT', 'RC', 'SCIENGP', 'SCIENGRLP',\n",
       "       'SFN', 'SFR', 'SOCP', 'VPS', 'WAOB', 'FAGEP', 'FANCP', 'FCITP',\n",
       "       'FCITWP', 'FCOWP', 'FDDRSP', 'FDEARP', 'FDEYEP', 'FDISP', 'FDOUTP',\n",
       "       'FDPHYP', 'FDRATP', 'FDRATXP', 'FDREMP', 'FENGP', 'FESRP', 'FFERP',\n",
       "       'FFODP', 'FGCLP', 'FGCMP', 'FGCRP', 'FHINS1P', 'FHINS2P', 'FHINS3C',\n",
       "       'FHINS3P', 'FHINS4C', 'FHINS4P', 'FHINS5C', 'FHINS5P', 'FHINS6P',\n",
       "       'FHINS7P', 'FHISP', 'FINDP', 'FINTP', 'FJWDP', 'FJWMNP', 'FJWRIP',\n",
       "       'FJWTRP', 'FLANP', 'FLANXP', 'FMARHDP', 'FMARHMP', 'FMARHTP',\n",
       "       'FMARHWP', 'FMARHYP', 'FMARP', 'FMIGP', 'FMIGSP', 'FMILPP',\n",
       "       'FMILSP', 'FOCCP', 'FOIP', 'FPAP', 'FPERNP', 'FPINCP', 'FPOBP',\n",
       "       'FPOWSP', 'FPRIVCOVP', 'FPUBCOVP', 'FRACP', 'FRELP', 'FRETP',\n",
       "       'FSCHGP', 'FSCHLP', 'FSCHP', 'FSEMP', 'FSEXP', 'FSSIP', 'FSSP',\n",
       "       'FWAGP', 'FWKHP', 'FWKLP', 'FWKWP', 'FWRKP', 'FYOEP', 'pwgtp1',\n",
       "       'pwgtp2', 'pwgtp3', 'pwgtp4', 'pwgtp5', 'pwgtp6', 'pwgtp7',\n",
       "       'pwgtp8', 'pwgtp9', 'pwgtp10', 'pwgtp11', 'pwgtp12', 'pwgtp13',\n",
       "       'pwgtp14', 'pwgtp15', 'pwgtp16', 'pwgtp17', 'pwgtp18', 'pwgtp19',\n",
       "       'pwgtp20', 'pwgtp21', 'pwgtp22', 'pwgtp23', 'pwgtp24', 'pwgtp25',\n",
       "       'pwgtp26', 'pwgtp27', 'pwgtp28', 'pwgtp29', 'pwgtp30', 'pwgtp31',\n",
       "       'pwgtp32', 'pwgtp33', 'pwgtp34', 'pwgtp35', 'pwgtp36', 'pwgtp37',\n",
       "       'pwgtp38', 'pwgtp39', 'pwgtp40', 'pwgtp41', 'pwgtp42', 'pwgtp43',\n",
       "       'pwgtp44', 'pwgtp45', 'pwgtp46', 'pwgtp47', 'pwgtp48', 'pwgtp49',\n",
       "       'pwgtp50', 'pwgtp51', 'pwgtp52', 'pwgtp53', 'pwgtp54', 'pwgtp55',\n",
       "       'pwgtp56', 'pwgtp57', 'pwgtp58', 'pwgtp59', 'pwgtp60', 'pwgtp61',\n",
       "       'pwgtp62', 'pwgtp63', 'pwgtp64', 'pwgtp65', 'pwgtp66', 'pwgtp67',\n",
       "       'pwgtp68', 'pwgtp69', 'pwgtp70', 'pwgtp71', 'pwgtp72', 'pwgtp73',\n",
       "       'pwgtp74', 'pwgtp75', 'pwgtp76', 'pwgtp77', 'pwgtp78', 'pwgtp79',\n",
       "       'pwgtp80'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(10)\n",
    "df.shape\n",
    "\n",
    "new_df = df.filter(items=['AGEP','PINCP','QTRBIR','JWAP','JWDP','DECADE','YOEP','WAOB','RAC1P'])\n",
    "new_df.shape\n",
    "new_df.to_csv('~/Downloads/ss15pusb2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>PINCP</th>\n",
       "      <th>QTRBIR</th>\n",
       "      <th>JWAP</th>\n",
       "      <th>JWDP</th>\n",
       "      <th>DECADE</th>\n",
       "      <th>YOEP</th>\n",
       "      <th>WAOB</th>\n",
       "      <th>RAC1P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>9100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>25300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>82.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>175.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19</td>\n",
       "      <td>19600.0</td>\n",
       "      <td>4</td>\n",
       "      <td>84.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>58</td>\n",
       "      <td>143000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>84.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53</td>\n",
       "      <td>56000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>67</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>34</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>35</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>113.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>42</td>\n",
       "      <td>73000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>41</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>91.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>220.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>51</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>58</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>68</td>\n",
       "      <td>32100.0</td>\n",
       "      <td>3</td>\n",
       "      <td>76.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>69</td>\n",
       "      <td>36800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>68</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>57</td>\n",
       "      <td>287000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528485</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528486</th>\n",
       "      <td>78</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528487</th>\n",
       "      <td>18</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>167.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528488</th>\n",
       "      <td>48</td>\n",
       "      <td>98000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>62.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528489</th>\n",
       "      <td>53</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>82.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528490</th>\n",
       "      <td>63</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>121.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528491</th>\n",
       "      <td>61</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>223.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528492</th>\n",
       "      <td>60</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528493</th>\n",
       "      <td>66</td>\n",
       "      <td>52200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528494</th>\n",
       "      <td>72</td>\n",
       "      <td>36100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528495</th>\n",
       "      <td>43</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>107.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528496</th>\n",
       "      <td>41</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>107.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528497</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528498</th>\n",
       "      <td>35</td>\n",
       "      <td>79500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528499</th>\n",
       "      <td>65</td>\n",
       "      <td>91500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528500</th>\n",
       "      <td>66</td>\n",
       "      <td>43000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528501</th>\n",
       "      <td>39</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>67.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528502</th>\n",
       "      <td>17</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528503</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528504</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528505</th>\n",
       "      <td>53</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>94.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528506</th>\n",
       "      <td>34</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528507</th>\n",
       "      <td>40</td>\n",
       "      <td>90020.0</td>\n",
       "      <td>4</td>\n",
       "      <td>115.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528508</th>\n",
       "      <td>41</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>117.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528509</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528510</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528511</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528513</th>\n",
       "      <td>60</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528514</th>\n",
       "      <td>83</td>\n",
       "      <td>25900.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528515</th>\n",
       "      <td>37</td>\n",
       "      <td>115000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>89.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1207639 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AGEP     PINCP  QTRBIR   JWAP   JWDP  DECADE  YOEP  WAOB  RAC1P\n",
       "0          77    9100.0       1    NaN    NaN     NaN   NaN     1      1\n",
       "1          30    5000.0       2    NaN    NaN     NaN   NaN     1      1\n",
       "2          65   20000.0       4    NaN    NaN     NaN   NaN     1      1\n",
       "3          69   11000.0       2    NaN    NaN     NaN   NaN     1      1\n",
       "4          24   25300.0       2   82.0   34.0     NaN   NaN     1      1\n",
       "5          25   15000.0       2  175.0  103.0     NaN   NaN     1      1\n",
       "6          20       0.0       2    NaN    NaN     NaN   NaN     1      1\n",
       "7          19   19600.0       4   84.0   43.0     NaN   NaN     1      1\n",
       "8          58  143000.0       2   84.0   37.0     NaN   NaN     1      1\n",
       "9          53   56000.0       1   78.0   37.0     NaN   NaN     1      1\n",
       "10         67   20000.0       1    NaN    NaN     NaN   NaN     1      1\n",
       "11         34    5000.0       3    NaN    NaN     NaN   NaN     1      1\n",
       "12         35   32000.0       1  113.0   67.0     NaN   NaN     1      1\n",
       "13         10       NaN       1    NaN    NaN     NaN   NaN     1      1\n",
       "14          7       NaN       3    NaN    NaN     NaN   NaN     1      1\n",
       "15          5       NaN       1    NaN    NaN     NaN   NaN     1      1\n",
       "16         42   73000.0       1   70.0   25.0     NaN   NaN     1      1\n",
       "17         41   35000.0       2   91.0   46.0     NaN   NaN     1      1\n",
       "18         14       NaN       4    NaN    NaN     NaN   NaN     1      1\n",
       "19         10       NaN       1    NaN    NaN     NaN   NaN     1      1\n",
       "20          5       NaN       4    NaN    NaN     NaN   NaN     1      1\n",
       "21         19    4000.0       1  220.0  127.0     NaN   NaN     1      1\n",
       "22         51   40000.0       2    NaN    NaN     NaN   NaN     1      1\n",
       "23         58   19000.0       4    NaN    NaN     NaN   NaN     1      1\n",
       "24         20       0.0       4    NaN    NaN     NaN   NaN     1      1\n",
       "25         11       NaN       4    NaN    NaN     NaN   NaN     1      1\n",
       "26         68   32100.0       3   76.0   31.0     NaN   NaN     1      1\n",
       "27         69   36800.0       4    NaN    NaN     NaN   NaN     1      1\n",
       "28         68   90000.0       2    NaN    NaN     NaN   NaN     1      1\n",
       "29         57  287000.0       1    NaN    NaN     NaN   NaN     1      1\n",
       "...       ...       ...     ...    ...    ...     ...   ...   ...    ...\n",
       "1528485     0       NaN       3    NaN    NaN     NaN   NaN     1      1\n",
       "1528486    78   18000.0       2    NaN    NaN     NaN   NaN     1      1\n",
       "1528487    18    6000.0       1  167.0  103.0     NaN   NaN     1      1\n",
       "1528488    48   98000.0       4   62.0   19.0     NaN   NaN     1      1\n",
       "1528489    53   30000.0       4   82.0   43.0     NaN   NaN     1      1\n",
       "1528490    63   63000.0       2  121.0   79.0     NaN   NaN     1      1\n",
       "1528491    61   80000.0       1  223.0  130.0     NaN   NaN     1      1\n",
       "1528492    60   55000.0       1   96.0   55.0     NaN   NaN     1      1\n",
       "1528493    66   52200.0       2  120.0   79.0     NaN   NaN     1      1\n",
       "1528494    72   36100.0       1    NaN    NaN     NaN   NaN     1      1\n",
       "1528495    43   24000.0       2  107.0   67.0     NaN   NaN     1      1\n",
       "1528496    41   70000.0       1  107.0   67.0     NaN   NaN     1      1\n",
       "1528497    14       NaN       4    NaN    NaN     NaN   NaN     1      1\n",
       "1528498    35   79500.0       1   98.0   58.0     NaN   NaN     1      1\n",
       "1528499    65   91500.0       2    NaN    NaN     NaN   NaN     1      1\n",
       "1528500    66   43000.0       3    NaN    NaN     NaN   NaN     1      1\n",
       "1528501    39   65000.0       4   67.0   22.0     NaN   NaN     1      1\n",
       "1528502    17    1500.0       2    NaN    NaN     NaN   NaN     1      1\n",
       "1528503    11       NaN       3    NaN    NaN     NaN   NaN     1      1\n",
       "1528504     9       NaN       1    NaN    NaN     NaN   NaN     1      1\n",
       "1528505    53   35000.0       2   94.0   46.0     NaN   NaN     1      1\n",
       "1528506    34     500.0       1    NaN    NaN     NaN   NaN     1      1\n",
       "1528507    40   90020.0       4  115.0   67.0     NaN   NaN     1      1\n",
       "1528508    41   90000.0       1  117.0   67.0     NaN   NaN     1      1\n",
       "1528509    14       NaN       2    NaN    NaN     NaN   NaN     1      1\n",
       "1528510    11       NaN       4    NaN    NaN     NaN   NaN     1      1\n",
       "1528511     8       NaN       1    NaN    NaN     NaN   NaN     1      1\n",
       "1528513    60   20000.0       2    NaN    NaN     NaN   NaN     1      1\n",
       "1528514    83   25900.0       1    NaN    NaN     NaN   NaN     1      1\n",
       "1528515    37  115000.0       4   89.0   43.0     NaN   NaN     1      1\n",
       "\n",
       "[1207639 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[new_df['RAC1P']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as p\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BinaryClassifierBase:\n",
    "    def __init__(self, eta, iterations=20, cost=0.001):\n",
    "        self.eta = eta\n",
    "        self.cost = cost\n",
    "        self.iters = iterations\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.w_ = np.zeros((x.shape[1],1))\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(x,y)\n",
    "            self.w_ += gradient*self.eta\n",
    "    \n",
    "    def predict_proba(self,x):\n",
    "        return 1/(1+np.exp(-(x @ self.w_)))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return (self.predict_proba(x)>0.5)\n",
    "    \n",
    "    \n",
    "class BinaryStochDescClassifier(BinaryClassifierBase):\n",
    "    def _get_gradient(self, x, y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(x[idx]) # get y difference (now scalar)\n",
    "        gradient = x[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.cost\n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "class BinarySteepDescClassifier(BinaryClassifierBase):\n",
    "    def _get_gradient(self, x, y):\n",
    "        ydiff = y-self.predict_proba(x).ravel()\n",
    "        gradient = np.mean(x * ydiff[:,np.newaxis], axis=0)\n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "    \n",
    "class BinaryNewtonClassifier(BinaryClassifierBase):\n",
    "    def fit(self, x, y):\n",
    "        def obj_fn(w, x, y, c):\n",
    "            g = expit(x @ w)\n",
    "            return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + c*sum(w**2)\n",
    "        \n",
    "        def obj_grad(w, x, y, c):\n",
    "            g = expit(x @ w)\n",
    "            ydiff = y-g\n",
    "            gradient = np.mean(x * ydiff[:,np.newaxis], axis=0)\n",
    "            gradient = gradient.reshape(w.shape)\n",
    "            gradient[1:] += -2 * w[1:] * c\n",
    "            return -gradient\n",
    "        \n",
    "        self.w_ = fmin_bfgs(obj_fn, \n",
    "                            np.zeros((x.shape[1], 1)), \n",
    "                            fprime=obj_grad, \n",
    "                            args=(x, y, self.cost), \n",
    "                            gtol=1e-03, \n",
    "                            maxiter=self.iters, \n",
    "                            disp=False).reshape((x.shape[1], 1))\n",
    "\n",
    "        \n",
    "class LogRegClassifier:\n",
    "    def __init__(self, eta, iterations=20, optimize='steepdesc', cost=0.001):\n",
    "        typesofoptimize = {\n",
    "            'steepdesc': BinarySteepDescClassifier, \n",
    "            'stochdesc': BinaryStochDescClassifier, \n",
    "            'newton': BinaryNewtonClassifier\n",
    "        }\n",
    "        if optimize not in typesofoptimize.keys():\n",
    "            raise ValueError('optimize must be one of: ' + ' '.join(typesofoptimize.keys()))\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.optimize = optimize\n",
    "        self.classifier = typesofoptimize[optimize]\n",
    "        self.cost = cost\n",
    "        self.classifiers = [] # fill with binary classifiers during fit\n",
    "    \n",
    "    def _add_bias(self, x):\n",
    "        return np.hstack((np.ones((x.shape[0],1)),x))\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        Xb = self._add_bias(x)\n",
    "        classes = np.unique(y)\n",
    "        \n",
    "        for cl in classes:\n",
    "            cur_y = y==cl\n",
    "            cur_classifier = self.classifier(self.eta, self.iters, cost=self.cost)\n",
    "            cur_classifier.fit(x, cur_y)\n",
    "            self.classifiers.append(cur_classifier)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if not self.classifiers:\n",
    "            raise RuntimeError('Classifier not fit!')\n",
    "        \n",
    "        probabilities = []\n",
    "        for classifier in self.classifiers:\n",
    "            probabilities.append(classifier.predict_proba(x))\n",
    "        probabilities = np.hstack(probabilities)\n",
    "        return np.argmax(probabilities,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steep: 0.96\n",
      "stoch: 0.666666666667\n",
      "newton: 0.966666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "ds = load_iris()\n",
    "regrs = {\n",
    "    'newton': LogRegClassifier(.1, 500, optimize='newton'),\n",
    "    'stoch': LogRegClassifier(.1, 500, optimize='stochdesc'), # Different results every time, random\n",
    "    'steep': LogRegClassifier(.1, 500, optimize='steepdesc')\n",
    "}\n",
    "for regr in regrs.values():\n",
    "    regr.fit(ds.data, ds.target)\n",
    "\n",
    "for key, val in regrs.items():\n",
    "    res = val.predict(ds.data)\n",
    "    print(key + ': ' +str(accuracy_score(ds.target, res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing Training And Test Data (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1528516 entries, 0 to 1528515\n",
      "Data columns (total 9 columns):\n",
      "AGEP      1528516 non-null int64\n",
      "PINCP     1265774 non-null float64\n",
      "QTRBIR    1528516 non-null int64\n",
      "JWAP      657901 non-null float64\n",
      "JWDP      657901 non-null float64\n",
      "DECADE    173327 non-null float64\n",
      "YOEP      173327 non-null float64\n",
      "WAOB      1528516 non-null int64\n",
      "RAC1P     1528516 non-null int64\n",
      "dtypes: float64(5), int64(4)\n",
      "memory usage: 105.0 MB\n"
     ]
    }
   ],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(1528516, n_iter=3, test_size=0.2, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "df_imputed = new_df.copy()\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'PINCP' in new_df:\n",
    "    y = df_imputed['PINCP'].values # get the labels we want\n",
    "    del df_imputed['PINCP'] # get rid of the class label\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-285906f8b540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# train the reusable logisitc regression model on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mlr_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# train object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get test set precitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/erikgabrielsen/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64, \n\u001b[0;32m-> 1142\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/erikgabrielsen/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    508\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    509\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Users/erikgabrielsen/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    396\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/erikgabrielsen/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     52\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     53\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 54\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object: \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Dataset Selection**\n",
    "\n",
    "Select a dataset identically to the way you selected for lab one, lab two, or lab three (table data, text data, or image data). You are not required to use the same dataset that you used in the past, but you are encouraged. You must identify a classification task from the dataset that contains three or more classes to predict. That is, it cannot be a binary classification; it must be multi-class prediction. \n",
    "\n",
    "**Grading Rubric**\n",
    "\n",
    "**Preparation** (15 points total)\n",
    "\n",
    "[5 points] (mostly the same as from lab four) Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the task is and what parties would be interested in the results. How well should your algorithm perform in order to be useful to third parties. \n",
    "\n",
    "[10 points] (mostly the same as from labs one through three) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "**Evaluation** (20 points total)\n",
    "\n",
    "[10 points] (mostly the same as from lab five) Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s generalization performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\n",
    "[10 points] (mostly the same as from lab five) Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "\n",
    "**Modeling** (55 points total)\n",
    "\n",
    "[35 points] Create a custom ensemble classifier that uses multi-layer perceptron models for the individual classifiers. You can use bagging or boosting to select the training examples for each MLP in the ensemble, whichever you prefer.   \n",
    "\n",
    "[20 points] Evaluate the performance of the ensemble classifier with your chosen evaluation metric(s). Visualize the results with a confusion matrix, receiver operating characteristic, and area under the curve. Visually compare its performance to the individual classifiers that make up the ensemble.\n",
    "\n",
    "**Exceptional Work** (10 points total)\n",
    "\n",
    "You have free reign to provide additional analyses.\n",
    "One idea: add randomized feature selection to your bagging or boosting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab 06 - Ensembles\n",
    "\n",
    "By Erik Gabrielsen, Danh Nguyen, and Conrad Appel\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "In this lab, we are interested in investigating image processing techniques and seeing how well these techniques will work with our custom multilayer perceptron and scikit-learn's. We will be using the CIFAR-10 small images (32x32) dataset, which contains the following labels:\n",
    "\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "The goal is to have the models predict as well as possible according to the F1 score, since we are interested in seeing how correctly images are classified. The highest score based on accuracy on Kaggle is around .96, but that requires hours and hours of computation time. If we can perform image recognition anywhere near as well with a relatively simple model to implement, then perhaps we have good processing techniques. This project has computer vision applications, and could be applied to robotics and especially people that wants to deploy simple image recognition models without access to large computationally expensive servers. Ideally, we would want our F1 score to be around 90%, so that the majority of images can be accomplished by the computer with some checks by people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "from random import randint\n",
    "import _pickle as cPickle\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.graph_objs import Scatter, Marker, Layout, XAxis, YAxis, Bar, Line\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from scipy.misc import imread\n",
    "from scipy.special import expit\n",
    "import pywt # conda install -c conda-forge pywavelets\n",
    "from skimage.filters import roberts\n",
    "from skimage.feature import daisy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext memory_profiler\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# a helper plotting function\n",
    "def plot_gallery(imgs, labels, h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(imgs[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(labels[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Download dataset from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# save all files to ./imgs/\n",
    "###\n",
    "\n",
    "def get_images(): # stuck this in a function to clean up memory    \n",
    "    dics = []\n",
    "    for root, directory, files in os.walk('imgs'):\n",
    "        for f in files:\n",
    "            if 'data_batch' in f or 'test_batch' in f:\n",
    "                with open(root+'/'+f, 'rb') as fo:\n",
    "                    dics.append(cPickle.load(fo, encoding='latin1'))\n",
    "\n",
    "    img_color = []\n",
    "    img_labels = []\n",
    "    for dic in dics:\n",
    "        for i in range(len(dic['data'])):\n",
    "            img_color.append(dic['data'][i]) # 1D img (1024 R, 1024 G, 1024 B)\n",
    "            img_labels.append(dic['labels'][i]) # int representing the label\n",
    "\n",
    "    img_color = np.array(img_color)\n",
    "    img_labels = np.array(img_labels)\n",
    "\n",
    "    # grab the mapping between label names and IDs\n",
    "    print('Labels:')\n",
    "    labels = {}\n",
    "    with open('./imgs/batches.meta', 'rb') as fo:\n",
    "        labels_tmp = cPickle.load(fo, encoding='latin1')\n",
    "        for i in range(len(labels_tmp['label_names'])):\n",
    "            labels[i] = labels_tmp['label_names'][i]\n",
    "            print(i, \"-->\", labels_tmp['label_names'][i])\n",
    "    print()\n",
    "\n",
    "    img_label_names = np.array([labels[x] for x in img_labels])\n",
    "\n",
    "    def toGrayscale(img):\n",
    "        r, g, b = img[:1024], img[1024:2048], img[2048:]\n",
    "        gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "        return gray\n",
    "\n",
    "    img_gray = np.array([toGrayscale(x) for x in img_color])\n",
    "    \n",
    "    return (img_color, img_gray, img_labels, img_label_names)\n",
    "\n",
    "img_color, img_gray, img_labels, img_label_names = get_images()\n",
    "img_color = None # clear this memory because my computer ugh\n",
    "img_gray = img_gray\n",
    "img_labels = img_labels\n",
    "print(\"n_samples: {}\".format(len(img_gray)))\n",
    "print(\"n_features: {}\".format(len(img_gray[0])))\n",
    "print(\"n_classes: {}\".format(len(np.unique(img_labels))))\n",
    "print(\"Original Image Size: {} x {}\".format(32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Image Preprocessing\n",
    "For image processing we tried a multitude of different processing techniques. The first was converting the images from color to grayscale, then using edge-detection to create features for each image. From this pre-processing, we were only able to generate a 30% f1 score with sci-kit learn's MLP classifier. Then on top of this, we tried the python wavelet transform on the images, which only got a slightly better f1 score of 32%. According to Kaggle's leaderboard score, the best score (which was achieved by convolutional neuron networks) was a 95.5% f1 score, and this was a model trained for over 20hrs. According a paper writen by Shouhan Lin, Roland Memisevic, and Kishore Konda, they expiremented the CIFAR-10 dataset and for preprocessing used a \"contrast normalization to have a zero mean, followed by PCA whitening, retaining 99% of the variance\" (6). Using ReLu Networks with different configurations, they were able to obtain a f1 score of 56.84%. Based off these results, our goal was to obtain a f1 score of around 50%. \n",
    "Using our MLP model, we then attempted this preprocessing technique and achieved an f1 score well below 30%. From more research, we attempted to do gray-scale, global contrast normalization on all images followed by ZCA whitening. This resulted in an f1 score of 35%. Since we were still not near our goal, we tried several other techniques, such as daisy feature extraction and color flattening. In addition, we extended color flattening and overlayed the edge-detection on top of that and yielded similar results to edge-detection and gray-scale. \n",
    "\n",
    "Finally we discovered that using grayscale and daisy feature extraction, we were consistantly able to get a f1 score of around 57%. Thus we perform daisy feature extraction and gray scale dimensionality reduction. Then we use Standard Scalar in our pipeline to scale down our data.\n",
    " \n",
    "The final dataset looks something like this, where each row is one image in gray scale. Daisy feature extraction is performed at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(img_gray, img_label_names, 32, 32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def global_contrast_normalization(x):\n",
    "    x = x - x.mean(axis=1)[:, np.newaxis]\n",
    "    normalizers = np.sqrt((x ** 2).sum(axis=1))\n",
    "    x /= normalizers[:, np.newaxis]\n",
    "    return x\n",
    "\n",
    "# contrast normalization\n",
    "normalized = np.array([np.concatenate(global_contrast_normalization(x.reshape((32, 32)))) for x in img_gray])\n",
    "plot_gallery(normalized, img_label_names, 32, 32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "my_file = Path('./imgs/cached_daisies')\n",
    "if my_file.is_file():\n",
    "    with open('./imgs/cached_daisies', 'rb') as f:\n",
    "        daisies = pickle.load(f)\n",
    "else:\n",
    "    daisies = np.array([np.concatenate(np.concatenatte(daisy(x.reshape((32,32)), step=16, radius=7, rings=2, histograms=8, orientations=5))) for x in img_gray])\n",
    "    with open('./imgs/cached_daisies', 'wb+') as f:\n",
    "        pickle.dump(daisies, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_estimators=20, samp_percent=.25, replacement=False, weighted=True, proba_voting=False):\n",
    "        super().__init__()\n",
    "        self.n_estimators = n_estimators\n",
    "        self.samp_percent = samp_percent\n",
    "        self.replacement = replacement\n",
    "        self.weighted = weighted\n",
    "        self.proba_voting = proba_voting\n",
    "        \n",
    "    def _get_subset(self, x, y, replacement=True, samp_percent=.25, return_other=False):\n",
    "        xy = np.hstack([x, y.reshape((len(y),1))])\n",
    "        np.random.shuffle(xy)\n",
    "        size = int(len(y)*samp_percent) if not replacement else len(y)\n",
    "        indexes = np.random.choice(range(len(y)), size=size, replace=replacement)\n",
    "        \n",
    "        new_x = xy[indexes][:, :-1]\n",
    "        new_y = xy[indexes][:, -1]\n",
    "        \n",
    "        if not return_other:\n",
    "            return new_x, new_y\n",
    "        else:\n",
    "            inv_indexes = [i for i in range(len(xy)) if i not in indexes]\n",
    "            alternate_x = xy[inv_indexes][:, :-1]\n",
    "            alternate_y = xy[inv_indexes][:, -1]\n",
    "            return new_x, new_y, alternate_x, alternate_y\n",
    "        \n",
    "        \n",
    "    def fit(self, x, y, print_progress=False):\n",
    "        x, y = check_X_y(x, y)\n",
    "        x, y = x.copy(), y.copy()\n",
    "        \n",
    "        self.classifiers_ = []\n",
    "        self.precisions_ = []\n",
    "        self.n_features_ = x.shape[1]\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)            \n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            if print_progress:\n",
    "                sys.stdout.write('Estimator '+str(i)+' fitting.....')\n",
    "            new_x, new_y, alt_x, alt_y = self._get_subset(x, y, return_other=True, replacement=self.replacement, samp_percent=self.samp_percent)\n",
    "            c = MLPClassifier()\n",
    "            c.fit(new_x, new_y)\n",
    "            self.classifiers_.append(c)\n",
    "            if print_progress:\n",
    "                sys.stdout.write('testing.....')\n",
    "            self.precisions_.append(f1_score(alt_y, c.predict(alt_x), average='weighted'))\n",
    "            if print_progress:\n",
    "                sys.stdout.write('complete!\\n')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = check_array(x)\n",
    "            \n",
    "        results = np.zeros((x.shape[0], self.n_classes_))\n",
    "        for classifier, precision in zip(self.classifiers_, self.precisions_):\n",
    "            if self.proba_voting:\n",
    "                res = classifier.predict_proba(x)\n",
    "                results += res * (precision if self.weighted else 1)\n",
    "            else:\n",
    "                res = classifier.predict(x).astype(np.int)\n",
    "                for i in range(len(res)):\n",
    "                    results[i][res[i]] += precision if self.weighted else 1\n",
    "        \n",
    "        # select highest picked class for each row\n",
    "        res = [np.argmax(x) for x in results]\n",
    "        res = self.classes_[res]\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p2 = Pipeline([('scaler', StandardScaler()), ('tlp', EnsembleClassifier(n_estimators=2, replacement=False, proba_voting=True))])\n",
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=.2)\n",
    "for train_index, test_index in cv.split(daisies, img_labels):\n",
    "    p2.fit(daisies[train_index], img_labels[train_index], tlp__print_progress=True)\n",
    "    yhat = p2.predict(daisies[test_index])\n",
    "    print('f1 score:', f1_score(img_labels[test_index], yhat, average='macro'))\n",
    "    print(np.bincount(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = daisies\n",
    "y = img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=.2)\n",
    "\n",
    "params =  dict(n_hidden=100, \n",
    "      C=0.05, # tradeoff L2 regularizer\n",
    "      epochs=200, # iterations\n",
    "      eta=0.0001,  # learning rate\n",
    "      random_state=1,\n",
    "      cost_func=\"cross entropy\",\n",
    "      first_func=\"sigmoid\")\n",
    "p2 = Pipeline([('scaler', StandardScaler()), ('TLP', TLPMiniBatch(**params))])\n",
    "for train_index, test_index in cv.split(x, y):\n",
    "    %time %memit p2.fit(x[train_index], y[train_index])\n",
    "    yhat = p2.predict(x[test_index])\n",
    "    print('f1 score:', f1_score(y[test_index], yhat, average='macro'))\n",
    "    print(np.bincount(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Initially, we planned to use Randomized Grid Search to determine the best combinations of cost, activation functions, eta, and L2 regularization to use against scikit-learn's implementation. However, after about 2 hours of letting grid search run, we decided not to go with this implementation. \n",
    "\n",
    "We ultimately chose to use a sigmoid, cross entropy MLP to tune hyperparameters and perform our modeling with. After trying out combinations of the following regularization and eta values: \n",
    "```\n",
    "C_vals = [0.0001, 0.05, 0.5]\n",
    "eta_vals = [0.0001, 0.001, 0.1]\n",
    "```\n",
    "We found that the combination of `C=0.05` and `eta=0.001` gave the best F1 score of 0.387. While this is not the F1 score we wanted to suggest that we can deploy a robust image processing technique, we have already tried a variety of processing techniques decided to continue with our analysis and this implementation appears to work the best with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Gridsearch \n",
    "params = dict(\n",
    "    tlp__n_hidden = [50],\n",
    "    tlp__eta = [.0001, .001, .1],\n",
    "    tlp__epochs = [200],\n",
    "    tlp__C = [0.0, .5, .2],\n",
    "    tlp__first_func = ('sigmoid', 'linear', 'relu'),\n",
    "    tlp__cost_func = ('quadratic', 'cross entropy'),\n",
    ")\n",
    "\n",
    "pl = Pipeline([('scaler', StandardScaler()), ('tlp', TLPMiniBatch())])\n",
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=.2)\n",
    "gs = RandomizedSearchCV(pl, params, n_jobs=-1, scoring='f1_macro', cv=cv)\n",
    "gs.fit(x, y)\n",
    "print('Best score: '+str(gs.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Custom Implementation with different evaluation metrics\n",
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=.2)\n",
    "C_vals = [0.0001, 0.05, 0.5]\n",
    "eta_vals = [0.0001, 0.001, 0.1]\n",
    "results = []\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=.2)\n",
    "\n",
    "for c in C_vals:\n",
    "    for eta in eta_vals:\n",
    "        params =  dict(n_hidden=100, \n",
    "              C=c, # tradeoff L2 regularizer\n",
    "              epochs=200, # iterations\n",
    "              eta=eta,  # learning rate\n",
    "              random_state=1,\n",
    "              cost_func=\"cross entropy\",\n",
    "              first_func=\"sigmoid\")\n",
    "        pl = Pipeline([('scaler', StandardScaler()), ('TLP', TLPMiniBatch(**params))])\n",
    "        for train_index, test_index in cv.split(x, y):\n",
    "            pl.fit(x[train_index], y[train_index])\n",
    "            yhat = pl.predict(x[test_index])\n",
    "            results.append(f1_score(y[test_index], yhat, average='macro'))\n",
    "            print('f1 score:', f1_score(y[test_index], yhat, average='macro'))\n",
    "            print(np.bincount(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(6,6))    \n",
    "\n",
    "plt.plot(results[0:2], label=\"C=0.001\")\n",
    "plt.plot(results[3:5], label=\"C=0.05\")\n",
    "plt.plot(results[6:8], label=\"C=0.5\")\n",
    "\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.title(\"F1 score variance with eta and C values\")\n",
    "plt.xlabel(\"eta value\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "conf = mt.confusion_matrix(y_test, y_hat)\n",
    "print(\"Confusion matrix\\n\",conf)\n",
    "print(cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Scikit Comparison\n",
    "\n",
    "Next, we deployed our tuned, sigmoid, cross entropy model against scikit-learn's MLP Classifier. Scikit-learn's default implementation is 100 hidden layers, using a RELU activation and `adam` gradient solver. We compared the the F1 scores of the two implementations, memory usage, and run time. Scikit provided better performance than ours across these three metrics. \n",
    "\n",
    "However, the scikit implementation with our preprocessing techniques hovers around an F1 score of .55. This means our image processing techniques is not great with this model for image detection. Hopefully as our It was still a great exploration into different types of image processing techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SKLearn's implementation\n",
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=.2)\n",
    "pl = Pipeline([('scaler', StandardScaler()), ('TLP', MLPClassifier())])\n",
    "for train_index, test_index in cv.split(x, y):\n",
    "    %time %memit pl.fit(x[train_index], y[train_index])\n",
    "    yhat = pl.predict(x[test_index])\n",
    "    print('f1 score:', f1_score(y[test_index], yhat, average='macro'))\n",
    "    print(np.bincount(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=.2)\n",
    "\n",
    "params =  dict(n_hidden=100, \n",
    "      C=0.05, # tradeoff L2 regularizer\n",
    "      epochs=200, # iterations\n",
    "      eta=0.0001,  # learning rate\n",
    "      random_state=1,\n",
    "      cost_func=\"cross entropy\",\n",
    "      first_func=\"sigmoid\")\n",
    "p2 = Pipeline([('scaler', StandardScaler()), ('TLP', TLPMiniBatch(**params))])\n",
    "for train_index, test_index in cv.split(x, y):\n",
    "    %time %memit p2.fit(x[train_index], y[train_index])\n",
    "    yhat = p2.predict(x[test_index])\n",
    "    print('f1 score:', f1_score(y[test_index], yhat, average='macro'))\n",
    "    print(np.bincount(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "objects = ['Scikitlearn MLP', 'Custom MLP']\n",
    "\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "performance = [929.15, 1162.11]\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos,objects)\n",
    "# plt.ylabel('Memory usage')\n",
    "# plt.title('Scikit vs Custom MLP Memory Usage')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "objects = ['Scikitlearn MLP', 'Custom MLP']\n",
    "\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "performance = [2.75, 5.8]\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos,objects)\n",
    "# plt.ylabel(\"Time in minutes\")\n",
    "# plt.title(\"Scikit vs Custom MLP Execution Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our version of MLP performed twice as long as Scikit Learn's implementation while only performing a little bit worse in terms of memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sources\n",
    "\n",
    "[1] Zhouhan Lin, Roland Memisevic, Kishore Konda. \"How Far can we go without Convolutions: Improving Fully-Connected Networks\". <i>Cambridge</i>. 2016. https://arxiv.org/pdf/1511.02580.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=.2)\n",
    "pl = Pipeline([('scaler', StandardScaler()), ('RFC', RandomForestClassifier(n_estimators=1000))])\n",
    "for train_index, test_index in cv.split(x, y):\n",
    "    %time pl.fit(x[train_index], y[train_index])\n",
    "    yhat = pl.predict(x[test_index])\n",
    "    print('f1 score:', f1_score(y[test_index], yhat, average='macro'))\n",
    "    print(np.bincount(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparation (40 points total)\n",
    "\n",
    "* [10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).   \n",
    "* [15 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "* [15 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "\n",
    "## Modeling (50 points total)\n",
    "\n",
    "* [20 points] Create a convolutional neural network to use on your data using tensorflow. \n",
    "* [20 points] Investigate at least two different deep network architectures. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab. Visualize the results of the CNNs. \n",
    "* [10 points] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve.   \n",
    "\n",
    "## Exceptional Work (10 points total)\n",
    "\n",
    "You have free reign to provide additional analyses.\n",
    "One idea: Visualize the convolutional filters chosen by your CNN. Try to interpret some of their meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.3'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Reshape, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30s - loss: 1.7429 - acc: 0.3803 - val_loss: 1.8175 - val_acc: 0.3499\n",
      "Epoch 2/50\n",
      "30s - loss: 1.4882 - acc: 0.4768 - val_loss: 2.5766 - val_acc: 0.2809\n",
      "Epoch 3/50\n",
      "30s - loss: 1.3897 - acc: 0.5140 - val_loss: 1.3168 - val_acc: 0.5409\n",
      "Epoch 4/50\n",
      "29s - loss: 1.3320 - acc: 0.5358 - val_loss: 1.2676 - val_acc: 0.5562\n",
      "Epoch 5/50\n",
      "29s - loss: 1.2866 - acc: 0.5517 - val_loss: 1.2594 - val_acc: 0.5682\n",
      "Epoch 6/50\n",
      "29s - loss: 1.2514 - acc: 0.5625 - val_loss: 1.5797 - val_acc: 0.4836\n",
      "Epoch 7/50\n",
      "30s - loss: 1.2299 - acc: 0.5720 - val_loss: 1.4318 - val_acc: 0.5109\n",
      "Epoch 8/50\n",
      "29s - loss: 1.2071 - acc: 0.5805 - val_loss: 1.5222 - val_acc: 0.4893\n",
      "Epoch 9/50\n",
      "29s - loss: 1.1900 - acc: 0.5865 - val_loss: 1.4509 - val_acc: 0.4974\n",
      "Epoch 10/50\n",
      "30s - loss: 1.1735 - acc: 0.5923 - val_loss: 1.3758 - val_acc: 0.5397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f599f3a1550>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_lambda = 0.0001\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "NUM_CLASSES = 10\n",
    "y_train_ohe = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# build model\n",
    "lenet = Sequential()\n",
    "lenet.add(Conv2D(filters=6,kernel_size=(5,5),\n",
    "               input_shape = (32,32,3), \n",
    "               padding='valid', \n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda)))\n",
    "lenet.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "lenet.add(BatchNormalization())\n",
    "lenet.add(Activation(\"sigmoid\"))\n",
    "lenet.add(Conv2D(filters=16,kernel_size=(5,5), \n",
    "               padding='valid', \n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda)))\n",
    "lenet.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "lenet.add(BatchNormalization())\n",
    "lenet.add(Activation(\"sigmoid\"))\n",
    "#lenet.add(Dropout(0.5))\n",
    "lenet.add(Conv2D(filters=120,kernel_size=(1,1), \n",
    "               padding='valid', \n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda)))\n",
    "\n",
    "lenet.add(Flatten())\n",
    "lenet.add(Dense(84))\n",
    "lenet.add(Activation(\"sigmoid\"))\n",
    "lenet.add(Dense(NUM_CLASSES))\n",
    "lenet.add(Activation('softmax'))\n",
    "lenet.compile(loss='categorical_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transofmrations\n",
    "lenet.fit_generator(datagen.flow(x_train, y_train_ohe, batch_size=128), \n",
    "  steps_per_epoch=int(len(x_train)/128), # how many generators to go through per epoch\n",
    "  epochs=50, verbose=2,\n",
    "  validation_data=(x_test,y_test_ohe),\n",
    "  callbacks=[EarlyStopping(monitor='val_loss', patience=4)]\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAABSCAYAAACv1pTPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABTRJREFUeJzt3etP1nUcxvEvyJoSpOMQhYSnbrOQ0gJP0xQVbPiA5XSd\nbD0glg6VNcSmzY1WKzctnZiljE6rJF1lrDzQzDk7YIUtjTALZAOLaIF0EqG8+xM+1wO2e/vs/Xp8\n7frxA732e/LdNy4ajQYAgC/xsf4BAADDj3EHAIcYdwBwiHEHAIcYdwBwiHEHAIcYdwBwiHEHAIcY\ndwBwKCFWDy7Kr5aOxnYWjTYzCQPaMyfd+6OZ+afieqmrsbk6TsnNWbFNes/1W94yMx2DaUpVOFx6\nt5kZSBspdZ1sqDLfc8rm7dI7vv3odjNT1b5cqQoJj19rZrJqO6WuuvzXzHe8J2eT9I4/L043MzMf\n/kapCo0tt9ldky9IXftn75H+vVZ9u0J6z9PrppuZtvuuUapCQV6LmWndPlXqatpXab7n1e6I9I7z\nyh8zM+/ufEGpCveXVpiZpCe7pK4P59VIf0u+3AHAIcYdABxi3AHAIcYdABxi3AHAIcYdABxi3AHA\nIcYdABxi3AHAoZidUI37T7u79fnSOjPzcX+O1HV23e1mpm/aKKlLNfL3ISm3ra3IzGQn92kPbTpj\nRq6snKV1CSYXtUm5lBH27+LvvWOlrs+PvGxmavrGSV2K7vna6eChBf1m5uu6aVJXRslvZqat7hap\nK8zWYgfO3inlItU9ZibpULbU1bXmLzNT2npQ6gqh0kwsKC2Tmnrm2t++C1+skroGCu29K0vR/h+p\n+HIHAIcYdwBwiHEHAIcYdwBwiHEHAIcYdwBwiHEHAIcYdwBwKGaHmKa/+p2U2zEl18x0bpghdbUc\n2G1m1lycKXWpfs3XrrMrzz5lZl7aWyJ1ZY3tMDM9s7RDZIru2glSrmDJWjOTMFW6QSzM2LTazPy5\n1D4cE0IIFbfamVG92u+rYHyrmXk/N1/qihT/ZGY63kmUulRzI/YzQwjhqcxDZqbwBu2Az9Diu8zM\nc6evSl1lwpmuxLZeqevYntfNTMkz4iGmXPsu0H21hVLXxh1SjC93APCIcQcAhxh3AHCIcQcAhxh3\nAHCIcQcAhxh3AHCIcQcAhxh3AHAoZidUP2i3T56GEMLA1mQzs2z+F1LXkkz7erPLRzOlLlX8v1pu\n1ZiLZqZGvAHwkeOfmZnDvfaVg6rRbzZJuaQu+wq3uKh9ki+EECZuPWdmjp28Q+pSpJdfkHLvncoz\nMwvzWqSuT3bbJ6+fyP1I6gphs5S6NKideH1w43ozM6n+S6mr/wH7xG5Gqn2tn+rcWu3KxKyEJDOT\n3vyH1HU5Y7SZiWqHs2V8uQOAQ4w7ADjEuAOAQ4w7ADjEuAOAQ4w7ADjEuAOAQ4w7ADgUs0NM1yVq\nh1WGbhxhZk78crPU1bfFPrzw9Ph6qUuVVd8u5favtg85rFqpHVh5dtdDZibl+ytSV2i0IwnjbpKq\n4j49Y2Z+2GUfdAohhKFLGWZmQ3GD1BVCpZk4GDkqNU04X2Zmjn+VI3XFjxk0M8uSz0tdqobIESm3\n9ESxmWl9xT40GEIIqWn2tXcjd6ZKXaHIjlQusq8IDCGEyBv2VY4Tm7UDlKF4jv285cP7t+TLHQAc\nYtwBwCHGHQAcYtwBwCHGHQAcYtwBwCHGHQAcYtwBwCHGHQAciotGo7H+GQAAw4wvdwBwiHEHAIcY\ndwBwiHEHAIcYdwBwiHEHAIcYdwBwiHEHAIcYdwBwiHEHAIcYdwBwiHEHAIcYdwBwiHEHAIcYdwBw\niHEHAIcYdwBwiHEHAIcYdwBwiHEHAIcYdwBwiHEHAIcYdwBw6H/O0/FrprGhLAAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5976e60240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAA1CAYAAAC6J6stAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhGjIPwGUZFw07GaaGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59775a9400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAA2CAYAAADHyO4ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAQRJREFUeJzt3bFpBEEABMG+R6l9CIpSISg3nZxxZQge7g+qAljGa9bZ\nPc7zDAAeVw8A4D0IAgCVIAAwggBAJQgAjCAAUAkCACMIAFSCAMAIAgBVfVw94C/Px+fL3tT4/vk6\n7n62zTZfcbbN99/8H24IAFSCAMAIAgCVIAAwggBAJQgAjCAAUAkCACMIAFSCAMAIAgCVIAAwggBA\nJQgAjCAAUAkCACMIAFSCAMAc5/myn9sAuDE3BAAqQQBgBAGAShAAGEEAoBIEAEYQAKgEAYARBAAq\nQQBgBAGAShAAGEEAoBIEAEYQAKgEAYARBAAqQQBgBAGAShAAGEEAoBIEAEYQAKjqF6D1QmfU94su\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59778b2048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_plots = 15\n",
    "for layer in lenet.layers:\n",
    "    if type(layer) == keras.layers.convolutional.Conv2D:\n",
    "        weights = layer.get_weights()[0]\n",
    "        for i in range(min(weights.shape[-1], max_plots)):\n",
    "            plt.subplot(1, min(weights.shape[-1], max_plots), i+1)\n",
    "            plt.imshow(weights[:,:,0,i])\n",
    "            plt.axis('off')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

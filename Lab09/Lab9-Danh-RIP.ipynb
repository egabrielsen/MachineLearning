{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 09: Recurrent Network Architectures\n",
    "\n",
    "Conrad Appel, Erik Gabrielsen, Danh Nguyen\n",
    "\n",
    "### Dataset Selection\n",
    "\n",
    "Select a dataset identically to lab two. That is, the dataset must be text data (or time series sequence). In terms of generalization performance, it is helpful to have a large dataset of similar sized text documents. It is fine to perform binary classification or multi-class classification. The classification can be \"many-to-one\" or \"many-to-many\" sequence classification, whichever you feel more comfortable with. \n",
    "\n",
    "### Preparation (40 points total)\n",
    "[20 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).   \n",
    "\n",
    "[10 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\n",
    "[10 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "\n",
    "### Modeling (50 points total)\n",
    "\n",
    "[25 points] Investigate at least two different recurrent network architectures (perhaps LSTM and GRU). Adjust hyper-parameters of the networks as needed to improve generalization performance. \n",
    "\n",
    "[25 points] Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab. Visualize the best results of the RNNs.   \n",
    "\n",
    "### Exceptional Work (10 points total)\n",
    "You have free reign to provide additional analyses.\n",
    "\n",
    "One idea: Use more than a single chain of LSTMs or GRUs (i.e., use multiple parallel chains). \n",
    "\n",
    "Another Idea: Try to create a RNN for generating novel text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=top_words,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=None,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_art_length = 500\n",
    "NUM_CLASSES = 46\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_art_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_art_length)\n",
    "\n",
    "y_train_ohe = utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_ohe = utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (8982, 500)\n",
      "<class 'numpy.ndarray'> (500,)\n",
      "Vocabulary size: 4999\n",
      "(8982,) 0 45\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train),X_train.shape)\n",
    "print(type(X_train[0]),X_train[0].shape)\n",
    "print('Vocabulary size:', np.max(X_train))\n",
    "print(y_train.shape, np.min(y_train), np.max(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                2346      \n",
      "=================================================================\n",
      "Total params: 257,396\n",
      "Trainable params: 257,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                2346      \n",
      "=================================================================\n",
      "Total params: 272,546\n",
      "Trainable params: 272,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 50)                15150     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 46)                2346      \n",
      "=================================================================\n",
      "Total params: 267,496\n",
      "Trainable params: 267,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBED_SIZE = 50\n",
    "rnns = []\n",
    "input_holder = Input(shape=(X_train.shape[1], ))\n",
    "shared_embed = Embedding(top_words, \n",
    "                EMBED_SIZE, \n",
    "                input_length=max_art_length)(input_holder)\n",
    "\n",
    "for func in [SimpleRNN, LSTM, GRU]:\n",
    "    \n",
    "    x = func(50,dropout=0.2, recurrent_dropout=0.2)(shared_embed)\n",
    "    x = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "    rnn=Model(inputs=input_holder,outputs=x)\n",
    "    rnn.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='rmsprop', \n",
    "                  metrics=['accuracy'])\n",
    "    print(rnn.summary())\n",
    "    rnns.append(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= lstm ========\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 20s - loss: 3.3987 - acc: 0.2219 - val_loss: 2.6831 - val_acc: 0.3606\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 40s - loss: 2.4870 - acc: 0.3484 - val_loss: 2.4345 - val_acc: 0.3620\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 39s - loss: 2.4148 - acc: 0.3517 - val_loss: 2.4133 - val_acc: 0.3620\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 29s - loss: 2.3833 - acc: 0.3517 - val_loss: 2.3690 - val_acc: 0.3620\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 19s - loss: 2.3443 - acc: 0.3570 - val_loss: 2.3069 - val_acc: 0.3776\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 18s - loss: 2.2802 - acc: 0.3793 - val_loss: 2.2632 - val_acc: 0.3900\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 20s - loss: 2.2243 - acc: 0.4037 - val_loss: 2.2385 - val_acc: 0.4020\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 20s - loss: 2.1839 - acc: 0.4194 - val_loss: 2.2041 - val_acc: 0.4159\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 19s - loss: 2.1464 - acc: 0.4311 - val_loss: 2.1852 - val_acc: 0.4203\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 20s - loss: 2.1053 - acc: 0.4389 - val_loss: 2.1703 - val_acc: 0.4270\n",
      "======= gru ========\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 55s - loss: 2.6831 - acc: 0.3497 - val_loss: 2.4435 - val_acc: 0.3620\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 55s - loss: 2.4113 - acc: 0.3517 - val_loss: 2.3740 - val_acc: 0.3620\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 55s - loss: 2.2509 - acc: 0.3582 - val_loss: 2.0371 - val_acc: 0.3825\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 55s - loss: 2.0214 - acc: 0.3908 - val_loss: 1.8908 - val_acc: 0.4835\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 55s - loss: 1.8507 - acc: 0.5009 - val_loss: 1.7820 - val_acc: 0.5343\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 67s - loss: 1.7752 - acc: 0.5256 - val_loss: 1.7533 - val_acc: 0.5441\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 74s - loss: 1.7119 - acc: 0.5468 - val_loss: 1.6826 - val_acc: 0.5708\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 106s - loss: 1.6392 - acc: 0.5626 - val_loss: 1.6411 - val_acc: 0.5828\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 108s - loss: 1.5911 - acc: 0.5719 - val_loss: 1.6144 - val_acc: 0.5810\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 104s - loss: 1.5545 - acc: 0.5815 - val_loss: 1.5954 - val_acc: 0.6082\n"
     ]
    }
   ],
   "source": [
    "for rnn, name in zip(rnns,['lstm','gru']):\n",
    "    print('=======',name,'========')\n",
    "    rnn.fit(X_train, y_train_ohe, epochs=10, batch_size=64, validation_data=(X_test, y_test_ohe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2 training, lstm ========\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 27s - loss: 2.0007 - acc: 0.4527 - val_loss: 1.9435 - val_acc: 0.4622\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 34s - loss: 1.9008 - acc: 0.4743 - val_loss: 1.8968 - val_acc: 0.4666\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 34s - loss: 1.8491 - acc: 0.4892 - val_loss: 1.8937 - val_acc: 0.4684\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 33s - loss: 1.8269 - acc: 0.4991 - val_loss: 1.8969 - val_acc: 0.4715\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 30s - loss: 1.7952 - acc: 0.5038 - val_loss: 1.8895 - val_acc: 0.4751\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 29s - loss: 1.7691 - acc: 0.5157 - val_loss: 1.8824 - val_acc: 0.4684\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 26s - loss: 1.7447 - acc: 0.5269 - val_loss: 1.9013 - val_acc: 0.4751\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 26s - loss: 1.7214 - acc: 0.5341 - val_loss: 1.8965 - val_acc: 0.4746\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 25s - loss: 1.7053 - acc: 0.5459 - val_loss: 1.9255 - val_acc: 0.4693\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 27s - loss: 1.6895 - acc: 0.5453 - val_loss: 1.9501 - val_acc: 0.4697\n",
      "Round 2 training, gru ========\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 99s - loss: 1.5057 - acc: 0.6029 - val_loss: 1.6088 - val_acc: 0.5953\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 101s - loss: 1.4633 - acc: 0.6230 - val_loss: 1.5315 - val_acc: 0.6242\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 103s - loss: 1.3908 - acc: 0.6451 - val_loss: 1.4939 - val_acc: 0.6469\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 109s - loss: 1.3414 - acc: 0.6533 - val_loss: 1.4412 - val_acc: 0.6514\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 101s - loss: 1.2851 - acc: 0.6696 - val_loss: 1.4414 - val_acc: 0.6541\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 98s - loss: 1.2361 - acc: 0.6829 - val_loss: 1.3698 - val_acc: 0.6719\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 93s - loss: 1.1928 - acc: 0.6929 - val_loss: 1.3423 - val_acc: 0.6830\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 89s - loss: 1.1473 - acc: 0.7039 - val_loss: 1.3494 - val_acc: 0.6834\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 90s - loss: 1.1155 - acc: 0.7105 - val_loss: 1.2868 - val_acc: 0.6932\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 89s - loss: 1.0683 - acc: 0.7252 - val_loss: 1.2862 - val_acc: 0.6981\n"
     ]
    }
   ],
   "source": [
    "for rnn, name in zip(rnns,['lstm','gru']):\n",
    "    print('Round 2 training,',name,'========')\n",
    "    rnn.fit(X_train, y_train_ohe, epochs=10, batch_size=64, validation_data=(X_test, y_test_ohe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rnn, name in zip(rnns,['lstm','gru']):\n",
    "    rnn.save(name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

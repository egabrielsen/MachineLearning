{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "Select a dataset identically to the way you selected for lab one or lab three (table data or image data). You are not required to use the same dataset that you used in the past, but you are encouraged. You must identify a classification task from the dataset that contains three or more classes to predict. That is, it cannot be a binary classification; it must be multi-class prediction. \n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "### Preparation (15 points total)\n",
    "[5 points] (mostly the same processes as from lab four) Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the task is and what parties would be interested in the results. How well would your prediction algorithm need to perform to be considered useful by interested parties?\n",
    "\n",
    "[10 points] (mostly the same processes as from lab one) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "### Evaluation (30 points total)\n",
    "[15 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s generalization performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\n",
    "[15 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "\n",
    "Important: You should use your chosen evaluation criteria and chosen method for dividing train/test data throughout the report. For example, arguing that f-score is the best evaluation method, but then using accuracy in a grid search will be regarded as a conceptual error and graded accordingly. \n",
    "### Modeling (45 points total)\n",
    "[20 points] Create a custom implementation of the multi-layer perceptron. Start with the implementation given to you in the course. Update the MLP class to:\n",
    "When instantiated, use a selectable nonlinearity function for the first layer: either sigmoid or linear \n",
    "Use a selectable cost function when instantiated: either quadratic or cross entropy\n",
    "\n",
    "[15 points] Tune the hyper-parameters of your MLP model (at least two hyper-parameters). While tuning hyper-parameters, analyze the results using your chosen metric(s) of evaluation. Visualize the evaluation metric(s) versus the hyper-parameters. Conclude what combination of parameters are best.\n",
    "\n",
    "[10 points] Compare the performance of your MLP training procedure to scikit-learn. Which implementation is better in terms of generalization performance, computation time, and memory used while training?\n",
    "\n",
    "### Exceptional Work (10 points total)\n",
    "You have free reign to provide additional analyses.\n",
    "One idea: Add support for more than one hidden layer or a different nonlinear activation (i.e., softmax, tanh). Please note that different activation functions might require different weight initializations to work well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "from random import randint\n",
    "import _pickle as cPickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "from plotly.graph_objs import Scatter, Marker, Layout, XAxis, YAxis, Bar, Line\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.misc import imread\n",
    "from scipy.special import expit\n",
    "import pywt # conda install -c conda-forge pywavelets\n",
    "from skimage.filters import roberts\n",
    "from skimage.feature import daisy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a helper plotting function\n",
    "def plot_gallery(imgs, labels, h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(imgs[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(labels[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "0 --> airplane\n",
      "1 --> automobile\n",
      "2 --> bird\n",
      "3 --> cat\n",
      "4 --> deer\n",
      "5 --> dog\n",
      "6 --> frog\n",
      "7 --> horse\n",
      "8 --> ship\n",
      "9 --> truck\n",
      "\n",
      "n_samples: 60000\n",
      "n_features: 1024\n",
      "n_classes: 10\n",
      "Original Image Size: 32 x 32\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Download dataset from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# save all files to ./imgs/\n",
    "###\n",
    "\n",
    "def get_images(): # stuck this in a function to clean up memory \n",
    "    dics = []\n",
    "    for root, directory, files in os.walk('imgs'):\n",
    "        for f in files:\n",
    "            if 'data_batch' in f or 'test_batch' in f:\n",
    "                with open(root+'/'+f, 'rb') as fo:\n",
    "                    dics.append(cPickle.load(fo, encoding='latin1'))\n",
    "\n",
    "    img_color = []\n",
    "    img_labels = []\n",
    "    for dic in dics:\n",
    "        for i in range(len(dic['data'])):\n",
    "            img_color.append(dic['data'][i]) # 1D img (1024 R, 1024 G, 1024 B)\n",
    "            img_labels.append(dic['labels'][i]) # int representing the label\n",
    "\n",
    "    img_color = np.array(img_color)\n",
    "    img_labels = np.array(img_labels)\n",
    "\n",
    "    # grab the mapping between label names and IDs\n",
    "    print('Labels:')\n",
    "    labels = {}\n",
    "    with open('./imgs/batches.meta', 'rb') as fo:\n",
    "        labels_tmp = cPickle.load(fo, encoding='latin1')\n",
    "        for i in range(len(labels_tmp['label_names'])):\n",
    "            labels[i] = labels_tmp['label_names'][i]\n",
    "            print(i, \"-->\", labels_tmp['label_names'][i])\n",
    "    print()\n",
    "\n",
    "    img_label_names = np.array([labels[x] for x in img_labels])\n",
    "\n",
    "    def toGrayscale(img):\n",
    "        r, g, b = img[:1024], img[1024:2048], img[2048:]\n",
    "        gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "        return gray\n",
    "\n",
    "    img_gray = np.array([toGrayscale(x) for x in img_color])\n",
    "    \n",
    "    return (img_color, img_gray, img_labels, img_label_names)\n",
    "\n",
    "img_color, img_gray, img_labels, img_label_names = get_images()\n",
    "img_gray = img_gray\n",
    "img_labels = img_labels\n",
    "print(\"n_samples: {}\".format(len(img_gray)))\n",
    "print(\"n_features: {}\".format(len(img_gray[0])))\n",
    "print(\"n_classes: {}\".format(len(np.unique(img_labels))))\n",
    "print(\"Original Image Size: {} x {}\".format(32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_gallery(img_gray, img_label_names, 32, 32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def global_contrast_normalization(x):\n",
    "    x = x - x.mean(axis=1)[:, np.newaxis]\n",
    "    normalizers = np.sqrt((x ** 2).sum(axis=1))\n",
    "    x /= normalizers[:, np.newaxis]\n",
    "    return x\n",
    "\n",
    "# contrast normalization\n",
    "normalized = np.array([np.concatenate(global_contrast_normalization(x.reshape((32, 32)))) for x in img_gray])\n",
    "plot_gallery(normalized, img_label_names, 32, 32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daisies = np.array([np.concatenate(np.concatenate(daisy(x.reshape((32,32)), step=16, radius=7, rings=2, histograms=8, orientations=5))) for x in img_gray])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal functions to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None,\n",
    "                 first_func=\"sigmoid\", cost_func=\"cross entropy\"):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.first_func = first_func\n",
    "        self.cost_func = cost_func\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        if self.first_func == \"relu\":\n",
    "            init_bound = np.sqrt(6. / (self.n_hidden + self.n_features_ + 1))\n",
    "            W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_ + 1))\n",
    "\n",
    "            init_bound = np.sqrt(2. / (self.n_output_ + self.n_hidden + 1))\n",
    "            W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden + 1))\n",
    "        else: \n",
    "            W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "            W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "            W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "\n",
    "            W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "            W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "            W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _relu(Z):\n",
    "        return np.maximum(0,Z.copy())\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        if self.cost_func == \"cross entropy\":\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        else: # quadratic\n",
    "            cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        Z1 = W1 @ A1.T\n",
    "        if self.first_func == \"sigmoid\":\n",
    "            A2 = self._sigmoid(Z1)\n",
    "        elif self.first_func == \"relu\":\n",
    "            A2 = self._relu(Z1)\n",
    "        else: \n",
    "            A2 = Z1\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # vectorized backpropagation\n",
    "        if self.cost_func == \"cross entropy\":\n",
    "            sigma3 = (A3-Y_enc)\n",
    "        else: # quadratic\n",
    "            sigma3 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        \n",
    "        if self.first_func == \"sigmoid\":\n",
    "            sigma2 = (W2.T @ sigma3)*A2*(1-A2)\n",
    "        elif self.first_func == \"relu\":\n",
    "            sigma2 = (W2.T @ sigma3) \n",
    "            Z1_with_bias = self._add_bias_unit(Z1,how='row')\n",
    "            sigma2[Z1_with_bias<=0] = 0\n",
    "        # relu derivative only zeros out certain values! easy!\n",
    "        else: # linear\n",
    "            sigma2 = (W2.T @ sigma3)\n",
    "        \n",
    "        grad1 = sigma2[1:,:] @ A1\n",
    "        grad2 = sigma3 @ A2.T\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        res = self.predict(x)\n",
    "        return accuracy_score(y, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# just start with the vectorized version and minibatch\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True, \n",
    "                 minibatches=1, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None,\n",
    "                 first_func=\"sigmoid\", cost_func=\"cross entropy\"):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(n_hidden=n_hidden, C=C, epochs=epochs, eta=eta, random_state=random_state, first_func=first_func, cost_func=cost_func)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        delta_W1_prev = np.zeros(self.W1.shape)\n",
    "        delta_W2_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2)\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                # momentum calculations\n",
    "                delta_W1, delta_W2 = self.eta * grad1, self.eta * grad2\n",
    "                self.W1 -= (delta_W1 + (self.alpha * delta_W1_prev))\n",
    "                self.W2 -= (delta_W2 + (self.alpha * delta_W2_prev))\n",
    "                delta_W1_prev, delta_W2_prev = delta_W1, delta_W2\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = img_gray\n",
    "y = img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params =  dict(n_hidden=100, \n",
    "              C=0.0001, # tradeoff L2 regularizer\n",
    "              epochs=200, # iterations\n",
    "              eta=0.0001,  # learning rate\n",
    "              random_state=1,\n",
    "              cost_func=\"cross entropy\",\n",
    "              first_func=\"sigmoid\")\n",
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=.2)\n",
    "pl = Pipeline([('scaler', StandardScaler()), ('TLP', TLPMiniBatch(**params))])\n",
    "for train_index, test_index in cv.split(x, y):\n",
    "    pl.fit(x[train_index], y[train_index])\n",
    "    yhat = pl.predict(x[test_index])\n",
    "    print('f1 score:', f1_score(y[test_index], yhat, average='macro'))\n",
    "    print(np.bincount(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SKLearn's implementation\n",
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=.2)\n",
    "pl = Pipeline([('scaler', StandardScaler()), ('TLP', MLPClassifier())])\n",
    "for train_index, test_index in cv.split(x, y):\n",
    "    pl.fit(x[train_index], y[train_index])\n",
    "    yhat = pl.predict(x[test_index])\n",
    "    print('f1 score:', f1_score(y[test_index], yhat, average='macro'))\n",
    "    print(np.bincount(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Gridsearch \n",
    "params = dict(\n",
    "    tlp__n_hidden = [50],\n",
    "    tlp__eta = [.0001, .001, .1],\n",
    "    tlp__epochs = [200],\n",
    "    tlp__C = [0.0, .5, .2],\n",
    "    tlp__first_func = ('sigmoid', 'linear', 'relu'),\n",
    "    tlp__cost_func = ('quadratic', 'cross entropy'),\n",
    ")\n",
    "\n",
    "pl = Pipeline([('scaler', StandardScaler()), ('tlp', TLPMiniBatch())])\n",
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=.2)\n",
    "gs = RandomizedSearchCV(pl, params, n_jobs=-1, scoring='f1_macro', cv=cv)\n",
    "gs.fit(x, y)\n",
    "print('Best score: '+str(gs.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
